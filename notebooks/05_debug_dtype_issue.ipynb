{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Root Cause and Fix\n",
    "\n",
    "### Root Cause Analysis\n",
    "\n",
    "The dtype mismatch error occurs because:\n",
    "\n",
    "1. **Stage 2 is wrapped with DataParallel** while Stage 1 is not\n",
    "   - Evidence: Stage 1 shows `0/136` steps, Stage 2 shows `0/68` steps (half!)\n",
    "   - The traceback includes `torch/nn/parallel/data_parallel.py`\n",
    "\n",
    "2. **DataParallel + AMP/autocast incompatibility**\n",
    "   - DataParallel uses worker threads for each GPU replica\n",
    "   - AMP/autocast context is **thread-local** - not inherited by worker threads\n",
    "   - In k-bit training, some layers (norms) run in fp32 for stability\n",
    "   - Without autocast in worker threads, fp32 activations flow to bf16 `lm_head` → crash\n",
    "\n",
    "3. **Why Stage 2 got DataParallel**\n",
    "   - The process sees 2 GPUs (`n_gpu=2`), and Trainer wraps DataParallel unless it detects model-parallelism.\n",
    "   - Model-parallelism is only detected if `hf_device_map` spans >1 GPU (or `is_parallelizable`+`model_parallel` are set).\n",
    "   - If `device_map` produces `hf_device_map={'': 0}` (single-GPU), Trainer will choose DataParallel by default.\n",
    "\n",
    "### The Fix (Implemented in `scripts/train_recipe.py`)\n",
    "\n",
    "```python\n",
    "# Fix for two-stage training: When loading from a checkpoint with multi-GPU,\n",
    "# force Trainer to behave as single-GPU to prevent DataParallel wrapping.\n",
    "# Note: this only prevents Trainer's DataParallel; it does not force model sharding.\n",
    "if checkpoint_path and torch.cuda.device_count() > 1:\n",
    "    training_args_recipe._n_gpu = 1\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Setting `_n_gpu=1` tells Trainer \"don't use DataParallel\"\n",
    "- If the model is sharded (multi-GPU `hf_device_map`), it can still use multiple GPUs via `device_map`\n",
    "- If the model fits on one GPU (`hf_device_map={'': 0}`), Stage 2 will run single-GPU (other GPU mostly idle)\n",
    "- This fix is conditional - only applies when loading from checkpoint\n",
    "- Other recipes (r1, r2, r4) that load from HuggingFace are unaffected\n",
    "\n",
    "### Verified Working (2024-12-15)\n",
    "\n",
    "```\n",
    "Stage 1: 136 steps completed (vision encoder training)\n",
    "Stage 2: 136 steps completed (LLM LoRA training from checkpoint)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug: Dtype Issue in Two-Stage Training\n",
    "\n",
    "This notebook investigates why loading a trained checkpoint with quantization causes dtype mismatch.\n",
    "\n",
    "**The Error:**\n",
    "```\n",
    "RuntimeError: expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16\n",
    "```\n",
    "\n",
    "**Hypothesis:** The Stage 1 checkpoint (trained with gradient checkpointing) has some float32 parameters that don't get properly converted when loaded with BitsAndBytes quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/miniconda3/envs/vlm_Qwen2VL_object_detection/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 4,5\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "PyTorch sees 2 GPU(s):\n",
      "  GPU 0: NVIDIA RTX 6000 Ada Generation\n",
      "  GPU 1: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"  # Use two GPUs for testing (physical IDs 4,5)\n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, Qwen2VLForConditionalGeneration\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch sees {torch.cuda.device_count()} GPU(s):\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "HUGGINGFACE_MODEL = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "STAGE1_CHECKPOINT = \"/ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-r3-stage1\"\n",
    "\n",
    "def analyze_model_dtypes(model, prefix=\"\"):\n",
    "    \"\"\"Analyze and print dtype distribution of model parameters.\"\"\"\n",
    "    dtype_counts = {\"vision\": Counter(), \"llm\": Counter(), \"other\": Counter()}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if \".visual.\" in name:\n",
    "            dtype_counts[\"vision\"][str(param.dtype)] += 1\n",
    "        elif \"model.layers\" in name or \"lm_head\" in name:\n",
    "            dtype_counts[\"llm\"][str(param.dtype)] += 1\n",
    "        else:\n",
    "            dtype_counts[\"other\"][str(param.dtype)] += 1\n",
    "    \n",
    "    print(f\"\\n{prefix}Parameter dtype distribution:\")\n",
    "    print(f\"  Vision encoder: {dict(dtype_counts['vision'])}\")\n",
    "    print(f\"  LLM layers:     {dict(dtype_counts['llm'])}\")\n",
    "    print(f\"  Other:          {dict(dtype_counts['other'])}\")\n",
    "    \n",
    "    return dtype_counts\n",
    "\n",
    "def print_sample_params(model, component=\"visual\", n=5):\n",
    "    \"\"\"Print dtype of first n parameters from a component.\"\"\"\n",
    "    count = 0\n",
    "    print(f\"\\nSample {component} parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if component in name:\n",
    "            print(f\"  {name}: {param.dtype}\")\n",
    "            count += 1\n",
    "            if count >= n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load HuggingFace Original (No Quantization)\n",
    "\n",
    "Baseline: What dtype does the original model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: HuggingFace Original (bf16, no quantization)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e9620a137a4d059bcc6b2da0eca4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HF Original bf16] Parameter dtype distribution:\n",
      "  Vision encoder: {'torch.bfloat16': 391}\n",
      "  LLM layers:     {'torch.bfloat16': 337}\n",
      "  Other:          {'torch.bfloat16': 2}\n",
      "\n",
      "Sample visual parameters:\n",
      "  model.visual.patch_embed.proj.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.bias: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.bias: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 1: HuggingFace Original (bf16, no quantization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_hf_bf16 = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    HUGGINGFACE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "analyze_model_dtypes(model_hf_bf16, \"[HF Original bf16] \")\n",
    "print_sample_params(model_hf_bf16, \"visual\")\n",
    "\n",
    "# Clean up\n",
    "del model_hf_bf16\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Load HuggingFace Original (With 4-bit Quantization)\n",
    "\n",
    "This is what r1-llm-only does. Should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 2: HuggingFace Original (4-bit quantization)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ef5d8129ab4caca2c7725fa4af854a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HF Original 4-bit] Parameter dtype distribution:\n",
      "  Vision encoder: {'torch.bfloat16': 261, 'torch.uint8': 130}\n",
      "  LLM layers:     {'torch.uint8': 196, 'torch.bfloat16': 141}\n",
      "  Other:          {'torch.bfloat16': 2}\n",
      "\n",
      "Sample visual parameters:\n",
      "  model.visual.patch_embed.proj.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.bias: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.bias: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 2: HuggingFace Original (4-bit quantization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_hf_4bit = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    HUGGINGFACE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "analyze_model_dtypes(model_hf_4bit, \"[HF Original 4-bit] \")\n",
    "print_sample_params(model_hf_4bit, \"visual\")\n",
    "\n",
    "# Clean up\n",
    "del model_hf_4bit\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Load Stage 1 Checkpoint (No Quantization)\n",
    "\n",
    "What dtype does the trained checkpoint have when loaded without quantization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 3: Stage 1 Checkpoint (bf16, no quantization)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ea595fbad04850a97a7b827f1ba0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage1 Checkpoint bf16] Parameter dtype distribution:\n",
      "  Vision encoder: {'torch.bfloat16': 391}\n",
      "  LLM layers:     {'torch.bfloat16': 337}\n",
      "  Other:          {'torch.bfloat16': 2}\n",
      "\n",
      "Sample visual parameters:\n",
      "  model.visual.patch_embed.proj.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.bias: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.bias: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 3: Stage 1 Checkpoint (bf16, no quantization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_stage1_bf16 = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    STAGE1_CHECKPOINT,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "analyze_model_dtypes(model_stage1_bf16, \"[Stage1 Checkpoint bf16] \")\n",
    "print_sample_params(model_stage1_bf16, \"visual\")\n",
    "\n",
    "# Clean up\n",
    "del model_stage1_bf16\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Load Stage 1 Checkpoint (With 4-bit Quantization)\n",
    "\n",
    "**THIS IS THE PROBLEMATIC CASE!** \n",
    "\n",
    "This is what r3-Stage2 tries to do and fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 4: Stage 1 Checkpoint (4-bit quantization)\n",
      "This is what causes the dtype mismatch!\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742810acd8ba4edda0f09db9402ebdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage1 Checkpoint 4-bit] Parameter dtype distribution:\n",
      "  Vision encoder: {'torch.bfloat16': 261, 'torch.uint8': 130}\n",
      "  LLM layers:     {'torch.uint8': 196, 'torch.bfloat16': 141}\n",
      "  Other:          {'torch.bfloat16': 2}\n",
      "\n",
      "Sample visual parameters:\n",
      "  model.visual.patch_embed.proj.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.bias: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.bias: torch.bfloat16\n",
      "\n",
      "lm_head dtype:\n",
      "  lm_head.weight: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 4: Stage 1 Checkpoint (4-bit quantization)\")\n",
    "print(\"This is what causes the dtype mismatch!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_stage1_4bit = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    STAGE1_CHECKPOINT,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "analyze_model_dtypes(model_stage1_4bit, \"[Stage1 Checkpoint 4-bit] \")\n",
    "print_sample_params(model_stage1_4bit, \"visual\")\n",
    "\n",
    "# Check lm_head dtype\n",
    "print(\"\\nlm_head dtype:\")\n",
    "for name, param in model_stage1_4bit.named_parameters():\n",
    "    if \"lm_head\" in name:\n",
    "        print(f\"  {name}: {param.dtype}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Apply Our Fix\n",
    "\n",
    "Cast vision encoder parameters to bf16 after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 5: Apply Fix - Cast vision encoder to bf16\n",
      "============================================================\n",
      "Cast 0 vision encoder parameters to bf16\n",
      "\n",
      "[After Fix] Parameter dtype distribution:\n",
      "  Vision encoder: {'torch.bfloat16': 261, 'torch.uint8': 130}\n",
      "  LLM layers:     {'torch.uint8': 196, 'torch.bfloat16': 141}\n",
      "  Other:          {'torch.bfloat16': 2}\n",
      "\n",
      "Sample visual parameters:\n",
      "  model.visual.patch_embed.proj.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm1.bias: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.weight: torch.bfloat16\n",
      "  model.visual.blocks.0.norm2.bias: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 5: Apply Fix - Cast vision encoder to bf16\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply fix: cast float32 vision params to bf16\n",
    "cast_count = 0\n",
    "for name, param in model_stage1_4bit.named_parameters():\n",
    "    if \".visual.\" in name and param.dtype == torch.float32:\n",
    "        param.data = param.data.to(torch.bfloat16)\n",
    "        cast_count += 1\n",
    "\n",
    "print(f\"Cast {cast_count} vision encoder parameters to bf16\")\n",
    "\n",
    "analyze_model_dtypes(model_stage1_4bit, \"[After Fix] \")\n",
    "print_sample_params(model_stage1_4bit, \"visual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, from the output, we can see this fix doesn't change anything, so we reverted the fix instead. Casting float32 vision params to bf16 is not the right solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Verify Forward Pass Works\n",
    "\n",
    "Try a simple forward pass to confirm the fix resolves the dtype mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 6: Forward Pass Test\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass SUCCESSFUL!\n",
      "  Output logits shape: torch.Size([1, 89, 152064])\n",
      "  Output logits dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 6: Forward Pass Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from transformers import Qwen2VLProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load processor\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    HUGGINGFACE_MODEL,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Create a simple test input\n",
    "# Use a small test image\n",
    "test_image = Image.new('RGB', (224, 224), color='red')\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": test_image},\n",
    "            {\"type\": \"text\", \"text\": \"What is this?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Process inputs\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[test_image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "inputs = {k: v.to(model_stage1_4bit.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "\n",
    "# Try forward pass\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        outputs = model_stage1_4bit(**inputs)\n",
    "    print(\"Forward pass SUCCESSFUL!\")\n",
    "    print(f\"  Output logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"  Output logits dtype: {outputs.logits.dtype}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Forward pass FAILED!\")\n",
    "    print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Compare the dtype distributions across all tests to understand the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ACTUAL FINDINGS (Updated after running)\n",
      "============================================================\n",
      "\n",
      "ACTUAL RESULTS:\n",
      "\n",
      "1. HF Original (bf16):        Vision = bf16 (391), LLM = bf16 (337)\n",
      "2. HF Original (4-bit):       Vision = bf16 (261) + uint8 (130), LLM = uint8 (196) + bf16 (141)\n",
      "3. Stage1 Checkpoint (bf16):  Vision = bf16 (391), LLM = bf16 (337)\n",
      "4. Stage1 Checkpoint (4-bit): Vision = bf16 (261) + uint8 (130), LLM = uint8 (196) + bf16 (141)\n",
      "5. After Fix:                 Cast 0 parameters (no float32 found!)\n",
      "6. Forward Pass:              SUCCESSFUL!\n",
      "\n",
      "CONCLUSION: \n",
      "- The vision encoder parameters are NOT float32!\n",
      "- My original hypothesis was WRONG.\n",
      "- The forward pass works fine in inference mode on single GPU.\n",
      "\n",
      "THE REAL ISSUE might be:\n",
      "- Multi-GPU + DataParallel interaction (original error shows data_parallel.py)\n",
      "- PEFT/LoRA wrapping the model during training\n",
      "- Training mode vs Inference mode differences\n",
      "\n",
      "The error only happens during TRAINING with:\n",
      "- 2 GPUs (device_map=\"balanced\")\n",
      "- SFTTrainer + PEFT/LoRA\n",
      "- Training mode (gradients enabled)\n",
      "\n",
      "Next step: Test with PEFT + training mode to reproduce the issue.\n",
      "\n",
      "\n",
      "GPU memory cleaned up.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ACTUAL FINDINGS (Updated after running)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "ACTUAL RESULTS:\n",
    "\n",
    "1. HF Original (bf16):        Vision = bf16 (391), LLM = bf16 (337)\n",
    "2. HF Original (4-bit):       Vision = bf16 (261) + uint8 (130), LLM = uint8 (196) + bf16 (141)\n",
    "3. Stage1 Checkpoint (bf16):  Vision = bf16 (391), LLM = bf16 (337)\n",
    "4. Stage1 Checkpoint (4-bit): Vision = bf16 (261) + uint8 (130), LLM = uint8 (196) + bf16 (141)\n",
    "5. After Fix:                 Cast 0 parameters (no float32 found!)\n",
    "6. Forward Pass:              SUCCESSFUL!\n",
    "\n",
    "CONCLUSION: \n",
    "- The vision encoder parameters are NOT float32!\n",
    "- My original hypothesis was WRONG.\n",
    "- The forward pass works fine in inference mode on single GPU.\n",
    "\n",
    "THE REAL ISSUE might be:\n",
    "- Multi-GPU + DataParallel interaction (original error shows data_parallel.py)\n",
    "- PEFT/LoRA wrapping the model during training\n",
    "- Training mode vs Inference mode differences\n",
    "\n",
    "The error only happens during TRAINING with:\n",
    "- 2 GPUs (device_map=\"balanced\")\n",
    "- SFTTrainer + PEFT/LoRA\n",
    "- Training mode (gradients enabled)\n",
    "\n",
    "Next step: Test with PEFT + training mode to reproduce the issue.\n",
    "\"\"\")\n",
    "\n",
    "# Clean up\n",
    "del model_stage1_4bit\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nGPU memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DataParallel Investigation\n",
    "\n",
    "The tests above showed that parameter dtypes are correct, but the error only occurs during training with:\n",
    "- 2 GPUs with `device_map=\"balanced\"`\n",
    "- SFTTrainer + PEFT/LoRA\n",
    "- Training mode (gradients enabled)\n",
    "\n",
    "**Key observation from error logs:**\n",
    "- Stage 1: Progress bar shows `0/136` steps (expected)\n",
    "- Stage 2: Progress bar shows `0/68` steps (half!)\n",
    "- Stage 2 traceback includes `torch/nn/parallel/data_parallel.py`\n",
    "\n",
    "**Hypothesis:** Stage 2 is being wrapped with DataParallel while Stage 1 is not.\n",
    "\n",
    "HuggingFace Trainer prevents DataParallel when it detects model parallelism via:\n",
    "- `model.hf_device_map` containing multiple devices\n",
    "- `model.is_parallelizable=True` and `model.model_parallel=True`\n",
    "\n",
    "**Important detail:** `hf_device_map` being present is not enough — Trainer only treats it as *model-parallel* if the map spans >1 GPU (or if `is_parallelizable` + `model_parallel` are set).\n",
    "\n",
    "Also, `device_map=\"auto\"` / `\"balanced\"` may keep a 4-bit model on a single GPU if it fits. So seeing `hf_device_map={'': 0}` is normal and means *no sharding happened*.\n",
    "\n",
    "If you need to **force** multi-GPU sharding for debugging, you must add explicit constraints (e.g. `max_memory`) so GPU 0 cannot hold the full model.\n",
    "\n",
    "**Reminder:** inside this notebook, GPUs are renumbered to `0,1` (these correspond to physical GPUs `4,5`).\n",
    "\n",
    "Let's investigate what attributes differ between HuggingFace loading vs checkpoint loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarifying: `device_map` vs DataParallel\n",
    "\n",
    "There are **two independent ways** a single Python process can \"use 2 GPUs\":\n",
    "\n",
    "1. **Model sharding (`device_map` / `hf_device_map`)**\n",
    "   - Splits *weights* across GPUs (model-parallel).\n",
    "   - Trainer will avoid DataParallel only if `hf_device_map` spans >1 GPU (or `is_parallelizable`+`model_parallel` are set).\n",
    "\n",
    "2. **DataParallel (Trainer wrapping)**\n",
    "   - Replicates the *entire model* onto each visible GPU and splits the batch.\n",
    "   - **Does not require sharding** → it can happen even when `hf_device_map={'': 0}`.\n",
    "\n",
    "**What the Stage 2 fix changes:** forcing `_n_gpu=1` disables Trainer's DataParallel wrapping. If the model is not sharded (e.g. `hf_device_map={'': 0}`), training becomes single-GPU and the other GPU will look idle. If the model *is* sharded, you can still use multiple GPUs via model-parallel sharding even with `_n_gpu=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for DataParallel analysis\n",
    "def analyze_trainer_attributes(model, source_name):\n",
    "    \"\"\"Check attributes that determine if Trainer uses DataParallel.\n",
    "    \n",
    "    HuggingFace Trainer (trainer.py) checks these attributes to decide\n",
    "    whether to wrap the model with DataParallel:\n",
    "    \n",
    "    1. is_parallelizable + model_parallel: If both True, Trainer sets n_gpu=1\n",
    "    2. hf_device_map: If present with multiple GPUs, Trainer sets n_gpu=1\n",
    "    3. is_loaded_in_8bit: If True, Trainer skips DataParallel wrapping\n",
    "    \n",
    "    DataParallel is applied when: n_gpu > 1 AND not is_loaded_in_8bit\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{source_name}]\")\n",
    "    print(f\"  hf_device_map: {getattr(model, 'hf_device_map', None)}\")\n",
    "    print(f\"  is_loaded_in_4bit: {getattr(model, 'is_loaded_in_4bit', False)}\")\n",
    "    print(f\"  is_loaded_in_8bit: {getattr(model, 'is_loaded_in_8bit', False)}\")\n",
    "    print(f\"  is_parallelizable: {getattr(model, 'is_parallelizable', False)}\")\n",
    "    print(f\"  model_parallel: {getattr(model, 'model_parallel', False)}\")\n",
    "\n",
    "def would_trainer_use_dataparallel(model, n_gpu=2):\n",
    "    \"\"\"Simulate Trainer's logic from trainer.py lines 503-624.\n",
    "    \n",
    "    Returns (would_use_dp, is_model_parallel, effective_n_gpu)\n",
    "    \"\"\"\n",
    "    is_model_parallel = False\n",
    "\n",
    "    # Check 1: Direct model parallelism flags\n",
    "    if getattr(model, \"is_parallelizable\", False) and getattr(model, \"model_parallel\", False):\n",
    "        is_model_parallel = True\n",
    "\n",
    "    # Check 2: Device map with multiple GPUs\n",
    "    hf_device_map = getattr(model, \"hf_device_map\", None)\n",
    "    if hf_device_map is not None:\n",
    "        devices = [d for d in set(hf_device_map.values()) if d not in [\"cpu\", \"disk\"]]\n",
    "        if len(devices) > 1:\n",
    "            is_model_parallel = True\n",
    "\n",
    "    # Trainer sets n_gpu=1 if model_parallel detected\n",
    "    effective_n_gpu = 1 if is_model_parallel else n_gpu\n",
    "\n",
    "    # DataParallel condition: n_gpu > 1 AND not 8-bit\n",
    "    would_use_dp = effective_n_gpu > 1 and not getattr(model, \"is_loaded_in_8bit\", False)\n",
    "\n",
    "    return would_use_dp, is_model_parallel, effective_n_gpu\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7: Compare Model Attributes (HuggingFace vs Checkpoint)\n",
    "\n",
    "Let's check the model attributes that Trainer uses to decide whether to apply DataParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 7: Compare Model Attributes (HuggingFace vs Checkpoint)\n",
      "============================================================\n",
      "\n",
      "PyTorch sees 2 GPU(s) in this process.\n",
      "\n",
      "Loading HuggingFace model with device_map='balanced'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6ef692a94d4e9f91c59dbe3264fe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HuggingFace (4-bit, device_map='balanced')]\n",
      "  hf_device_map: {'': 0}\n",
      "  is_loaded_in_4bit: True\n",
      "  is_loaded_in_8bit: False\n",
      "  is_parallelizable: False\n",
      "  model_parallel: False\n",
      "\n",
      "  Would Trainer use DataParallel? True\n",
      "  Is model parallel detected? False\n",
      "  Effective n_gpu: 2\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Loading Checkpoint model with device_map='balanced'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573b217c49fb4674acbee8d1e5673683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint (4-bit, device_map='balanced')]\n",
      "  hf_device_map: {'': 0}\n",
      "  is_loaded_in_4bit: True\n",
      "  is_loaded_in_8bit: False\n",
      "  is_parallelizable: False\n",
      "  model_parallel: False\n",
      "\n",
      "  Would Trainer use DataParallel? True\n",
      "  Is model parallel detected? False\n",
      "  Effective n_gpu: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 7: Compare Model Attributes (HuggingFace vs Checkpoint)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load HuggingFace model with device_map (simulating what works)\n",
    "print(f\"\\nPyTorch sees {torch.cuda.device_count()} GPU(s) in this process.\")\n",
    "print(\"\\nLoading HuggingFace model with device_map='balanced'...\")\n",
    "model_hf = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    HUGGINGFACE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "analyze_trainer_attributes(model_hf, \"HuggingFace (4-bit, device_map='balanced')\")\n",
    "would_dp, is_mp, eff_ngpu = would_trainer_use_dataparallel(model_hf, n_gpu=torch.cuda.device_count())\n",
    "print(f\"\\n  Would Trainer use DataParallel? {would_dp}\")\n",
    "print(f\"  Is model parallel detected? {is_mp}\")\n",
    "print(f\"  Effective n_gpu: {eff_ngpu}\")\n",
    "\n",
    "# Clean up\n",
    "del model_hf\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load Checkpoint model with device_map (simulating what fails)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nLoading Checkpoint model with device_map='balanced'...\")\n",
    "model_ckpt = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    STAGE1_CHECKPOINT,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "analyze_trainer_attributes(model_ckpt, \"Checkpoint (4-bit, device_map='balanced')\")\n",
    "would_dp, is_mp, eff_ngpu = would_trainer_use_dataparallel(model_ckpt, n_gpu=torch.cuda.device_count())\n",
    "print(f\"\\n  Would Trainer use DataParallel? {would_dp}\")\n",
    "print(f\"  Is model parallel detected? {is_mp}\")\n",
    "print(f\"  Effective n_gpu: {eff_ngpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 8: Check Device Map Details\n",
    "\n",
    "Let's examine what devices are in the device_map to understand if Trainer should detect model parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 8: Device Map Details\n",
      "============================================================\n",
      "\n",
      "Device map has 1 entries\n",
      "Device distribution: {0: 1}\n",
      "\n",
      "First 5 entries:\n",
      "  : 0\n",
      "\n",
      "Last 5 entries:\n",
      "  : 0\n",
      "\n",
      "GPU devices in device_map: [0]\n",
      "Number of GPU devices: 1\n",
      "\n",
      "Should Trainer detect model parallelism? False\n",
      "\n",
      "GPU memory cleaned up.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 8: Device Map Details\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check device map structure\n",
    "hf_device_map = getattr(model_ckpt, \"hf_device_map\", None)\n",
    "\n",
    "if hf_device_map:\n",
    "    print(f\"\\nDevice map has {len(hf_device_map)} entries\")\n",
    "    \n",
    "    # Count devices\n",
    "    device_counts = Counter(hf_device_map.values())\n",
    "    print(f\"Device distribution: {dict(device_counts)}\")\n",
    "    \n",
    "    # Show first/last few entries\n",
    "    items = list(hf_device_map.items())\n",
    "    print(f\"\\nFirst 5 entries:\")\n",
    "    for k, v in items[:5]:\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"\\nLast 5 entries:\")\n",
    "    for k, v in items[-5:]:\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Check GPU devices (excluding cpu/disk)\n",
    "    gpu_devices = [d for d in set(hf_device_map.values()) if d not in [\"cpu\", \"disk\"]]\n",
    "    print(f\"\\nGPU devices in device_map: {gpu_devices}\")\n",
    "    print(f\"Number of GPU devices: {len(gpu_devices)}\")\n",
    "    print(f\"\\nShould Trainer detect model parallelism? {len(gpu_devices) > 1}\")\n",
    "else:\n",
    "    print(\"No hf_device_map attribute found!\")\n",
    "\n",
    "# Clean up\n",
    "del model_ckpt\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nGPU memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Test 8 output\n",
    "\n",
    "- `hf_device_map={'': 0}` means `device_map` did **not** shard the model; everything lives on GPU 0.\n",
    "- With 2 visible GPUs, Trainer would normally wrap **DataParallel** (replicate the model onto both GPUs) unless model-parallel is detected.\n",
    "- The Stage 2 fix disables that wrapping, so you should expect **one GPU doing compute** unless you force sharding or switch to DDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 9: Check PEFT Wrapping Effect\n",
    "\n",
    "SFTTrainer internally wraps the model with PEFT. Let's check if PEFT preserves the attributes that Trainer uses for DataParallel detection.\n",
    "\n",
    "**Hypothesis:** PEFT wrapping might lose `hf_device_map` or `is_parallelizable` attributes, causing Trainer to incorrectly apply DataParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 9: PEFT Wrapping Effect on Model Attributes\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aea7b82c70a4766b8a3fade6adbaa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "BEFORE PEFT Wrapping:\n",
      "----------------------------------------\n",
      "\n",
      "[Base Model]\n",
      "  hf_device_map: {'': 0}\n",
      "  is_loaded_in_4bit: True\n",
      "  is_loaded_in_8bit: False\n",
      "  is_parallelizable: False\n",
      "  model_parallel: False\n",
      "  Would use DataParallel: True\n",
      "\n",
      "Applying PEFT (LoRA)...\n",
      "\n",
      "----------------------------------------\n",
      "AFTER PEFT Wrapping:\n",
      "----------------------------------------\n",
      "\n",
      "[PEFT Model]\n",
      "  hf_device_map: {'': 0}\n",
      "  is_loaded_in_4bit: True\n",
      "  is_loaded_in_8bit: False\n",
      "  is_parallelizable: False\n",
      "  model_parallel: False\n",
      "  Would use DataParallel: True\n",
      "\n",
      "----------------------------------------\n",
      "Summary:\n",
      "----------------------------------------\n",
      "Base model hf_device_map: Present (gpu_devices=1)\n",
      "PEFT model hf_device_map: Present (gpu_devices=1)\n",
      "\n",
      "PEFT preserves hf_device_map, but it is single-GPU -> Trainer will NOT treat this as model-parallel\n",
      "\n",
      "GPU memory cleaned up.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEST 9: PEFT Wrapping Effect on Model Attributes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Reload checkpoint model\n",
    "print(\"\\nLoading checkpoint model...\")\n",
    "model_ckpt = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    STAGE1_CHECKPOINT,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Check attributes BEFORE PEFT wrapping\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"BEFORE PEFT Wrapping:\")\n",
    "print(\"-\"*40)\n",
    "analyze_trainer_attributes(model_ckpt, \"Base Model\")\n",
    "would_dp, is_mp, eff_ngpu = would_trainer_use_dataparallel(model_ckpt, n_gpu=torch.cuda.device_count())\n",
    "print(f\"  Would use DataParallel: {would_dp}\")\n",
    "\n",
    "# Define LoRA config (same as r1-llm-only)\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT\n",
    "print(\"\\nApplying PEFT (LoRA)...\")\n",
    "peft_model = get_peft_model(model_ckpt, lora_config)\n",
    "\n",
    "# Check attributes AFTER PEFT wrapping\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"AFTER PEFT Wrapping:\")\n",
    "print(\"-\"*40)\n",
    "analyze_trainer_attributes(peft_model, \"PEFT Model\")\n",
    "would_dp, is_mp, eff_ngpu = would_trainer_use_dataparallel(peft_model, n_gpu=torch.cuda.device_count())\n",
    "print(f\"  Would use DataParallel: {would_dp}\")\n",
    "\n",
    "# Check if hf_device_map is preserved\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"Summary:\")\n",
    "print(\"-\"*40)\n",
    "def _count_gpu_devices(device_map):\n",
    "    if not device_map:\n",
    "        return 0\n",
    "    return len([d for d in set(device_map.values()) if d not in [\"cpu\", \"disk\"]])\n",
    "\n",
    "base_device_map = getattr(model_ckpt, \"hf_device_map\", None)\n",
    "peft_device_map = getattr(peft_model, \"hf_device_map\", None)\n",
    "base_gpu_count = _count_gpu_devices(base_device_map)\n",
    "peft_gpu_count = _count_gpu_devices(peft_device_map)\n",
    "\n",
    "print(f\"Base model hf_device_map: {'Present' if base_device_map else 'MISSING'} (gpu_devices={base_gpu_count})\")\n",
    "print(f\"PEFT model hf_device_map: {'Present' if peft_device_map else 'MISSING'} (gpu_devices={peft_gpu_count})\")\n",
    "\n",
    "if peft_device_map and peft_gpu_count > 1:\n",
    "    print(\"\\nPEFT preserves a multi-GPU hf_device_map -> Trainer should detect model-parallel and avoid DataParallel\")\n",
    "elif peft_device_map:\n",
    "    print(\"\\nPEFT preserves hf_device_map, but it is single-GPU -> Trainer will NOT treat this as model-parallel\")\n",
    "else:\n",
    "    print(\"\\n⚠️ PEFT loses hf_device_map -> This could cause DataParallel issues!\")\n",
    "\n",
    "# Clean up\n",
    "del model_ckpt, peft_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nGPU memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Root Cause and Fix\n",
    "\n",
    "### Root Cause Analysis\n",
    "\n",
    "The dtype mismatch error occurs because:\n",
    "\n",
    "1. **Stage 2 is wrapped with DataParallel** while Stage 1 is not\n",
    "   - Evidence: Stage 1 shows `0/136` steps, Stage 2 shows `0/68` steps (half!)\n",
    "   - The traceback includes `torch/nn/parallel/data_parallel.py`\n",
    "\n",
    "2. **DataParallel + AMP/autocast incompatibility**\n",
    "   - DataParallel uses worker threads for each GPU replica\n",
    "   - AMP/autocast context is **thread-local** - not inherited by worker threads\n",
    "   - In k-bit training, some layers (norms) run in fp32 for stability\n",
    "   - Without autocast in worker threads, fp32 activations flow to bf16 `lm_head` → crash\n",
    "\n",
    "3. **Why Stage 2 got DataParallel**\n",
    "   - The process sees 2 GPUs (`n_gpu=2`), and Trainer wraps DataParallel unless it detects model-parallelism.\n",
    "   - Model-parallelism is only detected if `hf_device_map` spans >1 GPU (or `is_parallelizable`+`model_parallel` are set).\n",
    "   - If `device_map` produces `hf_device_map={'': 0}` (single-GPU), Trainer will choose DataParallel by default.\n",
    "\n",
    "### The Fix (Implemented in `scripts/train_recipe.py`)\n",
    "\n",
    "```python\n",
    "# Fix for two-stage training: When loading from a checkpoint with multi-GPU,\n",
    "# force Trainer to behave as single-GPU to prevent DataParallel wrapping.\n",
    "# Note: this only prevents Trainer's DataParallel; it does not force model sharding.\n",
    "if checkpoint_path and torch.cuda.device_count() > 1:\n",
    "    training_args_recipe._n_gpu = 1\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Setting `_n_gpu=1` tells Trainer \"don't use DataParallel\"\n",
    "- If the model is sharded (multi-GPU `hf_device_map`), it can still use multiple GPUs via `device_map`\n",
    "- If the model fits on one GPU (`hf_device_map={'': 0}`), Stage 2 will run single-GPU (other GPU mostly idle)\n",
    "- This fix is conditional - only applies when loading from checkpoint\n",
    "- Other recipes (r1, r2, r4) that load from HuggingFace are unaffected\n",
    "\n",
    "### Verified Working (2025-12-15)\n",
    "\n",
    "```\n",
    "Stage 1: 136 steps completed (vision encoder training)\n",
    "Stage 2: 136 steps completed (LLM LoRA training from checkpoint)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
