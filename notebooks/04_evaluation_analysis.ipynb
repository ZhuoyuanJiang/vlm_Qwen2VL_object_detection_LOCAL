{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec49aaa",
   "metadata": {},
   "source": [
    "# 04 - Evaluation Analysis\n",
    "\n",
    "This notebook provides post-training evaluation and analysis of the fine-tuned Qwen2-VL model\n",
    "for nutrition table detection.\n",
    "\n",
    "**What this notebook covers:**\n",
    "1. Load the fine-tuned model (with LoRA adapters)\n",
    "2. Evaluate on the validation dataset using IoU metrics\n",
    "3. Compare performance with the base model (no fine-tuning)\n",
    "4. Visualize IoU distribution and predictions\n",
    "5. Analyze failure cases\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed training (have saved LoRA adapters)\n",
    "- `src/` modules installed (run from project root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a1b32",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e1f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/miniconda3/envs/vlm_Qwen2VL_object_detection/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory to path for src imports\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Project imports\n",
    "from src.training.evaluation import evaluate_model, print_evaluation_results, compare_models\n",
    "from src.models.inference import parse_qwen_bbox_output\n",
    "from src.utils.visualization import visualize_bbox_on_image, visualize_ground_truth_bbox\n",
    "from src.utils.gpu import clear_memory\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39ab68",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set paths to your trained model and configure evaluation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502c063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model v1 (all tokens): /ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-lora\n",
      "Model v3 (assistant-only): /ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-lora-assistantonly\n",
      "Base model: Qwen/Qwen2-VL-7B-Instruct\n",
      "Evaluation samples: 50\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Update these paths based on your training output\n",
    "\n",
    "# Path to saved LoRA adapters (from training)\n",
    "# Model v1: Trained on all tokens (original collator)\n",
    "ADAPTER_PATH_V1 = \"/ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-lora\"\n",
    "\n",
    "# Model v3: Trained with assistant-only collator (collate_fn_fixed_3)\n",
    "ADAPTER_PATH_V3 = \"/ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-lora-assistantonly\"\n",
    "\n",
    "# Base model ID\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# Evaluation settings\n",
    "NUM_EVAL_SAMPLES = 50  # Number of samples to evaluate (set lower for quick tests)\n",
    "DEVICE_MAP = \"balanced\"  # \"balanced\" for multi-GPU, \"auto\" for single GPU\n",
    "\n",
    "# Visualization settings\n",
    "NUM_VISUALIZATION_SAMPLES = 5  # Number of predictions to visualize\n",
    "\n",
    "print(f\"Model v1 (all tokens): {ADAPTER_PATH_V1}\")\n",
    "print(f\"Model v3 (assistant-only): {ADAPTER_PATH_V3}\")\n",
    "print(f\"Base model: {BASE_MODEL_ID}\")\n",
    "print(f\"Evaluation samples: {NUM_EVAL_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7be2e0",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce74dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenfoodfacts/nutrition-table-detection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get evaluation split\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# or 'validation' depending on dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eval_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_dataset\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm_Qwen2VL_object_detection/lib/python3.10/site-packages/datasets/dataset_dict.py:82\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     86\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "# Load the OpenFoodFacts nutrition table detection dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"openfoodfacts/nutrition-table-detection\")\n",
    "\n",
    "# Get evaluation split\n",
    "eval_dataset = dataset['validation']  # or 'validation' depending on dataset\n",
    "\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "print(f\"Dataset features: {eval_dataset.features}\")\n",
    "\n",
    "# Preview a sample\n",
    "sample = eval_dataset[0]\n",
    "print(f\"\\nSample keys: {sample.keys()}\")\n",
    "print(f\"Image size: {sample['image'].size}\")\n",
    "print(f\"Number of objects: {len(sample['objects']['bbox'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be688596",
   "metadata": {},
   "source": [
    "## 4. Load Fine-tuned Models\n",
    "\n",
    "Load both trained models for comparison:\n",
    "- **Model v1**: Trained on all tokens (original collator)\n",
    "- **Model v3**: Trained with assistant-only collator (collate_fn_fixed_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "# --- Load Model v1 (All Tokens) ---\n",
    "print(\"Loading Model v1 (trained on all tokens)...\")\n",
    "\n",
    "model_v1 = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=DEVICE_MAP,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model_v1 = PeftModel.from_pretrained(\n",
    "    model_v1,\n",
    "    ADAPTER_PATH_V1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "processor_v1 = Qwen2VLProcessor.from_pretrained(ADAPTER_PATH_V1)\n",
    "\n",
    "print(f\"  Loaded from: {ADAPTER_PATH_V1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62312e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Model v3 (Assistant-Only) ---\n",
    "print(\"\\nLoading Model v3 (trained with assistant-only collator)...\")\n",
    "\n",
    "model_v3 = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=DEVICE_MAP,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model_v3 = PeftModel.from_pretrained(\n",
    "    model_v3,\n",
    "    ADAPTER_PATH_V3,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "processor_v3 = Qwen2VLProcessor.from_pretrained(ADAPTER_PATH_V3)\n",
    "\n",
    "print(f\"  Loaded from: {ADAPTER_PATH_V3}\")\n",
    "print(\"\\nBoth models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54600737",
   "metadata": {},
   "source": [
    "## 5. Evaluate Both Fine-tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dab62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Model v1 ---\n",
    "print(f\"\\nEvaluating Model v1 (all tokens) on {NUM_EVAL_SAMPLES} samples...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "metrics_v1, ious_v1 = evaluate_model(\n",
    "    model_v1,\n",
    "    processor_v1,\n",
    "    eval_dataset,\n",
    "    num_samples=NUM_EVAL_SAMPLES\n",
    ")\n",
    "\n",
    "print_evaluation_results(metrics_v1, \"Model v1 (All Tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Model v3 ---\n",
    "print(f\"\\nEvaluating Model v3 (assistant-only) on {NUM_EVAL_SAMPLES} samples...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "metrics_v3, ious_v3 = evaluate_model(\n",
    "    model_v3,\n",
    "    processor_v3,\n",
    "    eval_dataset,\n",
    "    num_samples=NUM_EVAL_SAMPLES\n",
    ")\n",
    "\n",
    "print_evaluation_results(metrics_v3, \"Model v3 (Assistant-Only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare v1 vs v3 ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Model v1 (All Tokens) vs Model v3 (Assistant-Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "v1_better = metrics_v1['mean_iou'] > metrics_v3['mean_iou']\n",
    "diff = abs(metrics_v1['mean_iou'] - metrics_v3['mean_iou'])\n",
    "\n",
    "print(f\"\\nMean IoU:\")\n",
    "print(f\"  Model v1: {metrics_v1['mean_iou']:.4f}\")\n",
    "print(f\"  Model v3: {metrics_v3['mean_iou']:.4f}\")\n",
    "print(f\"  Winner: {'v1' if v1_better else 'v3'} (by {diff:.4f})\")\n",
    "\n",
    "print(f\"\\nDetection Rate:\")\n",
    "print(f\"  Model v1: {metrics_v1['detection_rate']:.2%}\")\n",
    "print(f\"  Model v3: {metrics_v3['detection_rate']:.2%}\")\n",
    "\n",
    "print(f\"\\nIoU > 0.5:\")\n",
    "print(f\"  Model v1: {metrics_v1['iou_threshold_0.5']:.2%}\")\n",
    "print(f\"  Model v3: {metrics_v3['iou_threshold_0.5']:.2%}\")\n",
    "\n",
    "# For backward compatibility, set the best model as \"finetuned\"\n",
    "if v1_better:\n",
    "    model_finetuned = model_v1\n",
    "    processor_finetuned = processor_v1\n",
    "    metrics_finetuned = metrics_v1\n",
    "    ious_finetuned = ious_v1\n",
    "    print(\"\\n--> Using Model v1 for visualizations (better performance)\")\n",
    "else:\n",
    "    model_finetuned = model_v3\n",
    "    processor_finetuned = processor_v3\n",
    "    metrics_finetuned = metrics_v3\n",
    "    ious_finetuned = ious_v3\n",
    "    print(\"\\n--> Using Model v3 for visualizations (better performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd38055",
   "metadata": {},
   "source": [
    "## 6. Load and Evaluate Base Model (Optional Comparison)\n",
    "\n",
    "Compare with the base model (without fine-tuning) to measure improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to run base model comparison (takes additional time)\n",
    "RUN_BASE_COMPARISON = True\n",
    "\n",
    "if RUN_BASE_COMPARISON:\n",
    "    print(\"\\nLoading base model for comparison...\")\n",
    "\n",
    "    # Clear some memory first\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=DEVICE_MAP,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base_processor = Qwen2VLProcessor.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "    print(f\"\\nEvaluating base model on {NUM_EVAL_SAMPLES} samples...\")\n",
    "    metrics_base, ious_base = evaluate_model(\n",
    "        base_model,\n",
    "        base_processor,\n",
    "        eval_dataset,\n",
    "        num_samples=NUM_EVAL_SAMPLES\n",
    "    )\n",
    "\n",
    "    print_evaluation_results(metrics_base, \"Base Model (No Fine-tuning)\")\n",
    "\n",
    "    # Compare models\n",
    "    compare_models(metrics_base, metrics_finetuned)\n",
    "\n",
    "    # Clean up base model\n",
    "    del base_model\n",
    "    del base_processor\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipping base model comparison (set RUN_BASE_COMPARISON=True to enable)\")\n",
    "    metrics_base = None\n",
    "    ious_base = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910c40a",
   "metadata": {},
   "source": [
    "## 7. Visualize IoU Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d459dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare v1 vs v3 IoU distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# IoU histogram for Model v1\n",
    "axes[0].hist(ious_v1, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(metrics_v1['mean_iou'], color='red', linestyle='--',\n",
    "                label=f\"Mean: {metrics_v1['mean_iou']:.3f}\")\n",
    "axes[0].axvline(0.5, color='green', linestyle=':', alpha=0.7, label='IoU=0.5')\n",
    "axes[0].set_xlabel('IoU Score')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_title('Model v1 (All Tokens) - IoU Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU histogram for Model v3\n",
    "axes[1].hist(ious_v3, bins=20, edgecolor='black', alpha=0.7, color='darkorange')\n",
    "axes[1].axvline(metrics_v3['mean_iou'], color='red', linestyle='--',\n",
    "                label=f\"Mean: {metrics_v3['mean_iou']:.3f}\")\n",
    "axes[1].axvline(0.5, color='green', linestyle=':', alpha=0.7, label='IoU=0.5')\n",
    "axes[1].set_xlabel('IoU Score')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_title('Model v3 (Assistant-Only) - IoU Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison bar chart: v1 vs v3 vs base (if available)\n",
    "metrics_names = ['Mean IoU', 'Median IoU', 'Det. Rate', 'IoU>0.5', 'IoU>0.7']\n",
    "v1_values = [\n",
    "    metrics_v1['mean_iou'],\n",
    "    metrics_v1['median_iou'],\n",
    "    metrics_v1['detection_rate'],\n",
    "    metrics_v1['iou_threshold_0.5'],\n",
    "    metrics_v1['iou_threshold_0.7']\n",
    "]\n",
    "v3_values = [\n",
    "    metrics_v3['mean_iou'],\n",
    "    metrics_v3['median_iou'],\n",
    "    metrics_v3['detection_rate'],\n",
    "    metrics_v3['iou_threshold_0.5'],\n",
    "    metrics_v3['iou_threshold_0.7']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.25\n",
    "\n",
    "if metrics_base:\n",
    "    base_values = [\n",
    "        metrics_base['mean_iou'],\n",
    "        metrics_base['median_iou'],\n",
    "        metrics_base['detection_rate'],\n",
    "        metrics_base['iou_threshold_0.5'],\n",
    "        metrics_base['iou_threshold_0.7']\n",
    "    ]\n",
    "    axes[2].bar(x - width, base_values, width, label='Base', alpha=0.8, color='gray')\n",
    "    axes[2].bar(x, v1_values, width, label='v1 (All)', alpha=0.8, color='steelblue')\n",
    "    axes[2].bar(x + width, v3_values, width, label='v3 (Asst)', alpha=0.8, color='darkorange')\n",
    "else:\n",
    "    axes[2].bar(x - width/2, v1_values, width, label='v1 (All Tokens)', alpha=0.8, color='steelblue')\n",
    "    axes[2].bar(x + width/2, v3_values, width, label='v3 (Assistant-Only)', alpha=0.8, color='darkorange')\n",
    "\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_title('Model Comparison')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(metrics_names, rotation=45, ha='right')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d6743",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions\n",
    "\n",
    "Show example predictions comparing ground truth vs model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55992981",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def run_inference_single(model, processor, image, prompt=\"Detect the bounding box of the nutrition table.\"):\n",
    "    \"\"\"Run inference on a single image and return parsed bbox.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "\n",
    "    generated_ids_trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "    return output_text, parse_qwen_bbox_output(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nVisualizing {NUM_VISUALIZATION_SAMPLES} predictions...\")\n",
    "\n",
    "fig, axes = plt.subplots(NUM_VISUALIZATION_SAMPLES, 2, figsize=(12, 5*NUM_VISUALIZATION_SAMPLES))\n",
    "\n",
    "for idx in range(NUM_VISUALIZATION_SAMPLES):\n",
    "    example = eval_dataset[idx]\n",
    "    image = example['image']\n",
    "    gt_bbox = example['objects']['bbox'][0]\n",
    "    gt_category = example['objects']['category_name'][0]\n",
    "\n",
    "    # Run inference\n",
    "    output_text, parsed_bbox = run_inference_single(model_finetuned, processor_finetuned, image)\n",
    "\n",
    "    # Ground truth visualization\n",
    "    img_gt = visualize_ground_truth_bbox(\n",
    "        image,\n",
    "        [gt_bbox],\n",
    "        [gt_category],\n",
    "        format='openfoodfacts'\n",
    "    )\n",
    "\n",
    "    # Prediction visualization\n",
    "    if parsed_bbox:\n",
    "        img_pred = visualize_bbox_on_image(image, parsed_bbox, normalize_coords=True)\n",
    "        pred_label = f\"Predicted: {parsed_bbox.get('object', 'detected')}\"\n",
    "    else:\n",
    "        img_pred = image.copy()\n",
    "        pred_label = \"No detection\"\n",
    "\n",
    "    # Plot\n",
    "    axes[idx, 0].imshow(img_gt)\n",
    "    axes[idx, 0].set_title(f\"Sample {idx+1} - Ground Truth: {gt_category}\")\n",
    "    axes[idx, 0].axis('off')\n",
    "\n",
    "    axes[idx, 1].imshow(img_pred)\n",
    "    axes[idx, 1].set_title(f\"Sample {idx+1} - {pred_label}\")\n",
    "    axes[idx, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6c641",
   "metadata": {},
   "source": [
    "## 9. Analyze Failure Cases\n",
    "\n",
    "Look at samples with low IoU to understand where the model struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find failure cases (IoU < 0.5)\n",
    "failure_threshold = 0.5\n",
    "failure_indices = [i for i, iou in enumerate(ious_finetuned) if iou < failure_threshold]\n",
    "\n",
    "print(f\"\\nFailure Analysis (IoU < {failure_threshold}):\")\n",
    "print(f\"  Total failures: {len(failure_indices)} / {len(ious_finetuned)} ({100*len(failure_indices)/len(ious_finetuned):.1f}%)\")\n",
    "\n",
    "if failure_indices:\n",
    "    print(f\"\\n  Failure case IoU scores:\")\n",
    "    for i, idx in enumerate(failure_indices[:10]):  # Show first 10\n",
    "        print(f\"    Sample {idx}: IoU = {ious_finetuned[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few failure cases\n",
    "NUM_FAILURE_VIS = min(3, len(failure_indices))\n",
    "\n",
    "if NUM_FAILURE_VIS > 0:\n",
    "    print(f\"\\nVisualizing {NUM_FAILURE_VIS} failure cases...\")\n",
    "\n",
    "    fig, axes = plt.subplots(NUM_FAILURE_VIS, 2, figsize=(12, 5*NUM_FAILURE_VIS))\n",
    "    if NUM_FAILURE_VIS == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, idx in enumerate(failure_indices[:NUM_FAILURE_VIS]):\n",
    "        example = eval_dataset[idx]\n",
    "        image = example['image']\n",
    "        gt_bbox = example['objects']['bbox'][0]\n",
    "        gt_category = example['objects']['category_name'][0]\n",
    "\n",
    "        # Run inference\n",
    "        output_text, parsed_bbox = run_inference_single(model_finetuned, processor_finetuned, image)\n",
    "\n",
    "        # Ground truth\n",
    "        img_gt = visualize_ground_truth_bbox(image, [gt_bbox], [gt_category], format='openfoodfacts')\n",
    "\n",
    "        # Prediction\n",
    "        if parsed_bbox:\n",
    "            img_pred = visualize_bbox_on_image(image, parsed_bbox, normalize_coords=True)\n",
    "        else:\n",
    "            img_pred = image.copy()\n",
    "\n",
    "        axes[i, 0].imshow(img_gt)\n",
    "        axes[i, 0].set_title(f\"Failure {i+1} (idx={idx}) - Ground Truth\\nIoU: {ious_finetuned[idx]:.3f}\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(img_pred)\n",
    "        axes[i, 1].set_title(f\"Failure {i+1} - Model Prediction\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No failure cases to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11c522",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Print final summary of evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nEvaluated on: {NUM_EVAL_SAMPLES} samples\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"{'Model':<30} {'Mean IoU':>10} {'Det Rate':>10} {'IoU>0.5':>10} {'IoU>0.7':>10}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if metrics_base:\n",
    "    print(f\"{'Base (No Fine-tuning)':<30} {metrics_base['mean_iou']:>10.4f} {metrics_base['detection_rate']:>10.2%} {metrics_base['iou_threshold_0.5']:>10.2%} {metrics_base['iou_threshold_0.7']:>10.2%}\")\n",
    "\n",
    "print(f\"{'v1 (All Tokens)':<30} {metrics_v1['mean_iou']:>10.4f} {metrics_v1['detection_rate']:>10.2%} {metrics_v1['iou_threshold_0.5']:>10.2%} {metrics_v1['iou_threshold_0.7']:>10.2%}\")\n",
    "print(f\"{'v3 (Assistant-Only)':<30} {metrics_v3['mean_iou']:>10.4f} {metrics_v3['detection_rate']:>10.2%} {metrics_v3['iou_threshold_0.5']:>10.2%} {metrics_v3['iou_threshold_0.7']:>10.2%}\")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Determine winner\n",
    "v1_better = metrics_v1['mean_iou'] > metrics_v3['mean_iou']\n",
    "winner = \"v1 (All Tokens)\" if v1_better else \"v3 (Assistant-Only)\"\n",
    "diff = abs(metrics_v1['mean_iou'] - metrics_v3['mean_iou'])\n",
    "\n",
    "print(f\"\\nBest Model: {winner}\")\n",
    "print(f\"  Margin: {diff:.4f} IoU difference\")\n",
    "\n",
    "if metrics_base:\n",
    "    best_iou = max(metrics_v1['mean_iou'], metrics_v3['mean_iou'])\n",
    "    improvement = best_iou - metrics_base['mean_iou']\n",
    "    print(f\"\\nImprovement over Base Model:\")\n",
    "    print(f\"  Mean IoU: +{improvement:.4f} ({100*improvement/max(metrics_base['mean_iou'], 0.001):.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22940f1f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on these results, you might consider:\n",
    "\n",
    "1. **If IoU is low**:\n",
    "   - Train for more epochs\n",
    "   - Adjust learning rate\n",
    "   - Check data quality\n",
    "\n",
    "2. **If detection rate is low**:\n",
    "   - Model might not be learning the task format\n",
    "   - Check collator is working correctly\n",
    "\n",
    "3. **If both are good**:\n",
    "   - Merge LoRA weights for faster inference\n",
    "   - Deploy with vLLM or similar"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
