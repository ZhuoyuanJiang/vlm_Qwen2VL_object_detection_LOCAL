{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56965bf9",
   "metadata": {},
   "source": [
    "# Generate Golden Test Data\n",
    "\n",
    "**Purpose**: Print data at each stage of the pipeline so you can copy-paste into golden tests.\n",
    "\n",
    "**Steps**:\n",
    "1. Run each cell\n",
    "2. Copy the output\n",
    "3. Paste into `tests/test_golden_output.py`\n",
    "\n",
    "**Simple!** output raw dataï¼Œno fancy formattingã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804d54b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b2c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports ready\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our modules\n",
    "from datasets import load_dataset\n",
    "from src.data.dataset import convert_to_conversation_format\n",
    "from src.data.collators import restore_images_in_conversations\n",
    "\n",
    "print(\"âœ… Imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c213d",
   "metadata": {},
   "source": [
    "## Stage 1: Raw Sample from Dataset\n",
    "\n",
    "Load the first training sample - this is what comes directly from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2567fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '0009800892204_1',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>,\n",
       " 'width': 2592,\n",
       " 'height': 1944,\n",
       " 'meta': {'barcode': '0009800892204',\n",
       "  'off_image_id': '1',\n",
       "  'image_url': 'https://static.openfoodfacts.org/images/products/000/980/089/2204/1.jpg'},\n",
       " 'objects': {'bbox': [[0.057098764926195145,\n",
       "    0.014274691231548786,\n",
       "    0.603501558303833,\n",
       "    0.991126537322998]],\n",
       "  'category_id': [0],\n",
       "  'category_name': ['nutrition-table']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset (first sample only)\n",
    "ds = load_dataset(\"openfoodfacts/nutrition-table-detection\", split=\"train[:1]\", streaming=False)\n",
    "raw_sample = ds[0]\n",
    "\n",
    "# Output the raw sample\n",
    "# You'll copy this to understand the raw format\n",
    "raw_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c06ac",
   "metadata": {},
   "source": [
    "### ðŸ“‹ What to copy from above:\n",
    "- Note the keys: `image`, `objects` with `bbox` and `category_name`\n",
    "- Note the bbox format: `[y_min, x_min, y_max, x_max]` normalized [0,1]\n",
    "- Note it's a PIL Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03255763",
   "metadata": {},
   "source": [
    "## Stage 2: After convert_to_conversation_format()\n",
    "\n",
    "This is what the data looks like after preprocessing (with `IMAGE_PLACEHOLDER`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb0cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'image', 'image': 'IMAGE_PLACEHOLDER'},\n",
       "    {'type': 'text',\n",
       "     'text': 'Detect the bounding box of the nutrition table.'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}],\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to conversation format\n",
    "converted = convert_to_conversation_format(raw_sample)\n",
    "\n",
    "# Output the converted sample\n",
    "converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f5970",
   "metadata": {},
   "source": [
    "### ðŸ“‹ What to copy from above:\n",
    "- Copy the entire `converted` dict\n",
    "- Note: `'image'` is still PIL Image\n",
    "- Note: `'messages'` has `'IMAGE_PLACEHOLDER'` string\n",
    "- This is what gets saved to disk (serializable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0effb",
   "metadata": {},
   "source": [
    "## Stage 3: Batch of Samples (2 DIFFERENT samples)\n",
    "\n",
    "This is what a real batch looks like - each item is a DIFFERENT sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86948b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'messages': [{'role': 'system',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
       "   {'role': 'user',\n",
       "    'content': [{'type': 'image', 'image': 'IMAGE_PLACEHOLDER'},\n",
       "     {'type': 'text',\n",
       "      'text': 'Detect the bounding box of the nutrition table.'}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}],\n",
       "  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
       "   {'role': 'user',\n",
       "    'content': [{'type': 'image', 'image': 'IMAGE_PLACEHOLDER'},\n",
       "     {'type': 'text',\n",
       "      'text': 'Detect the bounding box of the nutrition table.'}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>'}]}],\n",
       "  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408>}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load 2 different samples for real batch\n",
    "ds_batch = load_dataset(\"openfoodfacts/nutrition-table-detection\", split=\"train[:2]\", streaming=False)\n",
    "\n",
    "# Convert both samples\n",
    "sample_0 = convert_to_conversation_format(ds_batch[0])\n",
    "sample_1 = convert_to_conversation_format(ds_batch[1])\n",
    "\n",
    "# Create real batch with DIFFERENT samples\n",
    "batch = [sample_0, sample_1]\n",
    "\n",
    "# Output the batch\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8e4bb",
   "metadata": {},
   "source": [
    "### ðŸ“‹ What to copy from above:\n",
    "- It's a list of 2 dicts\n",
    "- Each sample is DIFFERENT (different images, different bboxes)\n",
    "- Each dict has 'messages' (with 'IMAGE_PLACEHOLDER') and 'image' (PIL Image)\n",
    "- This is what gets passed to collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8062c9f9",
   "metadata": {},
   "source": [
    "## Stage 4: After restore_images_in_conversations()\n",
    "\n",
    "This is the format that goes into `apply_chat_template` (PIL Images restored).\n",
    "\n",
    "- **Input**: messages with 'IMAGE_PLACEHOLDER' (string)\n",
    "- **Output**: messages with actual `<PIL.Image>` objects\n",
    "\n",
    "This output format is what your instruction shows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63778818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'image',\n",
       "     'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>},\n",
       "    {'type': 'text',\n",
       "     'text': 'Detect the bounding box of the nutrition table.'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}],\n",
       " [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'image',\n",
       "     'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408>},\n",
       "    {'type': 'text',\n",
       "     'text': 'Detect the bounding box of the nutrition table.'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>'}]}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract messages and images\n",
    "messages_list = [sample['messages'] for sample in batch]\n",
    "images_list = [sample['image'] for sample in batch]\n",
    "\n",
    "# Restore images in messages\n",
    "restored = restore_images_in_conversations(messages_list, images_list)\n",
    "\n",
    "# Output the restored format\n",
    "restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4470a9",
   "metadata": {},
   "source": [
    "### ðŸ“‹ What to copy from above:\n",
    "- Copy the structure of `restored`\n",
    "- Note: Now has actual PIL Images, not `'IMAGE_PLACEHOLDER'`\n",
    "- This is what goes into `processor.apply_chat_template()`\n",
    "- **CRITICAL**: Images must be PIL.Image objects at this stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a7e90",
   "metadata": {},
   "source": [
    "## Summary - What You've Generated\n",
    "\n",
    "You now have outputs for all stages:\n",
    "\n",
    "1. **Raw sample** - from dataset\n",
    "2. **Converted** - after `convert_to_conversation_format()`\n",
    "3. **Batch** - list of samples\n",
    "4. **Restored** - after `restore_images_in_conversations()`\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Copy outputs from cells above\n",
    "2. Paste into `tests/test_golden_output.py`:\n",
    "   - Cell 2 output â†’ `test_convert_to_conversation_format_golden_output()`\n",
    "   - Cell 4 output â†’ `test_restore_images_golden_output()`\n",
    "3. Run the test to verify format stays consistent"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
