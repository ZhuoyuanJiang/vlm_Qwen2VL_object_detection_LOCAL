{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Qwen2-VL Fine-tuning\n",
    "\n",
    "This notebook documents and visualizes the data preprocessing pipeline for fine-tuning Qwen2-VL on the nutrition table detection task.\n",
    "\n",
    "**Purpose:**\n",
    "- Convert raw dataset samples to Qwen2-VL conversation format\n",
    "- Understand the data transformations step-by-step\n",
    "- Debug and visualize the preprocessing pipeline\n",
    "\n",
    "**Key Transformations:**\n",
    "1. OpenFoodFacts bbox format `[y_min, x_min, y_max, x_max]` in [0,1] → Qwen2-VL format `(x1,y1),(x2,y2)` in [0,1000)\n",
    "2. Dataset samples → OpenAI conversation format (system, user, assistant)\n",
    "3. PIL images handled via IMAGE_PLACEHOLDER pattern for HuggingFace dataset serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/miniconda3/envs/vlm_Qwen2VL_object_detection/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# HuggingFace imports\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "print(\"Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b0a9eed65e48be94f776e7517bd311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e1d5bdbd0545f2a5851ad208f25a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/291M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee0359777ee4cffbd5286e7c4fecd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/285M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a22af5ac44047bb8b910185c1ac64b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/val-00000-of-00001.parquet:   0%|          | 0.00/64.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9a2caef8b64839b1e6baa62d3a4245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42578fe712ec47f2b780f721fcfc8c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1083\n",
      "Validation samples: 123\n",
      "Dataset features: {'image_id': Value('string'), 'image': Image(mode=None, decode=True), 'width': Value('int64'), 'height': Value('int64'), 'meta': {'barcode': Value('string'), 'off_image_id': Value('string'), 'image_url': Value('string')}, 'objects': {'bbox': List(List(Value('float32'))), 'category_id': List(Value('int64')), 'category_name': List(Value('string'))}}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
    "ds = load_dataset(dataset_id)\n",
    "\n",
    "# Split into train and validation\n",
    "train_dataset = ds['train']\n",
    "eval_dataset = ds['val']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Dataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The dataset requires conversion to be compatible with the Hugging Face (HF) library. Specifically, each sample must be reformatted into the OpenAI conversation format, comprising:\n",
    "\n",
    "- Roles: system, user, and assistant\n",
    "- User input: Provide an image and ask, \"Detect the bounding box of the nutrition table.\"\n",
    "- Assistant response: Format compatible with Qwen2-VL's detection question responses\n",
    "    * See pages 7 and 43 of this [paper](https://arxiv.org/pdf/2409.12191) and  [Model Card](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct#more-usage-tips) for tips\n",
    "    * Ensure inclusion of class name and bounding box coordinates using the proper special tokens.\n",
    "    * Check the expected range of bb coordinates\n",
    "    * Pay attention to the order of x,y coordinates as expected by Qwen\n",
    "\n",
    "Here is an example system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from product images.\n",
    "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: write a function to map each sample to a list of 3 dicts (one for each role)\n",
    "\n",
    "def convert_to_conversation_format(example):\n",
    "    \"\"\"\n",
    "    Convert a dataset example to Qwen2-VL conversation format.\n",
    "    \n",
    "    Why IMAGE_PLACEHOLDER?\n",
    "    - HuggingFace dataset.map() needs serializable data (PIL images aren't)\n",
    "    - The placeholder is replaced with the actual image during training (in collate_fn)\n",
    "    - Image is stored separately at example['image'] to avoid duplication\n",
    "    \n",
    "    Coordinate Conversion:\n",
    "    - Input: OpenFoodFacts [y_min, x_min, y_max, x_max] in [0,1]\n",
    "    - Output: Qwen2-VL (x1,y1),(x2,y2) in [0,1000)\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset sample with 'image' and 'objects' fields\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'messages' (conversation) and 'image' (PIL object)\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if 'objects' not in example or 'bbox' not in example['objects']:\n",
    "        raise ValueError(\"Missing objects or bbox in example\")\n",
    "    \n",
    "    # Extract nutrition table bounding boxes\n",
    "    bboxes = example['objects']['bbox']\n",
    "    categories = example['objects']['category_name']  # Fixed: 'category_name' not 'category'\n",
    "    \n",
    "    # Format the assistant response with Qwen2-VL special tokens\n",
    "    # Convert normalized [0,1] bbox to Qwen's [0,1000) format\n",
    "    assistant_responses = []\n",
    "    for bbox, category in zip(bboxes, categories):\n",
    "        # Validate bbox values are in [0,1] range (with small tolerance for rounding)\n",
    "        if not all(-0.001 <= coord <= 1.001 for coord in bbox):\n",
    "            print(f\"Warning: bbox coordinates out of [0,1] range: {bbox}\")\n",
    "        \n",
    "        # CRITICAL: OpenFoodFacts uses [y_min, x_min, y_max, x_max] format\n",
    "        # But Qwen2VL expects (x_top_left, y_top_left), (x_bottom_right, y_bottom_right)\n",
    "        y_min, x_min, y_max, x_max = bbox  # Unpack OpenFoodFacts format\n",
    "        \n",
    "        # Convert to Qwen format: (x,y) coordinates in [0,1000) range\n",
    "        # Note: multiply by 1000 to convert from [0,1] to [0,1000)\n",
    "        x1 = int(x_min * 1000)  # x_top_left\n",
    "        y1 = int(y_min * 1000)  # y_top_left\n",
    "        x2 = int(x_max * 1000)  # x_bottom_right\n",
    "        y2 = int(y_max * 1000)  # y_bottom_right\n",
    "        \n",
    "        # Format: <|object_ref_start|>object<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\n",
    "        response = f\"<|object_ref_start|>{category}<|object_ref_end|><|box_start|>({x1},{y1}),({x2},{y2})<|box_end|>\"\n",
    "        assistant_responses.append(response)\n",
    "    \n",
    "    # Combine multiple detections if present\n",
    "    assistant_text = \" \".join(assistant_responses)\n",
    "    \n",
    "    # Create conversation format WITHOUT the PIL image embedded\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_message\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": \"IMAGE_PLACEHOLDER\"  # Use placeholder instead of actual image\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Detect the bounding box of the nutrition table.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": assistant_text\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Return messages and image separately\n",
    "    return {\n",
    "        \"messages\": conversation,\n",
    "        \"image\": example['image']  # Store PIL image at top level\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's format the data using the chatbot structure. This will allow us to set up the interactions appropriately for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _has_image(example):\n",
    "    \"\"\"\n",
    "    Filter out samples with missing or invalid images.\n",
    "    \n",
    "    This function is crucial for preventing None values from leaking into\n",
    "    process_vision_info during collation/training, which would cause crashes.\n",
    "    \n",
    "    Why this is necessary:\n",
    "    - Some dataset samples may have corrupted or missing images\n",
    "    - PIL Image loading can fail silently, leaving None values\n",
    "    - The collate_fn and process_vision_info expect valid PIL images\n",
    "    - Filtering ensures training stability and prevents runtime errors\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset sample that should contain an 'image' field\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if example has a valid PIL image with 'size' attribute\n",
    "    \"\"\"\n",
    "    img = example.get('image')\n",
    "    try:\n",
    "        # Treat as valid only if it looks like a PIL image\n",
    "        return (img is not None) and hasattr(img, 'size')\n",
    "    except Exception:\n",
    "        return img is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e7170718004996a79b4e7fd0586ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b1c073cc4b4d15857aab824b4ad658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering:\n",
      "  Training samples: 1083\n",
      "  Validation samples: 123\n",
      "\n",
      "train_dataset[0]: {'image_id': '0009800892204_1', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F8620064EB0>, 'width': 2592, 'height': 1944, 'meta': {'barcode': '0009800892204', 'off_image_id': '1', 'image_url': 'https://static.openfoodfacts.org/images/products/000/980/089/2204/1.jpg'}, 'objects': {'bbox': [[0.057098764926195145, 0.014274691231548786, 0.603501558303833, 0.991126537322998]], 'category_id': [0], 'category_name': ['nutrition-table']}}\n"
     ]
    }
   ],
   "source": [
    "# Apply filtering before formatting to remove samples with missing/invalid images\n",
    "train_dataset = train_dataset.filter(_has_image)\n",
    "eval_dataset = eval_dataset.filter(_has_image)\n",
    "\n",
    "print(f\"After filtering:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"\\ntrain_dataset[0]: {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9212155f90241a6a54bdfe6f953602e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6004b9a38ed48c3856f2af0638907ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted training samples: 1083\n",
      "Formatted evaluation samples: 123\n"
     ]
    }
   ],
   "source": [
    "# Task: apply the function above to all samples in the training and eval datasets\n",
    "# Use remove_columns to get clean output with only 'messages' and 'image' fields\n",
    "columns_to_remove = ['image_id', 'width', 'height', 'meta', 'objects']\n",
    "\n",
    "# Unfortunately, HuggingFace datasets adds None fields during serialization of nested dicts\n",
    "# This is a known behavior. We have two options:\n",
    "# Option 1: Accept the None fields (they don't affect training, collate_fn handles them)\n",
    "# Option 2: Post-process to remove them (adds overhead but cleaner)\n",
    "\n",
    "# For now, using Option 1 - the collate_fn already handles None values correctly\n",
    "train_dataset_formatted = train_dataset.map(\n",
    "    convert_to_conversation_format,\n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "eval_dataset_formatted = eval_dataset.map(\n",
    "    convert_to_conversation_format, \n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "\n",
    "print(f\"Formatted training samples: {len(train_dataset_formatted)}\")\n",
    "print(f\"Formatted evaluation samples: {len(eval_dataset_formatted)}\")\n",
    "\n",
    "# NOTE: HuggingFace automatically adds 'image': None and 'text': None to content items\n",
    "# This is expected behavior and the collate_fn handles it correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT DISCOVERY: None fields in HuggingFace Datasets\n",
    "\n",
    "**Key points:**\n",
    "1. HuggingFace uses Apache Arrow which enforces schema consistency\n",
    "2. Dataset.from_list() will ALWAYS add None fields back for nested dicts with varying keys\n",
    "3. The None fields don't affect training - collate_fn handles them correctly\n",
    "4. This is only a cosmetic issue when inspecting the dataset\n",
    "\n",
    "**Example of the None values issue:**\n",
    "\n",
    "When `convert_to_conversation_format` returns clean data:\n",
    "```python\n",
    "{\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\"}\n",
    "{\"type\": \"text\", \"text\": \"Detect the bounding box...\"}\n",
    "```\n",
    "\n",
    "After `dataset.map()`, HuggingFace adds None fields for schema consistency:\n",
    "```python\n",
    "{\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\", \"text\": None}  # <-- \"text\": None added!\n",
    "{\"type\": \"text\", \"text\": \"Detect the bounding box...\", \"image\": None}  # <-- \"image\": None added!\n",
    "```\n",
    "\n",
    "This happens because Apache Arrow requires all dicts in a list to have the same keys.\n",
    "\n",
    "We keep the current approach: use .map() with remove_columns for efficiency.\n",
    "The collate_fn properly filters out None values during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Output: BEFORE vs AFTER `dataset.map()`\n",
    "\n",
    "The next two cells show the **same data** at different stages:\n",
    "\n",
    "1. **`sample`** = Direct call to `convert_to_conversation_format(train_dataset[0])` → **CLEAN** output, no None fields\n",
    "2. **`train_dataset_formatted[0]`** = After `dataset.map()` → **Has None fields** added by HuggingFace\n",
    "\n",
    "This helps you see exactly what None fields get added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample 0 after conversion:\n",
      "============================================================\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': 'IMAGE_PLACEHOLDER'}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}], 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F85E80CDD50>}\n",
      "\n",
      "============================================================\n",
      "Sample 1 after conversion:\n",
      "============================================================\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': 'IMAGE_PLACEHOLDER'}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>'}]}], 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x7F85E80CDE10>}\n"
     ]
    }
   ],
   "source": [
    "# Simple check - just print what convert_to_conversation_format produces\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample 0 after conversion:\")\n",
    "print(\"=\"*60)\n",
    "sample = convert_to_conversation_format(train_dataset[0])\n",
    "print(sample)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample 1 after conversion:\")\n",
    "print(\"=\"*60)\n",
    "sample2 = convert_to_conversation_format(train_dataset[1])\n",
    "print(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample 0 - Better formatted\n",
      "============================================================\n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F85E80CDD50>,\n",
      " 'messages': [{'content': [{'text': 'You are a Vision Language Model '\n",
      "                                    'specialized in interpreting visual data '\n",
      "                                    'from product images.\\n'\n",
      "                                    'Your task is to analyze the provided '\n",
      "                                    'product images and detect the nutrition '\n",
      "                                    'tables in a certain format.\\n'\n",
      "                                    'Focus on delivering accurate, succinct '\n",
      "                                    'answers based on the visual information. '\n",
      "                                    'Avoid additional explanation unless '\n",
      "                                    'absolutely necessary.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'system'},\n",
      "              {'content': [{'image': 'IMAGE_PLACEHOLDER', 'type': 'image'},\n",
      "                           {'text': 'Detect the bounding box of the nutrition '\n",
      "                                    'table.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'user'},\n",
      "              {'content': [{'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'assistant'}]}\n",
      "\n",
      "============================================================\n",
      "Sample 1 - Better formatted\n",
      "============================================================\n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x7F85E80CDE10>,\n",
      " 'messages': [{'content': [{'text': 'You are a Vision Language Model '\n",
      "                                    'specialized in interpreting visual data '\n",
      "                                    'from product images.\\n'\n",
      "                                    'Your task is to analyze the provided '\n",
      "                                    'product images and detect the nutrition '\n",
      "                                    'tables in a certain format.\\n'\n",
      "                                    'Focus on delivering accurate, succinct '\n",
      "                                    'answers based on the visual information. '\n",
      "                                    'Avoid additional explanation unless '\n",
      "                                    'absolutely necessary.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'system'},\n",
      "              {'content': [{'image': 'IMAGE_PLACEHOLDER', 'type': 'image'},\n",
      "                           {'text': 'Detect the bounding box of the nutrition '\n",
      "                                    'table.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'user'},\n",
      "              {'content': [{'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Display the same samples with better formatting (line breaks)\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample 0 - Better formatted\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "pprint(sample)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample 1 - Better formatted\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "pprint(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "After apply_chat_template\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674f0536927743c3a02bb7ca394a1f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acadbe3336694bdc8d6f29b93acf3b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d267e7cfd2494ddda44cf240e96f7836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d7642a09d94571b788cabd8d5aaa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304796ff0bb24ad9b65ae4353837cbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202aa2703a464d43b18c67a518d9f7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0 after apply_chat_template:\n",
      "<|im_start|>system\n",
      "You are a Vision Language Model specialized in interpreting visual data from product images.\n",
      "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
      "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>Detect the bounding box of the nutrition table.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|><|im_end|>\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "Sample 1 after apply_chat_template:\n",
      "<|im_start|>system\n",
      "You are a Vision Language Model specialized in interpreting visual data from product images.\n",
      "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
      "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>Detect the bounding box of the nutrition table.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show what happens after apply_chat_template\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"After apply_chat_template\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load processor to apply chat template\n",
    "from transformers import Qwen2VLProcessor\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "# Apply chat template to sample 0\n",
    "text_sample0 = processor.apply_chat_template(\n",
    "    sample['messages'], \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=False  # False for training\n",
    ")\n",
    "print(\"\\nSample 0 after apply_chat_template:\")\n",
    "print(text_sample0)\n",
    "\n",
    "# Apply chat template to sample 1\n",
    "text_sample1 = processor.apply_chat_template(\n",
    "    sample2['messages'],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nSample 1 after apply_chat_template:\")\n",
    "print(text_sample1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see what ACTUALLY goes to the DataLoader\n",
    "\n",
    "Above, we saw `sample` - the **clean** output from `convert_to_conversation_format()`. \n",
    "\n",
    "You might expect that `train_dataset_formatted = train_dataset.map(convert_to_conversation_format)` would simply apply our function to every sample and give us the same clean format.\n",
    "\n",
    "**But that's not what happens!** HuggingFace's `dataset.map()` adds extra `None` fields due to Apache Arrow's schema requirements.\n",
    "\n",
    "**Why does this matter?** If the collate_fn doesn't handle these None values, it could break `process_vision_info()` or `apply_chat_template()` during training. Understanding this difference is key to debugging data pipeline issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INSPECTING train_dataset_formatted - ACTUAL DATASET PASSED TO TRAINER\n",
      "================================================================================\n",
      "\n",
      "Sample 0:\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F85E8127FA0>, 'messages': [{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]}\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x7F85E8127FA0>, 'messages': [{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first two elements of train_dataset_formatted (what actually goes to DataLoader)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSPECTING train_dataset_formatted - ACTUAL DATASET PASSED TO TRAINER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simple inspection first\n",
    "for i in range(2):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(train_dataset_formatted[i])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "train_dataset_formatted[0] - Better formatted\n",
      "============================================================\n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F85E8127340>,\n",
      " 'messages': [{'content': [{'image': None,\n",
      "                            'text': 'You are a Vision Language Model '\n",
      "                                    'specialized in interpreting visual data '\n",
      "                                    'from product images.\\n'\n",
      "                                    'Your task is to analyze the provided '\n",
      "                                    'product images and detect the nutrition '\n",
      "                                    'tables in a certain format.\\n'\n",
      "                                    'Focus on delivering accurate, succinct '\n",
      "                                    'answers based on the visual information. '\n",
      "                                    'Avoid additional explanation unless '\n",
      "                                    'absolutely necessary.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'system'},\n",
      "              {'content': [{'image': 'IMAGE_PLACEHOLDER',\n",
      "                            'text': None,\n",
      "                            'type': 'image'},\n",
      "                           {'image': None,\n",
      "                            'text': 'Detect the bounding box of the nutrition '\n",
      "                                    'table.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'user'},\n",
      "              {'content': [{'image': None,\n",
      "                            'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'assistant'}]}\n",
      "\n",
      "============================================================\n",
      "train_dataset_formatted[1] - Better formatted\n",
      "============================================================\n",
      "\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x7F85E8127460>,\n",
      " 'messages': [{'content': [{'image': None,\n",
      "                            'text': 'You are a Vision Language Model '\n",
      "                                    'specialized in interpreting visual data '\n",
      "                                    'from product images.\\n'\n",
      "                                    'Your task is to analyze the provided '\n",
      "                                    'product images and detect the nutrition '\n",
      "                                    'tables in a certain format.\\n'\n",
      "                                    'Focus on delivering accurate, succinct '\n",
      "                                    'answers based on the visual information. '\n",
      "                                    'Avoid additional explanation unless '\n",
      "                                    'absolutely necessary.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'system'},\n",
      "              {'content': [{'image': 'IMAGE_PLACEHOLDER',\n",
      "                            'text': None,\n",
      "                            'type': 'image'},\n",
      "                           {'image': None,\n",
      "                            'text': 'Detect the bounding box of the nutrition '\n",
      "                                    'table.',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'user'},\n",
      "              {'content': [{'image': None,\n",
      "                            'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>',\n",
      "                            'type': 'text'}],\n",
      "               'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Display the same train_dataset_formatted samples with better formatting\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"train_dataset_formatted[0] - Better formatted\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "formatted_sample_0 = train_dataset_formatted[0]\n",
    "pprint(formatted_sample_0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"train_dataset_formatted[1] - Better formatted\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "formatted_sample_1 = train_dataset_formatted[1]\n",
    "pprint(formatted_sample_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step debugging the collating function before apply to chat template\n",
    "\n",
    "The following cells walk through the data transformation pipeline step by step,\n",
    "showing exactly what happens to the data at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F85E80CE260>, 'messages': [{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]}\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'dict'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_formatted[0])\n",
    "print(type(train_dataset_formatted))\n",
    "print(type(train_dataset_formatted[0]))\n",
    "print(type([train_dataset_formatted[0:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_dataset_formatted\n",
    "\n",
    "# Extract messages and images from each sample\n",
    "messages_list = [sample['messages'] for sample in batch]\n",
    "images_list = [sample.get('image', None) for sample in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F8620066260>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x7F8620065990>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(10,8),(978,994)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=692x720 at 0x7F86200660E0>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(320,171),(627,625)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3120x4208 at 0x7F8620067460>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(143,251),(376,468)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1944x2592 at 0x7F8620067940>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(type(messages_list))\n",
    "print(type(images_list))\n",
    "# print(messages_list)\n",
    "# print(images_list)\n",
    "for i in range(5):\n",
    "    print(messages_list[i])\n",
    "    print(images_list[i])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1083\n"
     ]
    }
   ],
   "source": [
    "print(len(messages_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out samples without images\n",
    "valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "if not valid_pairs:\n",
    "    raise ValueError(\"Batch contains no valid images.\")\n",
    "\n",
    "messages_list, images_list = zip(*valid_pairs)\n",
    "messages_list = list(messages_list)\n",
    "images_list = list(images_list)\n",
    "\n",
    "# Process each sample\n",
    "texts = []\n",
    "all_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 messages_with_image examples:\n",
      "\n",
      "Example 0:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F8620066260>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}]\n",
      "\n",
      "Example 1:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x7F8620065990>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>'}]}]\n",
      "\n",
      "Example 2:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=692x720 at 0x7F86200660E0>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(10,8),(978,994)<|box_end|>'}]}]\n",
      "\n",
      "Example 3:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3120x4208 at 0x7F8620067460>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(320,171),(627,625)<|box_end|>'}]}]\n",
      "\n",
      "Example 4:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1944x2592 at 0x7F8620067940>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(143,251),(376,468)<|box_end|>'}]}]\n"
     ]
    }
   ],
   "source": [
    "all_conversations = []  # This will hold complete conversations\n",
    "\n",
    "for messages, image in zip(messages_list, images_list):\n",
    "    messages_with_image = []\n",
    "    # Clean up messages: remove None values and restore images\n",
    "    for msg in messages:\n",
    "        msg_copy = {'role': msg['role'], 'content': []}\n",
    "        \n",
    "        for content_item in msg['content']:\n",
    "            # Skip None entries entirely\n",
    "            if content_item is None:\n",
    "                continue\n",
    "                \n",
    "            # Process text content - filter out None text values\n",
    "            if content_item.get('type') == 'text':\n",
    "                text_value = content_item.get('text')\n",
    "                if text_value is not None and text_value != 'None':  # Check for actual None and string 'None'\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'text',\n",
    "                        'text': text_value\n",
    "                    })\n",
    "            # Process image content - replace IMAGE_PLACEHOLDER with actual PIL image\n",
    "            elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                # Check if it's IMAGE_PLACEHOLDER and replace with actual image\n",
    "                image_value = content_item.get('image')\n",
    "                if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                    # Replace with the actual PIL image from top level\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image  # Use the actual PIL image\n",
    "                    })\n",
    "                elif image_value and image_value != 'None':\n",
    "                    # Use existing image if it's not placeholder\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image_value\n",
    "                    })\n",
    "        \n",
    "        if msg_copy['content']:\n",
    "            messages_with_image.append(msg_copy)\n",
    "\n",
    "    # Add the complete conversation (3 messages) to collection\n",
    "    all_conversations.append(messages_with_image)\n",
    "\n",
    "print(\"First 5 messages_with_image examples:\")\n",
    "for i in range(min(5, len(all_conversations))):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(all_conversations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 all_conversations examples:\n",
      "\n",
      "Example 0:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x7F7FA93BC280>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}]\n",
      "\n",
      "Example 1:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x7F7FA93BF220>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>'}]}]\n",
      "\n",
      "Example 2:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=692x720 at 0x7F7FA93BF160>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(10,8),(978,994)<|box_end|>'}]}]\n",
      "\n",
      "Example 3:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3120x4208 at 0x7F7FA93BF0A0>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(320,171),(627,625)<|box_end|>'}]}]\n",
      "\n",
      "Example 4:\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1944x2592 at 0x7F7FA93BF010>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(143,251),(376,468)<|box_end|>'}]}]\n"
     ]
    }
   ],
   "source": [
    "# Final Code to make sure the data is formatted correctly after dataset.map:\n",
    "\n",
    "\n",
    "batch = train_dataset_formatted\n",
    "\n",
    "# Extract messages and images from each sample\n",
    "messages_list = [sample['messages'] for sample in batch]\n",
    "images_list = [sample.get('image', None) for sample in batch]\n",
    "\n",
    "# Filter out samples without images\n",
    "valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "if not valid_pairs:\n",
    "    raise ValueError(\"Batch contains no valid images.\")\n",
    "\n",
    "messages_list, images_list = zip(*valid_pairs)\n",
    "messages_list = list(messages_list)\n",
    "images_list = list(images_list)\n",
    "\n",
    "# Process each sample\n",
    "# texts = []\n",
    "# all_images = []\n",
    "\n",
    "all_conversations = []  # This will hold complete conversations, each conversation is a list of 3 messages (system, assistant, user)\n",
    "\n",
    "for messages, image in zip(messages_list, images_list):\n",
    "    messages_with_image = []\n",
    "    # Clean up messages: remove None values and restore images\n",
    "    for msg in messages:\n",
    "        msg_copy = {'role': msg['role'], 'content': []}\n",
    "        \n",
    "        for content_item in msg['content']:\n",
    "            # Skip None entries entirely\n",
    "            if content_item is None:\n",
    "                continue\n",
    "                \n",
    "            # Process text content - filter out None text values\n",
    "            if content_item.get('type') == 'text':\n",
    "                text_value = content_item.get('text')\n",
    "                if text_value is not None and text_value != 'None':  # Check for actual None and string 'None'\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'text',\n",
    "                        'text': text_value\n",
    "                    })\n",
    "            # Process image content - replace IMAGE_PLACEHOLDER with actual PIL image\n",
    "            elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                # Check if it's IMAGE_PLACEHOLDER and replace with actual image\n",
    "                image_value = content_item.get('image')\n",
    "                if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                    # Replace with the actual PIL image from top level\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image  # Use the actual PIL image\n",
    "                    })\n",
    "                elif image_value and image_value != 'None':\n",
    "                    # Use existing image if it's not placeholder\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image_value\n",
    "                    })\n",
    "        \n",
    "        if msg_copy['content']:\n",
    "            messages_with_image.append(msg_copy)\n",
    "\n",
    "    # Add the complete conversation (3 messages) to collection\n",
    "    all_conversations.append(messages_with_image)\n",
    "\n",
    "print(\"First 5 all_conversations examples:\")\n",
    "for i in range(min(5, len(all_conversations))):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(all_conversations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a Vision Language Model specialized in interpreting visual data from product images.\n",
      "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
      "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>Detect the bounding box of the nutrition table.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|><|im_end|>\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<|im_start|>system\n",
      "You are a Vision Language Model specialized in interpreting visual data from product images.\n",
      "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
      "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>Detect the bounding box of the nutrition table.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|><|im_end|>\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# put a batch in apply_chat_template \n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    all_conversations,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    print(text[i])\n",
    "    print(\"-\"*100)\n",
    "\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=2604x1932 at 0x7F85E80CD480>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<PIL.Image.Image image mode=RGB size=308x420 at 0x7F8626DC9120>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<PIL.Image.Image image mode=RGB size=700x728 at 0x7F85E80CEA70>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<PIL.Image.Image image mode=RGB size=3080x4144 at 0x7F85E8064B80>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<PIL.Image.Image image mode=RGB size=1932x2604 at 0x7F8626DCA740>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "image, video = process_vision_info(all_conversations)\n",
    "for i in range(5):\n",
    "    print(image[i])\n",
    "    print(\"-\"*100)\n",
    "# print(video[0:5])\n",
    "\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = processor(\n",
    "    text=text,\n",
    "    images=image,\n",
    "    # videos=all_videos,\n",
    "    padding=True,\n",
    "    truncation=False,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs: \n",
      " tensor([[    24,     16,     11,     21,     15,     18,      8, 151649, 151645,\n",
      "            198],\n",
      "        [    16,     21,     11,     20,     23,     23,      8, 151649, 151645,\n",
      "            198]])\n",
      "batch_inputs[\"attention_mask\"]: \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "batch_inputs[\"input_ids\"]: \n",
      " tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643],\n",
      "        [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643]])\n",
      "batch_inputs[\"attention_mask\"]: \n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input shapes:\n",
      "input_ids: torch.Size([1083, 16433])\n",
      "attention_mask: torch.Size([1083, 16433])\n",
      "pixel_values: torch.Size([31255872, 1176])\n",
      "image_grid_thw: torch.Size([1083, 3])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "type(batch_inputs): \n",
      " <class 'transformers.feature_extraction_utils.BatchFeature'>\n"
     ]
    }
   ],
   "source": [
    "print(f'batch_inputs: \\n {batch_inputs[\"input_ids\"][0:2, -10:]}') ## First 2 samples, last 10 tokens\n",
    "print(f'batch_inputs[\"attention_mask\"]: \\n {batch_inputs[\"attention_mask\"][0:2, -10:]}') ## 1=real token, 0=padding\n",
    "\n",
    "print(f'batch_inputs[\"input_ids\"]: \\n {batch_inputs[\"input_ids\"][0:2, 0:10]}') # First 2 samples, first 10 tokens\n",
    "print(f'batch_inputs[\"attention_mask\"]: \\n {batch_inputs[\"attention_mask\"][0:2, 0:10]}')\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "for key, value in batch_inputs.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(f'type(batch_inputs): \\n {type(batch_inputs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Code starting from Apply Chat template \n",
    "\n",
    "# Apply chat template to get text\n",
    "text = processor.apply_chat_template(\n",
    "    all_conversations,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "# process_vision_info to get image and video\n",
    "image, video = process_vision_info(all_conversations)\n",
    "\n",
    "# Process texts and images together\n",
    "batch_inputs = processor( # batch_inputs is a dictionary containing input_ids, attention_mask, pixel_values, and image_grid_thw\n",
    "    text=text,\n",
    "    images=image,\n",
    "    # videos=all_videos,\n",
    "    padding=True,\n",
    "    truncation=False,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor.tokenizer.pad_token_id: 151643\n",
      "processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'): 151652\n",
      "processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'): 151653\n",
      "processor.tokenizer.convert_tokens_to_ids('<|image_pad|>'): 151655\n"
     ]
    }
   ],
   "source": [
    "# __init__ method\n",
    "print(\n",
    "    f\"processor.tokenizer.pad_token_id: {processor.tokenizer.pad_token_id}\\n\"\n",
    "    f\"processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'): {processor.tokenizer.convert_tokens_to_ids('<|vision_start|>')}\\n\"\n",
    "    f\"processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'): {processor.tokenizer.convert_tokens_to_ids('<|vision_end|>')}\\n\"\n",
    "    f\"processor.tokenizer.convert_tokens_to_ids('<|image_pad|>'): {processor.tokenizer.convert_tokens_to_ids('<|image_pad|>')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Memory\n",
    "\n",
    "Before proceeding with training, clear current variables and clean the GPU to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.00 GB\n",
      "GPU reserved memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if 'inputs' in globals(): del globals()['inputs']\n",
    "    if 'model' in globals(): del globals()['model']\n",
    "    if 'processor' in globals(): del globals()['processor']\n",
    "    if 'trainer' in globals(): del globals()['trainer']\n",
    "    if 'peft_model' in globals(): del globals()['peft_model']\n",
    "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
