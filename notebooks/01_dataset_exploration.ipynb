{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Dataset Exploration: OpenFoodFacts Nutrition Tables\n",
    "\n",
    "This notebook provides a detailed exploration of the **OpenFoodFacts nutrition-table-detection** dataset used for fine-tuning vision-language models.\n",
    "\n",
    "## Objectives\n",
    "- Load and understand the dataset structure\n",
    "- Visualize sample images with bounding boxes\n",
    "- Analyze image size distributions\n",
    "- Study bounding box characteristics\n",
    "- Generate insights for model training\n",
    "\n",
    "## Dataset Info\n",
    "- **Source**: [openfoodfacts/nutrition-table-detection](https://huggingface.co/datasets/openfoodfacts/nutrition-table-detection)\n",
    "- **Task**: Object detection for nutrition tables in product images\n",
    "- **Format**: Images with normalized bounding boxes [y_min, x_min, y_max, x_max]\n",
    "- **Splits**: Train and Validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
    "print(f\"Loading dataset: {dataset_id}\")\n",
    "ds = load_dataset(dataset_id)\n",
    "\n",
    "# Split into training and evaluation sets\n",
    "train_dataset = ds['train']\n",
    "eval_dataset = ds['val']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(ds)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "for feature_name, feature_type in train_dataset.features.items():\n",
    "    print(f\"  {feature_name:12} : {feature_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Sample Data\n",
    "\n",
    "Let's examine the structure of a single training example to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first training example\n",
    "example = train_dataset[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FIRST TRAINING EXAMPLE - DETAILED INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüÜî BASIC INFO:\")\n",
    "print(f\"  Image ID     : {example['image_id']}\")\n",
    "print(f\"  Dimensions   : {example['width']} x {example['height']}\")\n",
    "print(f\"  Image Object : {example['image']}\")\n",
    "\n",
    "print(f\"\\nüìä METADATA:\")\n",
    "pp = pprint.PrettyPrinter(indent=4, width=80)\n",
    "pp.pprint(example['meta'])\n",
    "\n",
    "print(f\"\\nüéØ ANNOTATIONS:\")\n",
    "print(f\"  Number of objects: {len(example['objects']['category_name'])}\")\n",
    "print(f\"  Category names   : {example['objects']['category_name']}\")\n",
    "print(f\"  Category IDs     : {example['objects']['category_id']}\")\n",
    "print(f\"  Bounding boxes   :\")\n",
    "for i, bbox in enumerate(example['objects']['bbox']):\n",
    "    print(f\"    Object {i}: [y_min={bbox[0]:.3f}, x_min={bbox[1]:.3f}, y_max={bbox[2]:.3f}, x_max={bbox[3]:.3f}]\")\n",
    "\n",
    "print(f\"\\nüí° NOTE: Bounding box coordinates are normalized to [0, 1] for resolution independence.\")\n",
    "print(f\"   Format: [y_min, x_min, y_max, x_max] (OpenFoodFacts convention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Single Example\n",
    "\n",
    "Let's visualize one example to understand how bounding boxes map to actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SINGLE IMAGE VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the bounding box and image\n",
    "bbox = example['objects']['bbox'][0]\n",
    "pil_width, pil_height = example['image'].size\n",
    "\n",
    "print(f\"Image dimensions: {pil_width} x {pil_height} pixels\")\n",
    "print(f\"Raw bbox (normalized): {bbox}\")\n",
    "\n",
    "# Convert normalized coordinates to pixel coordinates\n",
    "y_min, x_min, y_max, x_max = bbox\n",
    "x_min_px = x_min * pil_width\n",
    "y_min_px = y_min * pil_height\n",
    "x_max_px = x_max * pil_width\n",
    "y_max_px = y_max * pil_height\n",
    "\n",
    "print(f\"Pixel coordinates: [{x_min_px:.1f}, {y_min_px:.1f}, {x_max_px:.1f}, {y_max_px:.1f}]\")\n",
    "print(f\"Box size: {x_max_px-x_min_px:.1f} x {y_max_px-y_min_px:.1f} pixels\")\n",
    "\n",
    "# Create side-by-side visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: Original image\n",
    "ax1.imshow(example['image'])\n",
    "ax1.set_title(\"Original Image\")\n",
    "ax1.axis('off')\n",
    "\n",
    "# Right: Image with bounding box\n",
    "img_with_bbox = example['image'].copy()\n",
    "draw = ImageDraw.Draw(img_with_bbox)\n",
    "draw.rectangle([x_min_px, y_min_px, x_max_px, y_max_px], outline='red', width=3)\n",
    "\n",
    "ax2.imshow(img_with_bbox)\n",
    "ax2.set_title(f\"With Bounding Box: {example['objects']['category_name'][0]}\")\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Category detected: {example['objects']['category_name'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Multiple Examples\n",
    "\n",
    "Display a grid of training examples to see the variety in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MULTIPLE EXAMPLES VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Displaying 6 training examples with nutrition table bounding boxes:\\n\")\n",
    "\n",
    "# Create a 2x3 grid for displaying 6 examples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(6):\n",
    "    example = train_dataset[idx]\n",
    "    img = example['image'].copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    pil_width, pil_height = img.size\n",
    "    num_objects = len(example['objects']['bbox'])\n",
    "    \n",
    "    print(f\"  Example {idx}: {pil_width}x{pil_height}px, {num_objects} object(s)\")\n",
    "    \n",
    "    # Calculate proportional line width\n",
    "    line_width = max(3, min(pil_width, pil_height) // 150)\n",
    "    \n",
    "    # Draw all bounding boxes for this image\n",
    "    for i, (bbox, category) in enumerate(zip(example['objects']['bbox'], \n",
    "                                             example['objects']['category_name'])):\n",
    "        y_min, x_min, y_max, x_max = bbox\n",
    "        x_min_px = x_min * pil_width\n",
    "        y_min_px = y_min * pil_height\n",
    "        x_max_px = x_max * pil_width\n",
    "        y_max_px = y_max * pil_height\n",
    "        \n",
    "        draw.rectangle([x_min_px, y_min_px, x_max_px, y_max_px], \n",
    "                      outline='red', width=line_width)\n",
    "        \n",
    "        # Add category label\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        text_y = max(10, y_min_px - 25)\n",
    "        draw.text((x_min_px, text_y), f\"{category}\", fill='red', font=font)\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Example {idx}\\nSize: {pil_width}x{pil_height}px | Objects: {num_objects}\", \n",
    "                       fontsize=10, pad=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Size Distribution Analysis\n",
    "\n",
    "Analyze the distribution of image sizes to understand the variety in input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"IMAGE SIZE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Processing {len(train_dataset)} training examples...\\n\")\n",
    "\n",
    "# Collect statistics\n",
    "image_widths = []\n",
    "image_heights = []\n",
    "image_areas = []\n",
    "num_bboxes_per_image = []\n",
    "\n",
    "for example in train_dataset:\n",
    "    image_widths.append(example['width'])\n",
    "    image_heights.append(example['height'])\n",
    "    image_areas.append(example['width'] * example['height'])\n",
    "    num_bboxes_per_image.append(len(example['objects']['bbox']))\n",
    "\n",
    "# Convert to numpy arrays\n",
    "image_widths = np.array(image_widths)\n",
    "image_heights = np.array(image_heights)\n",
    "image_areas = np.array(image_areas)\n",
    "num_bboxes_per_image = np.array(num_bboxes_per_image)\n",
    "\n",
    "print(f\"üìä IMAGE DIMENSIONS SUMMARY:\")\n",
    "print(f\"  Width  - Min: {image_widths.min():4d}px | Max: {image_widths.max():4d}px | Mean: {image_widths.mean():.1f}px\")\n",
    "print(f\"  Height - Min: {image_heights.min():4d}px | Max: {image_heights.max():4d}px | Mean: {image_heights.mean():.1f}px\")\n",
    "print(f\"  Area   - Min: {image_areas.min():8.0f} | Max: {image_areas.max():8.0f} | Mean: {image_areas.mean():.0f}\")\n",
    "\n",
    "print(f\"\\nüéØ BOUNDING BOXES SUMMARY:\")\n",
    "print(f\"  Min boxes per image: {num_bboxes_per_image.min()}\")\n",
    "print(f\"  Max boxes per image: {num_bboxes_per_image.max()}\")\n",
    "print(f\"  Mean boxes per image: {num_bboxes_per_image.mean():.2f}\")\n",
    "print(f\"  Total bounding boxes: {num_bboxes_per_image.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Image widths histogram\n",
    "ax1.hist(image_widths, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Image Width (pixels)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Image Widths')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(image_widths.mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {image_widths.mean():.0f}px')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Image heights histogram\n",
    "ax2.hist(image_heights, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax2.set_xlabel('Image Height (pixels)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Image Heights')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axvline(image_heights.mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {image_heights.mean():.0f}px')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Image areas histogram\n",
    "ax3.hist(image_areas/1e6, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "ax3.set_xlabel('Image Area (Megapixels)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Distribution of Image Areas')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axvline(image_areas.mean()/1e6, color='red', linestyle='--', \n",
    "            label=f'Mean: {image_areas.mean()/1e6:.1f}MP')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Bounding boxes per image\n",
    "bbox_counts = np.bincount(num_bboxes_per_image)\n",
    "bbox_labels = np.arange(len(bbox_counts))\n",
    "ax4.bar(bbox_labels, bbox_counts, alpha=0.7, color='coral', edgecolor='black')\n",
    "ax4.set_xlabel('Number of Bounding Boxes per Image')\n",
    "ax4.set_ylabel('Number of Images')\n",
    "ax4.set_title('Distribution of Bounding Boxes per Image\\n(Critical for Model Configuration)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xticks(bbox_labels)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, count in enumerate(bbox_counts):\n",
    "    if count > 0:\n",
    "        percentage = (count / len(train_dataset)) * 100\n",
    "        ax4.text(i, count + 0.5, f'{count}\\n({percentage:.1f}%)', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed bbox distribution\n",
    "unique_bbox_counts, bbox_frequencies = np.unique(num_bboxes_per_image, return_counts=True)\n",
    "print(f\"\\nüìä Bounding box distribution breakdown:\")\n",
    "for count, freq in zip(unique_bbox_counts, bbox_frequencies):\n",
    "    percentage = (freq / len(train_dataset)) * 100\n",
    "    print(f\"  {count} box(es): {freq:3d} images ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bounding Box Characteristics Analysis\n",
    "\n",
    "Detailed analysis of bounding box properties for anchor configuration and model optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BOUNDING BOX DETAILED ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Analyzing bbox sizes, positions, and coverage...\\n\")\n",
    "\n",
    "# Collect detailed bbox statistics\n",
    "bbox_widths = []\n",
    "bbox_heights = []\n",
    "bbox_areas = []\n",
    "bbox_aspect_ratios = []\n",
    "coverage_ratios = []\n",
    "center_x_positions = []\n",
    "center_y_positions = []\n",
    "\n",
    "for example in train_dataset:\n",
    "    img_width, img_height = example['width'], example['height']\n",
    "    img_area = img_width * img_height\n",
    "    \n",
    "    for bbox in example['objects']['bbox']:\n",
    "        # Convert normalized coordinates to pixels\n",
    "        # Note: bbox format is [y_min, x_min, y_max, x_max]\n",
    "        y_min, x_min, y_max, x_max = bbox\n",
    "        w = (x_max - x_min) * img_width\n",
    "        h = (y_max - y_min) * img_height\n",
    "        area = w * h\n",
    "        aspect_ratio = w / h if h > 0 else 0\n",
    "        coverage = area / img_area if img_area > 0 else 0\n",
    "        \n",
    "        # Calculate center position (normalized)\n",
    "        center_x = (x_min + x_max) / 2\n",
    "        center_y = (y_min + y_max) / 2\n",
    "        \n",
    "        bbox_widths.append(w)\n",
    "        bbox_heights.append(h)\n",
    "        bbox_areas.append(area)\n",
    "        bbox_aspect_ratios.append(aspect_ratio)\n",
    "        coverage_ratios.append(coverage)\n",
    "        center_x_positions.append(center_x)\n",
    "        center_y_positions.append(center_y)\n",
    "\n",
    "# Convert to numpy\n",
    "bbox_widths = np.array(bbox_widths)\n",
    "bbox_heights = np.array(bbox_heights)\n",
    "bbox_areas = np.array(bbox_areas)\n",
    "bbox_aspect_ratios = np.array(bbox_aspect_ratios)\n",
    "coverage_ratios = np.array(coverage_ratios)\n",
    "\n",
    "print(f\"‚úÖ Analyzed {len(bbox_widths)} bounding boxes!\")\n",
    "\n",
    "print(f\"\\nüìè BOUNDING BOX SIZE STATISTICS:\")\n",
    "print(f\"  Width  - Min: {bbox_widths.min():4.0f}px | Max: {bbox_widths.max():4.0f}px | Mean: {bbox_widths.mean():.0f}px | Std: {bbox_widths.std():.0f}px\")\n",
    "print(f\"  Height - Min: {bbox_heights.min():4.0f}px | Max: {bbox_heights.max():4.0f}px | Mean: {bbox_heights.mean():.0f}px | Std: {bbox_heights.std():.0f}px\")\n",
    "print(f\"  Area   - Min: {bbox_areas.min():8.0f} | Max: {bbox_areas.max():8.0f} | Mean: {bbox_areas.mean():.0f}\")\n",
    "\n",
    "print(f\"\\nüìê ASPECT RATIO & COVERAGE:\")\n",
    "print(f\"  Aspect Ratio (W/H) - Min: {bbox_aspect_ratios.min():.2f} | Max: {bbox_aspect_ratios.max():.2f} | Mean: {bbox_aspect_ratios.mean():.2f}\")\n",
    "print(f\"  Image Coverage     - Min: {coverage_ratios.min():.1%} | Max: {coverage_ratios.max():.1%} | Mean: {coverage_ratios.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Bounding Box Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Bbox widths and heights\n",
    "ax1.hist(bbox_widths, bins=25, alpha=0.6, color='purple', edgecolor='black', label='Width')\n",
    "ax1.hist(bbox_heights, bins=25, alpha=0.6, color='orange', edgecolor='black', label='Height')\n",
    "ax1.set_xlabel('Size (pixels)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Bounding Box Dimensions')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Aspect ratios\n",
    "ax2.hist(bbox_aspect_ratios, bins=25, alpha=0.7, color='brown', edgecolor='black')\n",
    "ax2.set_xlabel('Aspect Ratio (Width/Height)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Bounding Box Aspect Ratios')\n",
    "ax2.axvline(bbox_aspect_ratios.mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {bbox_aspect_ratios.mean():.2f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Coverage ratios\n",
    "ax3.hist(coverage_ratios * 100, bins=25, alpha=0.7, color='green', edgecolor='black')\n",
    "ax3.set_xlabel('Image Coverage (%)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Bounding Box Coverage of Image')\n",
    "ax3.axvline(coverage_ratios.mean() * 100, color='red', linestyle='--', \n",
    "            label=f'Mean: {coverage_ratios.mean():.1%}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Bbox center positions\n",
    "ax4.scatter(center_x_positions, center_y_positions, alpha=0.6, color='red', s=20)\n",
    "ax4.set_xlabel('Center X (normalized)')\n",
    "ax4.set_ylabel('Center Y (normalized)')\n",
    "ax4.set_title('Bounding Box Center Positions')\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.invert_yaxis()  # Match image coordinates (0,0 at top-left)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CATEGORY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect all categories\n",
    "all_categories = []\n",
    "for example in train_dataset:\n",
    "    all_categories.extend(example['objects']['category_name'])\n",
    "\n",
    "category_counts = Counter(all_categories)\n",
    "\n",
    "print(f\"\\nCategory distribution:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(all_categories)) * 100\n",
    "    print(f\"  {category}: {count} instances ({percentage:.1f}%)\")\n",
    "\n",
    "# Check for category variations\n",
    "unique_categories = set(all_categories)\n",
    "print(f\"\\nUnique category strings: {len(unique_categories)}\")\n",
    "if len(unique_categories) > 1:\n",
    "    print(f\"  ‚ö†Ô∏è  Multiple category variants detected: {unique_categories}\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ Consistent single category: {list(unique_categories)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Recommendations\n",
    "\n",
    "Based on the dataset analysis, here are important insights for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüîß MODEL PREPARATION INSIGHTS:\")\n",
    "print(f\"  Most common image aspect ratio: {image_widths.mean()/image_heights.mean():.2f} (width/height)\")\n",
    "print(f\"  Average bbox coverage: {coverage_ratios.mean():.1%} of image\")\n",
    "print(f\"  Average bbox aspect ratio: {bbox_aspect_ratios.mean():.2f}\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "\n",
    "# Single vs multi-object detection\n",
    "if num_bboxes_per_image.max() == 1:\n",
    "    print(f\"  ‚úÖ Single object detection - simpler model configuration\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Multi-object detection - configure model for up to {num_bboxes_per_image.max()} objects\")\n",
    "\n",
    "# Input resolution recommendation\n",
    "suggested_resolution = int(np.sqrt(image_areas.mean()))\n",
    "print(f\"  üìê Consider input resolution around {suggested_resolution}√ó{suggested_resolution} pixels\")\n",
    "\n",
    "# Anchor configuration\n",
    "mean_coverage = coverage_ratios.mean()\n",
    "std_coverage = coverage_ratios.std()\n",
    "scale_small = max(0.2, mean_coverage - std_coverage)\n",
    "scale_medium = mean_coverage\n",
    "scale_large = min(0.9, mean_coverage + std_coverage)\n",
    "print(f\"\\nüéØ ANCHOR CONFIGURATION RECOMMENDATIONS:\")\n",
    "print(f\"  Recommended anchor scales: [{scale_small:.1f}, {scale_medium:.1f}, {scale_large:.1f}]\")\n",
    "\n",
    "# Aspect ratios\n",
    "mean_aspect = bbox_aspect_ratios.mean()\n",
    "std_aspect = bbox_aspect_ratios.std()\n",
    "ratio_narrow = max(0.5, mean_aspect - std_aspect)\n",
    "ratio_medium = mean_aspect\n",
    "ratio_wide = min(2.0, mean_aspect + std_aspect)\n",
    "print(f\"  Recommended aspect ratios: [{ratio_narrow:.1f}, {ratio_medium:.1f}, {ratio_wide:.1f}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset analysis complete!\")\n",
    "print(f\"   Use these insights to configure your vision-language model for optimal performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided a comprehensive exploration of the OpenFoodFacts nutrition table detection dataset. Key findings:\n",
    "\n",
    "1. **Dataset size**: Training and validation splits with nutrition table annotations\n",
    "2. **Image characteristics**: Variable sizes with diverse aspect ratios\n",
    "3. **Bounding boxes**: Normalized coordinates in [y_min, x_min, y_max, x_max] format\n",
    "4. **Object detection**: Single or multi-object per image\n",
    "5. **Coverage patterns**: Nutrition tables typically occupy a significant portion of images\n",
    "\n",
    "Use these insights when:\n",
    "- Configuring model input resolution\n",
    "- Setting up data augmentation\n",
    "- Designing anchor boxes (if applicable)\n",
    "- Tuning training hyperparameters\n",
    "\n",
    "**Next steps**: Proceed to model understanding and fine-tuning in the main training notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
