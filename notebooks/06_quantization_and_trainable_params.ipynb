{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3b28c7",
   "metadata": {},
   "source": [
    "# Quantization & Trainable Parameters Diagnostic\n",
    "\n",
    "**Purpose**: Answer 3 key questions about Qwen2-VL model training:\n",
    "\n",
    "1. **Q1**: What does quantization do to trainable layers?\n",
    "2. **Q2**: Do `q_proj` layers apply only to LLM or also vision encoder?\n",
    "3. **Q3**: After `get_peft_model()`, which modules become trainable?\n",
    "\n",
    "**Use Case**: Preparation for tutor session to demonstrate understanding of:\n",
    "- 4-bit quantization effects\n",
    "- Model architecture layer naming\n",
    "- LoRA adapter targeting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200c9b0",
   "metadata": {},
   "source": [
    "## Section 1: Setup\n",
    "\n",
    "**IMPORTANT**: GPU configuration must be set BEFORE importing torch!\n",
    "\n",
    "This section:\n",
    "1. Auto-detects available GPUs (max 2)\n",
    "2. Sets `CUDA_VISIBLE_DEVICES` environment variable\n",
    "3. Then imports torch and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fa5e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUTOMATIC GPU CONFIGURATION\n",
      "============================================================\n",
      "Looking for up to 2 available GPU(s)...\n",
      "Found 2 available GPU(s): [3, 5]\n",
      "\n",
      "CUDA_VISIBLE_DEVICES: 3,5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GPU CONFIGURATION - MUST RUN FIRST (before importing torch!)\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Set memory management for better GPU utilization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# CONFIGURATION\n",
    "MAX_GPUS = 2  # Maximum number of GPUs to use\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTOMATIC GPU CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def find_available_gpus(num_gpus_needed=2):\n",
    "    \"\"\"Find available GPUs by checking memory usage.\n",
    "\n",
    "    Criteria for \"available\":\n",
    "    - Less than 2GB memory used (essentially idle)\n",
    "    - At least 40GB total memory (suitable for 7B model)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total',\n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "\n",
    "        available = []\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            parts = line.split(',')\n",
    "            gpu_idx = int(parts[0])\n",
    "            mem_used = int(parts[1])\n",
    "            mem_total = int(parts[2])\n",
    "\n",
    "            # Consider GPU available if <2GB used and has at least 40GB total\n",
    "            if mem_used < 2000 and mem_total > 40000:\n",
    "                available.append(gpu_idx)\n",
    "\n",
    "        # Return exactly the number of GPUs requested (not more!)\n",
    "        if len(available) >= num_gpus_needed:\n",
    "            return available[:num_gpus_needed]\n",
    "        else:\n",
    "            return available\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking GPUs: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(f\"Looking for up to {MAX_GPUS} available GPU(s)...\")\n",
    "\n",
    "# Find available GPUs\n",
    "available_gpus = find_available_gpus(MAX_GPUS)\n",
    "\n",
    "if len(available_gpus) > 0:\n",
    "    gpu_string = ','.join(str(g) for g in available_gpus)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_string\n",
    "    print(f\"Found {len(available_gpus)} available GPU(s): {available_gpus}\")\n",
    "else:\n",
    "    # Fallback: find GPU with most free memory\n",
    "    print(\"No free GPUs found with standard criteria, checking all GPUs...\")\n",
    "\n",
    "    best_gpu = None\n",
    "    best_free_mem = 0\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total',\n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            parts = line.split(',')\n",
    "            gpu_idx = int(parts[0])\n",
    "            mem_used = int(parts[1])\n",
    "            mem_total = int(parts[2])\n",
    "            free_mem = mem_total - mem_used\n",
    "\n",
    "            print(f\"   GPU {gpu_idx}: {free_mem/1024:.1f} GB free\")\n",
    "\n",
    "            if free_mem > best_free_mem and free_mem > 20000:  # At least 20GB free\n",
    "                best_gpu = gpu_idx\n",
    "                best_free_mem = free_mem\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if best_gpu is not None:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(best_gpu)\n",
    "        print(f\"Using GPU {best_gpu} with {best_free_mem/1024:.1f} GB free memory\")\n",
    "    else:\n",
    "        print(\"No GPUs available with sufficient memory!\")\n",
    "        print(\"Please free up GPU memory or wait for GPUs to become available\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(f\"\\nCUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2171db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/miniconda3/envs/vlm_Qwen2VL_object_detection/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "PyTorch sees 2 GPU(s):\n",
      "  GPU 0: NVIDIA RTX 6000 Ada Generation (47.5 GB)\n",
      "  GPU 1: NVIDIA RTX 6000 Ada Generation (47.5 GB)\n",
      "\n",
      "CONFIRMED: Using 2 GPU(s) (max allowed: 2)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# NOW import torch (after CUDA_VISIBLE_DEVICES is set)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "from transformers import BitsAndBytesConfig, Qwen2VLForConditionalGeneration\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"PyTorch sees {device_count} GPU(s):\")\n",
    "    for i in range(device_count):\n",
    "        mem_gb = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} ({mem_gb:.1f} GB)\")\n",
    "    print(f\"\\nCONFIRMED: Using {device_count} GPU(s) (max allowed: {MAX_GPUS})\")\n",
    "else:\n",
    "    print(\"CUDA not available!\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0677a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTION: Summarize Trainable Parameters by Area\n",
    "# ============================================================\n",
    "\n",
    "def summarize_trainables_by_area(model, title=\"Model\"):\n",
    "    \"\"\"\n",
    "    Summarize trainable parameters grouped by model area.\n",
    "\n",
    "    Returns:\n",
    "        List of trainable parameter names\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    trainable_names = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            trainable_names.append(name)\n",
    "\n",
    "    pct = 100 * trainable_params / max(total_params, 1)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total params:     {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,} ({pct:.4f}%)\")\n",
    "\n",
    "    # Count by area\n",
    "    # Parameter paths from named_parameters():\n",
    "    #   Vision: model.visual.blocks.0.attn.qkv.weight\n",
    "    #   LLM:    model.language_model.layers.0.self_attn.q_proj.weight\n",
    "    areas = {\"visual\": 0, \"language_model\": 0, \"lm_head\": 0, \"other\": 0}\n",
    "    for name in trainable_names:\n",
    "        if \".visual.\" in name:\n",
    "            areas[\"visual\"] += 1\n",
    "        elif \"language_model\" in name:\n",
    "            areas[\"language_model\"] += 1\n",
    "        elif \"lm_head\" in name:\n",
    "            areas[\"lm_head\"] += 1\n",
    "        else:\n",
    "            areas[\"other\"] += 1\n",
    "\n",
    "    print(\"\\nTrainable params by area:\")\n",
    "    for area, count in areas.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {area}: {count} parameters\")\n",
    "\n",
    "    return trainable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b015d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTION: Find Modules by Pattern\n",
    "# ============================================================\n",
    "\n",
    "def find_modules_by_pattern(model, pattern, max_print=30):\n",
    "    \"\"\"\n",
    "    Find all modules containing a specific pattern in their name.\n",
    "\n",
    "    Args:\n",
    "        model: The model to inspect\n",
    "        pattern: Substring to search for (e.g., \"q_proj\", \"qkv\")\n",
    "        max_print: Maximum number to print\n",
    "\n",
    "    Returns:\n",
    "        List of matching module names\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for name, _ in model.named_modules():\n",
    "        if pattern in name:\n",
    "            matches.append(name)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Modules containing '{pattern}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total found: {len(matches)}\")\n",
    "\n",
    "    # Show first few and last few\n",
    "    if len(matches) <= max_print:\n",
    "        for name in matches:\n",
    "            print(f\"  {name}\")\n",
    "    else:\n",
    "        for name in matches[:max_print//2]:\n",
    "            print(f\"  {name}\")\n",
    "        print(f\"  ... ({len(matches) - max_print} more) ...\")\n",
    "        for name in matches[-max_print//2:]:\n",
    "            print(f\"  {name}\")\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9c6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTION: Clean Up GPU Memory\n",
    "# ============================================================\n",
    "\n",
    "def cleanup_model(model_to_delete, model_name=\"model\"):\n",
    "    \"\"\"Delete model and free GPU memory.\"\"\"\n",
    "    del model_to_delete\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"\\n[Cleanup] Deleted {model_name}\")\n",
    "    print(f\"[Cleanup] GPU memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c16e82",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Q1 - Quantization Effect on Trainable Layers\n",
    "\n",
    "**Question**: What does quantization do to trainable layers?\n",
    "\n",
    "We'll compare:\n",
    "1. **BF16 model** (no quantization): All layers have `requires_grad=True` by default\n",
    "2. **4-bit quantized model**: Quantized layers are frozen (`requires_grad=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28082ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BF16 model (no quantization)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bfacd3234d4914a3283d6491eafe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BF16 Base Model (no quantization)\n",
      "============================================================\n",
      "Total params:     8,291,375,616\n",
      "Trainable params: 8,291,375,616 (100.0000%)\n",
      "\n",
      "Trainable params by area:\n",
      "  visual: 391 parameters\n",
      "  language_model: 338 parameters\n",
      "  lm_head: 1 parameters\n",
      "\n",
      "GPU memory with BF16 model: 7.05 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD BF16 MODEL (No Quantization)\n",
    "# ============================================================\n",
    "print(\"Loading BF16 model (no quantization)...\")\n",
    "\n",
    "model_bf16 = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "bf16_trainable = summarize_trainables_by_area(model_bf16, \"BF16 Base Model (no quantization)\")\n",
    "print(f\"\\nGPU memory with BF16 model: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "969ff9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Cleanup] Deleted model_bf16\n",
      "[Cleanup] GPU memory after cleanup: 7.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Clean up BF16 model before loading 4-bit\n",
    "cleanup_model(model_bf16, \"model_bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2435d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 4-bit quantized model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fb73d48a2e4aa0bca5debccd30458c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4-bit Quantized Model\n",
      "============================================================\n",
      "Total params:     4,691,876,352\n",
      "Trainable params: 1,091,870,720 (23.2715%)\n",
      "\n",
      "Trainable params by area:\n",
      "  visual: 131 parameters\n",
      "  language_model: 58 parameters\n",
      "  lm_head: 1 parameters\n",
      "\n",
      "GPU memory with 4-bit model: 8.95 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD 4-BIT QUANTIZED MODEL\n",
    "# ============================================================\n",
    "print(\"Loading 4-bit quantized model...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_4bit = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "quant_trainable = summarize_trainables_by_area(model_4bit, \"4-bit Quantized Model\")\n",
    "print(f\"\\nGPU memory with 4-bit model: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5adfed",
   "metadata": {},
   "source": [
    "### Q1 Answer: Quantization Effect\n",
    "\n",
    "| Model | Trainable Params | Why? |\n",
    "|-------|-----------------|------|\n",
    "| **BF16** | ~100% (8.3B params) | Default behavior - all params have `requires_grad=True` |\n",
    "| **4-bit** | ~23% (1.1B params) | Only **Linear layers** are quantized; other layers remain trainable |\n",
    "\n",
    "**Key Insight**: BitsAndBytes 4-bit quantization does NOT freeze everything!\n",
    "\n",
    "| Layer Type | Quantized? | `requires_grad` |\n",
    "|------------|-----------|-----------------|\n",
    "| **Linear** (q_proj, k_proj, etc.) | Yes (4-bit NF4) | False, but gradients flow properly |\n",
    "| **LayerNorm** | No (bf16) | True (trainable) |\n",
    "| **Embeddings** | No (bf16) | True (trainable) |\n",
    "| **lm_head** | No (bf16) | True (trainable) |\n",
    "\n",
    "The ~23% trainable params are the **non-quantized layers** (LayerNorm, embeddings, etc.).\n",
    "\n",
    "**Why we still need LoRA**: Even though some params are \"trainable\", the quantized Linear layers\n",
    "(which contain most of the model's capacity) cannot be effectively trained. LoRA adds trainable\n",
    "adapters specifically to these frozen Linear layers.\n",
    "\n",
    "**What `prepare_model_for_kbit_training()` does**: Explicitly freezes ALL parameters (including\n",
    "the non-quantized ones), then `get_peft_model()` adds trainable LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53c720",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Q2 - Layer Names (q_proj vs qkv)\n",
    "\n",
    "**Question**: Do `q_proj` layers apply only to LLM or also vision encoder?\n",
    "\n",
    "We'll search for:\n",
    "- `q_proj` - should be **LLM only**\n",
    "- `qkv` - should be **Vision encoder only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e11fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Modules containing 'q_proj'\n",
      "============================================================\n",
      "Total found: 28\n",
      "  model.language_model.layers.0.self_attn.q_proj\n",
      "  model.language_model.layers.1.self_attn.q_proj\n",
      "  model.language_model.layers.2.self_attn.q_proj\n",
      "  model.language_model.layers.3.self_attn.q_proj\n",
      "  model.language_model.layers.4.self_attn.q_proj\n",
      "  model.language_model.layers.5.self_attn.q_proj\n",
      "  model.language_model.layers.6.self_attn.q_proj\n",
      "  model.language_model.layers.7.self_attn.q_proj\n",
      "  model.language_model.layers.8.self_attn.q_proj\n",
      "  model.language_model.layers.9.self_attn.q_proj\n",
      "  model.language_model.layers.10.self_attn.q_proj\n",
      "  model.language_model.layers.11.self_attn.q_proj\n",
      "  model.language_model.layers.12.self_attn.q_proj\n",
      "  model.language_model.layers.13.self_attn.q_proj\n",
      "  model.language_model.layers.14.self_attn.q_proj\n",
      "  model.language_model.layers.15.self_attn.q_proj\n",
      "  model.language_model.layers.16.self_attn.q_proj\n",
      "  model.language_model.layers.17.self_attn.q_proj\n",
      "  model.language_model.layers.18.self_attn.q_proj\n",
      "  model.language_model.layers.19.self_attn.q_proj\n",
      "  model.language_model.layers.20.self_attn.q_proj\n",
      "  model.language_model.layers.21.self_attn.q_proj\n",
      "  model.language_model.layers.22.self_attn.q_proj\n",
      "  model.language_model.layers.23.self_attn.q_proj\n",
      "  model.language_model.layers.24.self_attn.q_proj\n",
      "  model.language_model.layers.25.self_attn.q_proj\n",
      "  model.language_model.layers.26.self_attn.q_proj\n",
      "  model.language_model.layers.27.self_attn.q_proj\n",
      "\n",
      "q_proj in vision encoder: 0\n",
      "CONFIRMED: q_proj exists ONLY in LLM, NOT in vision encoder\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SEARCH FOR q_proj MODULES\n",
    "# ============================================================\n",
    "# Expected: Found ONLY in LLM (language_model.layers.*.self_attn.q_proj)\n",
    "\n",
    "q_proj_modules = find_modules_by_pattern(model_4bit, \"q_proj\")\n",
    "\n",
    "# Check if any are in vision encoder\n",
    "q_proj_in_vision = [m for m in q_proj_modules if \"visual\" in m]\n",
    "print(f\"\\nq_proj in vision encoder: {len(q_proj_in_vision)}\")\n",
    "if len(q_proj_in_vision) == 0:\n",
    "    print(\"CONFIRMED: q_proj exists ONLY in LLM, NOT in vision encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdfe9bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Modules containing 'qkv'\n",
      "============================================================\n",
      "Total found: 32\n",
      "  model.visual.blocks.0.attn.qkv\n",
      "  model.visual.blocks.1.attn.qkv\n",
      "  model.visual.blocks.2.attn.qkv\n",
      "  model.visual.blocks.3.attn.qkv\n",
      "  model.visual.blocks.4.attn.qkv\n",
      "  model.visual.blocks.5.attn.qkv\n",
      "  model.visual.blocks.6.attn.qkv\n",
      "  model.visual.blocks.7.attn.qkv\n",
      "  model.visual.blocks.8.attn.qkv\n",
      "  model.visual.blocks.9.attn.qkv\n",
      "  model.visual.blocks.10.attn.qkv\n",
      "  model.visual.blocks.11.attn.qkv\n",
      "  model.visual.blocks.12.attn.qkv\n",
      "  model.visual.blocks.13.attn.qkv\n",
      "  model.visual.blocks.14.attn.qkv\n",
      "  ... (2 more) ...\n",
      "  model.visual.blocks.17.attn.qkv\n",
      "  model.visual.blocks.18.attn.qkv\n",
      "  model.visual.blocks.19.attn.qkv\n",
      "  model.visual.blocks.20.attn.qkv\n",
      "  model.visual.blocks.21.attn.qkv\n",
      "  model.visual.blocks.22.attn.qkv\n",
      "  model.visual.blocks.23.attn.qkv\n",
      "  model.visual.blocks.24.attn.qkv\n",
      "  model.visual.blocks.25.attn.qkv\n",
      "  model.visual.blocks.26.attn.qkv\n",
      "  model.visual.blocks.27.attn.qkv\n",
      "  model.visual.blocks.28.attn.qkv\n",
      "  model.visual.blocks.29.attn.qkv\n",
      "  model.visual.blocks.30.attn.qkv\n",
      "  model.visual.blocks.31.attn.qkv\n",
      "\n",
      "qkv in LLM: 0\n",
      "CONFIRMED: qkv exists ONLY in Vision encoder, NOT in LLM\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SEARCH FOR qkv MODULES\n",
    "# ============================================================\n",
    "# Expected: Found ONLY in Vision encoder (model.visual.blocks.*.attn.qkv)\n",
    "\n",
    "qkv_modules = find_modules_by_pattern(model_4bit, \"qkv\")\n",
    "\n",
    "# Check if any are in LLM\n",
    "qkv_in_llm = [m for m in qkv_modules if \"language_model\" in m]\n",
    "print(f\"\\nqkv in LLM: {len(qkv_in_llm)}\")\n",
    "if len(qkv_in_llm) == 0:\n",
    "    print(\"CONFIRMED: qkv exists ONLY in Vision encoder, NOT in LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d19084",
   "metadata": {},
   "source": [
    "### Q2 Answer: Layer Naming Conventions\n",
    "\n",
    "| Layer Pattern | Location | Full Path Example |\n",
    "|--------------|----------|-------------------|\n",
    "| `q_proj`, `k_proj`, `v_proj`, `o_proj` | **LLM only** | `model.language_model.layers.0.self_attn.q_proj` |\n",
    "| `qkv` (combined Q,K,V) | **Vision encoder only** | `model.visual.blocks.0.attn.qkv` |\n",
    "| `gate_proj`, `up_proj`, `down_proj` | **LLM only** | `model.language_model.layers.0.mlp.gate_proj` |\n",
    "| `fc1`, `fc2` | **Vision encoder only** | `model.visual.blocks.0.mlp.fc1` |\n",
    "\n",
    "**Key Insight**: The LLM and Vision encoder use **different naming conventions**:\n",
    "- **LLM (Qwen2)**: Separate Q, K, V, O projections (`q_proj`, `k_proj`, etc.)\n",
    "- **Vision (ViT)**: Combined QKV projection (`qkv`) - more efficient for ViT architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21633db1",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Q3 - get_peft_model LoRA Target Verification\n",
    "\n",
    "**Question**: After `get_peft_model()`, which modules become trainable?\n",
    "\n",
    "We'll:\n",
    "1. Define LoRA config targeting LLM layers\n",
    "2. Apply `get_peft_model()`\n",
    "3. Verify only LoRA adapters are trainable\n",
    "4. Confirm vision encoder remains frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89523604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Config:\n",
      "  Rank (r): 64\n",
      "  Alpha: 16\n",
      "  Target modules: {'o_proj', 'gate_proj', 'v_proj', 'up_proj', 'down_proj', 'k_proj', 'q_proj'}\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ============================================================\n",
    "# DEFINE LORA CONFIG (LLM TARGETS ONLY)\n",
    "# ============================================================\n",
    "# These target modules exist ONLY in the LLM, not vision encoder\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # LLM attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",      # LLM MLP\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"LoRA Config:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f2b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying get_peft_model()...\n",
      "\n",
      "LoRA adapters attached!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# APPLY get_peft_model\n",
    "# ============================================================\n",
    "print(\"\\nApplying get_peft_model()...\")\n",
    "\n",
    "# Note: For 4-bit models, you typically call prepare_model_for_kbit_training first\n",
    "# But for this diagnostic, we'll apply LoRA directly to see the effect\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model_prepared = prepare_model_for_kbit_training(model_4bit)\n",
    "peft_model = get_peft_model(model_prepared, lora_config)\n",
    "\n",
    "print(\"\\nLoRA adapters attached!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a3dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4-bit + LoRA (after get_peft_model)\n",
      "============================================================\n",
      "Total params:     4,853,357,056\n",
      "Trainable params: 161,480,704 (3.3272%)\n",
      "\n",
      "Trainable params by area:\n",
      "  language_model: 392 parameters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHECK TRAINABLE PARAMETERS AFTER get_peft_model\n",
    "# ============================================================\n",
    "\n",
    "trainable_names_after_lora = summarize_trainables_by_area(\n",
    "    peft_model,\n",
    "    \"4-bit + LoRA (after get_peft_model)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b56908b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICATION: Vision Encoder Trainable Parameters\n",
      "============================================================\n",
      "Vision encoder trainable params: 0\n",
      "CONFIRMED: Vision encoder is COMPLETELY FROZEN\n",
      "           Only LLM LoRA adapters are trainable\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EXPLICIT CHECK: Are any vision encoder params trainable?\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION: Vision Encoder Trainable Parameters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vision_trainables = [n for n in trainable_names_after_lora if \"visual\" in n]\n",
    "print(f\"Vision encoder trainable params: {len(vision_trainables)}\")\n",
    "\n",
    "if len(vision_trainables) == 0:\n",
    "    print(\"CONFIRMED: Vision encoder is COMPLETELY FROZEN\")\n",
    "    print(\"           Only LLM LoRA adapters are trainable\")\n",
    "else:\n",
    "    print(\"WARNING: Some vision params are trainable!\")\n",
    "    for n in vision_trainables[:10]:\n",
    "        print(f\"  - {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a5b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample of Trainable Parameters (first 15)\n",
      "============================================================\n",
      "   1. base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "   2. base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "   3. base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "   4. base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "   5. base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "   6. base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "   7. base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "   8. base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "   9. base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "  10. base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "  11. base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "  12. base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "  13. base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "  14. base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "  15. base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "  ... and 377 more\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SHOW SAMPLE OF TRAINABLE PARAMETER NAMES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample of Trainable Parameters (first 15)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, name in enumerate(trainable_names_after_lora[:15]):\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "\n",
    "if len(trainable_names_after_lora) > 15:\n",
    "    print(f\"  ... and {len(trainable_names_after_lora) - 15} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "117788de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FULL PEFT MODEL STRUCTURE (with LoRA adapters)\n",
      "============================================================\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2VLForConditionalGeneration(\n",
      "      (model): Qwen2VLModel(\n",
      "        (visual): Qwen2VisionTransformerPretrainedModel(\n",
      "          (patch_embed): PatchEmbed(\n",
      "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "          )\n",
      "          (rotary_pos_emb): VisionRotaryEmbedding()\n",
      "          (blocks): ModuleList(\n",
      "            (0-31): 32 x Qwen2VLVisionBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): VisionAttention(\n",
      "                (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
      "                (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
      "              )\n",
      "              (mlp): VisionMlp(\n",
      "                (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "                (act): QuickGELUActivation()\n",
      "                (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (merger): PatchMerger(\n",
      "            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear4bit(in_features=5120, out_features=3584, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (language_model): Qwen2VLTextModel(\n",
      "          (embed_tokens): Embedding(152064, 3584)\n",
      "          (layers): ModuleList(\n",
      "            (0-27): 28 x Qwen2VLDecoderLayer(\n",
      "              (self_attn): Qwen2VLAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "              )\n",
      "              (mlp): Qwen2MLP(\n",
      "                (gate_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=18944, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (up_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=18944, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (down_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=18944, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            )\n",
      "          )\n",
      "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRINT FULL PEFT MODEL STRUCTURE\n",
    "# ============================================================\n",
    "# This shows the complete model architecture with LoRA adapters inserted.\n",
    "# Look for \"lora_A\" and \"lora_B\" modules - these are the trainable adapters.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL PEFT MODEL STRUCTURE (with LoRA adapters)\")\n",
    "print(\"=\"*60)\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663a518",
   "metadata": {},
   "source": [
    "### Q3 Answer: get_peft_model Trainable Modules\n",
    "\n",
    "After calling `get_peft_model()` with LLM target modules:\n",
    "\n",
    "| Component | Trainable? | Why? |\n",
    "|-----------|-----------|------|\n",
    "| **LLM LoRA adapters** (`lora_A`, `lora_B`) | Yes | These are the new low-rank matrices added by LoRA |\n",
    "| **LLM base weights** | No | Frozen by 4-bit quantization |\n",
    "| **Vision encoder** | No | Not targeted by LoRA config |\n",
    "\n",
    "**Key Insight**:\n",
    "- LoRA adds new trainable parameters (`lora_A.weight`, `lora_B.weight`) to each target module\n",
    "- The original weights remain frozen\n",
    "- Only modules matching `target_modules` get LoRA adapters\n",
    "- Since `q_proj` etc. only exist in LLM, vision encoder is untouched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ec544",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Summary & Key Takeaways\n",
    "\n",
    "### Answers to All 3 Questions:\n",
    "\n",
    "**Q1: What does quantization do to trainable layers?**\n",
    "> 4-bit quantization only quantizes **Linear layers** (stored as 4-bit integers).\n",
    "> Non-linear layers (LayerNorm, embeddings, lm_head) remain in bf16 with `requires_grad=True`.\n",
    "> However, quantized Linear layers cannot be effectively trained, so we need LoRA adapters.\n",
    "> `prepare_model_for_kbit_training()` freezes everything, then `get_peft_model()` adds trainable LoRA.\n",
    "\n",
    "**Q2: Do `q_proj` layers apply only to LLM or also vision encoder?**\n",
    "> `q_proj` exists **ONLY in the LLM** (language_model.layers.*.self_attn.q_proj).\n",
    "> Vision encoder uses **`qkv`** (combined Q,K,V) instead (model.visual.blocks.*.attn.qkv).\n",
    "\n",
    "**Q3: After `get_peft_model()`, which modules become trainable?**\n",
    "> Only the **LoRA adapter weights** (lora_A, lora_B) are trainable.\n",
    "> With LLM-only target modules, **vision encoder remains completely frozen**.\n",
    "\n",
    "---\n",
    "\n",
    "### Layer Naming Quick Reference:\n",
    "\n",
    "```\n",
    "LLM (Qwen2) Layers:\n",
    "  - Attention: q_proj, k_proj, v_proj, o_proj\n",
    "  - MLP: gate_proj, up_proj, down_proj\n",
    "  - Path: model.language_model.layers.{i}.self_attn.q_proj\n",
    "\n",
    "Vision Encoder (ViT) Layers:\n",
    "  - Attention: qkv (combined), proj (output)\n",
    "  - MLP: fc1, fc2\n",
    "  - Path: model.visual.blocks.{i}.attn.qkv\n",
    "```\n",
    "\n",
    "### To Train Vision Encoder with LoRA:\n",
    "```python\n",
    "# Add vision targets to your LoRA config:\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # LLM\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",      # LLM MLP\n",
    "    \"qkv\", \"fc1\", \"fc2\",                      # Vision encoder\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bbb445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Cleanup] Deleted peft_model\n",
      "[Cleanup] GPU memory after cleanup: 10.08 GB\n",
      "\n",
      "Notebook complete!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "cleanup_model(peft_model, \"peft_model\")\n",
    "print(\"\\nNotebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
