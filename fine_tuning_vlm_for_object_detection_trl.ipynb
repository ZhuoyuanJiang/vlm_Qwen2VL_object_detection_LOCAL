{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d954aa3",
   "metadata": {
    "id": "vKadZFQ2IdJb"
   },
   "source": [
    "# Fine-Tuning Qwen2-VL-7B for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbafba",
   "metadata": {},
   "source": [
    "## üñ•Ô∏è GPU Configuration and Selection\n",
    "\n",
    "**AUTOMATIC GPU SELECTION**: The script automatically finds available GPUs!\n",
    "\n",
    "**Original Paper Requirements:**\n",
    "- Recommended: 2x A6000 (48GB each) or 1x A100 (40/80GB)\n",
    "- Your hardware: RTX 6000 Ada (48GB) - equivalent to A6000\n",
    "\n",
    "**How it works:**\n",
    "1. Automatically scans all GPUs for availability\n",
    "2. Selects GPUs with <2GB memory used and <10% utilization\n",
    "3. Can use 1 or 2 GPUs (configurable with `prefer_multi_gpu`)\n",
    "4. Falls back to the GPU with most free memory if auto-selection fails\n",
    "\n",
    "**To use 2 GPUs** (like original paper): Set `prefer_multi_gpu=True` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU CONFIGURATION - MUST RUN FIRST!\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "# Note: torch import moved below after setting CUDA_VISIBLE_DEVICES\n",
    "\n",
    "# Set memory management for better GPU utilization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# CONFIGURATION: Set this based on your needs\n",
    "# Original paper uses 2x A6000 (48GB each), but 1x RTX 6000 Ada (48GB) may be sufficient\n",
    "USE_DUAL_GPU = True  # Set to True to use 2 GPUs like the original paper, False for 1 GPU\n",
    "MAX_GPUS = 2  # Maximum number of GPUs to use (to avoid occupying all lab resources)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üñ•Ô∏è  AUTOMATIC GPU CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Function to find available GPUs\n",
    "def find_available_gpus(num_gpus_needed=1):\n",
    "    \"\"\"Find available GPUs by checking memory usage.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total', \n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        \n",
    "        available = []\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            parts = line.split(',')\n",
    "            gpu_idx = int(parts[0])\n",
    "            mem_used = int(parts[1])\n",
    "            mem_total = int(parts[2])\n",
    "            \n",
    "            # Consider GPU available if <2GB used and has at least 40GB total\n",
    "            if mem_used < 2000 and mem_total > 40000:\n",
    "                available.append(gpu_idx)\n",
    "        \n",
    "        # Return exactly the number of GPUs requested (not more!)\n",
    "        # IMPORTANT: This ensures we only use the requested number of GPUs\n",
    "        # to avoid occupying all lab resources\n",
    "        if len(available) >= num_gpus_needed:\n",
    "            return available[:num_gpus_needed]  # Take ONLY what we need, not all available\n",
    "        else:\n",
    "            return available  # Return what we have (might be less than requested)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking GPUs: {e}\")\n",
    "        return []\n",
    "\n",
    "# Determine how many GPUs to use (enforce MAX_GPUS limit)\n",
    "num_gpus = 2 if USE_DUAL_GPU else 1\n",
    "num_gpus = min(num_gpus, MAX_GPUS)  # Never exceed MAX_GPUS\n",
    "print(f\"Configuration: {'DUAL GPU (like original paper)' if USE_DUAL_GPU else 'SINGLE GPU (test mode)'}\")\n",
    "print(f\"Looking for {num_gpus} available GPU(s) (max {MAX_GPUS} to preserve lab resources)...\")\n",
    "\n",
    "# Find available GPUs\n",
    "available_gpus = find_available_gpus(num_gpus)\n",
    "\n",
    "if len(available_gpus) > 0:\n",
    "    # Ensure we never use more than MAX_GPUS even if more are found\n",
    "    available_gpus = available_gpus[:MAX_GPUS]\n",
    "    gpu_string = ','.join(str(g) for g in available_gpus)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_string\n",
    "    print(f\"‚úÖ Found {len(available_gpus)} available GPU(s): {available_gpus}\")\n",
    "    if USE_DUAL_GPU and len(available_gpus) == 1:\n",
    "        print(\"‚ö†Ô∏è Warning: Only 1 GPU available, but 2 were requested\")\n",
    "else:\n",
    "    # Try to find ANY available GPU as fallback\n",
    "    print(\"‚ö†Ô∏è No free GPUs found with standard criteria, checking all GPUs...\")\n",
    "    \n",
    "    # Try to find the GPU with most free memory\n",
    "    best_gpu = None\n",
    "    best_free_mem = 0\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total', \n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        \n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            parts = line.split(',')\n",
    "            gpu_idx = int(parts[0])\n",
    "            mem_used = int(parts[1])\n",
    "            mem_total = int(parts[2])\n",
    "            free_mem = mem_total - mem_used\n",
    "            \n",
    "            print(f\"   GPU {gpu_idx}: {free_mem/1024:.1f} GB free\")\n",
    "            \n",
    "            if free_mem > best_free_mem and free_mem > 20000:  # At least 20GB free\n",
    "                best_gpu = gpu_idx\n",
    "                best_free_mem = free_mem\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if best_gpu is not None:\n",
    "        # For fallback, still respect the MAX_GPUS limit (use only 1 GPU in fallback)\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(best_gpu)\n",
    "        print(f\"‚úÖ Using GPU {best_gpu} with {best_free_mem/1024:.1f} GB free memory\")\n",
    "        print(f\"   (Using single GPU in fallback mode to preserve lab resources)\")\n",
    "    else:\n",
    "        print(\"‚ùå No GPUs available with sufficient memory!\")\n",
    "        print(\"   Please free up GPU memory or wait for GPUs to become available\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Verify configuration\n",
    "print(f\"\\nCUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "\n",
    "# Import torch AFTER setting CUDA_VISIBLE_DEVICES\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    total_memory = 0\n",
    "    print(f\"PyTorch sees {device_count} GPU(s):\")\n",
    "    \n",
    "    # Verify we're not using more than MAX_GPUS\n",
    "    if device_count > MAX_GPUS:\n",
    "        print(f\"‚ö†Ô∏è ERROR: PyTorch sees {device_count} GPUs but MAX_GPUS={MAX_GPUS}\")\n",
    "        print(\"   This should not happen - CUDA_VISIBLE_DEVICES may not be set correctly\")\n",
    "        \n",
    "    for i in range(device_count):\n",
    "        mem_gb = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        total_memory += mem_gb\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} ({mem_gb:.1f} GB)\")\n",
    "    print(f\"\\nTotal GPU memory available: {total_memory:.1f} GB\")\n",
    "    \n",
    "    # Final confirmation\n",
    "    print(f\"\\n‚úÖ CONFIRMED: Using exactly {device_count} GPU(s) (max allowed: {MAX_GPUS})\")\n",
    "    \n",
    "    if device_count > 1:\n",
    "        print(\"\\nüìù Multi-GPU Training Notes:\")\n",
    "        print(\"  - Model will automatically use DataParallel\")\n",
    "        print(\"  - Effective batch size = per_device_batch_size √ó 2\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available! Exiting...\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61d5ba",
   "metadata": {
    "id": "rdc7yvCQ7JGf"
   },
   "source": [
    "## üåü WHAT?\n",
    "\n",
    "In this notebook, you will learn how to fine-tune [Qwen2-VL-7B](https://qwenlm.github.io/blog/qwen2-vl/) for for detecting nutrition tables from product images using Hugging Face.\n",
    "\n",
    "![Image of nutrition table detection](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*tcy5oCWmHT3jeVN7Lw-FpQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c6c2e",
   "metadata": {
    "id": "NZ84pmL57JGf"
   },
   "source": [
    "üí° You can execute this Jupyter Notebook on a remote machine and then access and interact with it in your local web browser, leveraging the remote machine's computational resources.\n",
    "- On remote: jupyter notebook --no-browser --port=8080\n",
    "- On local: ssh -L 8080:localhost:8080 ntajbakhsh@workstation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f5806",
   "metadata": {
    "id": "JATmSI8mcyW2"
   },
   "source": [
    "üö® **WARNING**: Please note that QWEN2-VL-7B is a relatively large model, requiring significant computational resources for fine-tuning. I recommend using either 2x A6000 or 1x A100 GPUs to ensure sufficient memory and processing power. While I haven't experimented with other GPUs, you're welcome to try alternative options. However, please be aware that other GPUs may not have enough memory to accommodate the model and optimizer states during training.\n",
    "\n",
    "üö® **WARNING**: Training transformers can be significantly more memory-efficient with Flash Attention (FA) compared to traditional attention mechanisms. However, FA support is currently limited to Nvidia's Ampere series of GPUs (A100, A6000, etc.) or better. If you're using an older GPU generation, please note that you'll need to disable FA to avoid error messages. Keep in mind that disabling FA may require using additional GPUs to compensate for the reduced memory efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee478d4",
   "metadata": {
    "id": "gSHmDKNFoqjC"
   },
   "source": [
    "# 1. Install Dependencies\n",
    "\n",
    "Let‚Äôs start by installing the essential libraries we‚Äôll need for fine-tuning! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a53884",
   "metadata": {
    "id": "GCMhPmFdIGSb"
   },
   "outputs": [],
   "source": [
    "# SKIP THIS CELL - Already installed in environment\n",
    "# !pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
    "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1\n",
    "# !pip install  matplotlib IPython\n",
    "print(\"‚úÖ Dependencies already installed in conda environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e854c",
   "metadata": {
    "id": "J4pAvoQaOJ1M"
   },
   "source": [
    "We‚Äôll also need to install an earlier version of *PyTorch*, as the latest version has an issue that currently prevents this notebook from running correctly. You can learn more about the issue [here](https://github.com/pytorch/pytorch/issues/138340) and consider updating to the latest version once it‚Äôs resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d5cc5",
   "metadata": {
    "id": "D8iRteA4oXVj"
   },
   "outputs": [],
   "source": [
    "# SKIP THIS CELL - PyTorch already installed\n",
    "# !pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "print(\"‚úÖ PyTorch 2.4.1 with CUDA 12.1 already installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a69674",
   "metadata": {
    "id": "upZjD4bH7JGh"
   },
   "source": [
    "# 2. HF Login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118663e",
   "metadata": {
    "id": "V0-2Lso6wkIh"
   },
   "source": [
    "Log in to Hugging Face to upload your fine-tuned model! üóùÔ∏è\n",
    "\n",
    "You‚Äôll need to authenticate with your Hugging Face account to save and share your model directly from this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86afbc1",
   "metadata": {
    "id": "xcL4-bwGIoaR"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.environ['HF_TOKEN']) # export your HF_TOKEN first. You can add this to your ~/.bashrc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e2d65",
   "metadata": {
    "id": "SMZD2Wv57JGi"
   },
   "source": [
    "## Optional Settings for an Improved Jupyter Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e540d9",
   "metadata": {
    "id": "N2BTm9Ug7JGi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ddc74",
   "metadata": {
    "id": "g9QXwbJ7ovM5"
   },
   "source": [
    "# 2. Load and Understand Dataset üìÅ\n",
    "\n",
    "In this section, you should load the [openfoodfacts/nutrition-table-detection](https://huggingface.co/datasets/openfoodfacts/nutrition-table-detection) dataset. This dataset contains product images, the extracted bar codes, and bounding boxes for the nutrition tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270a1b0",
   "metadata": {},
   "source": [
    "üìö **Note**: For detailed dataset exploration and analysis, see [`notebooks/01_dataset_exploration.ipynb`](notebooks/01_dataset_exploration.ipynb).\n",
    "\n",
    "That notebook provides in-depth analysis including:\n",
    "- Image size distributions and statistics\n",
    "- Bounding box characteristics (aspect ratios, coverage, positions)\n",
    "- Visualization of multiple examples\n",
    "- Recommendations for anchor configuration\n",
    "\n",
    "Below shows a streamlined version sufficient for understanding the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745029bc",
   "metadata": {
    "id": "HKIHSAHX7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: load the dataset into training and evaluation sets\n",
    "from datasets import load_dataset\n",
    "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
    "\n",
    "# Load the dataset with train and validation splits\n",
    "ds = load_dataset(dataset_id)\n",
    "\n",
    "# inspect the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(ds)\n",
    "\n",
    "# split the dataset into training and evaluation sets\n",
    "train_dataset = ds['train']\n",
    "eval_dataset = ds['val']  \n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Dataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f81098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: inspect the content of a training example\n",
    "# Let's look at the first training example\n",
    "import pprint\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET FEATURES BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã FEATURE SCHEMA:\")\n",
    "for feature_name, feature_type in train_dataset.features.items():\n",
    "    print(f\"  {feature_name:12} : {feature_type}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INSPECTING train_dataset[0] - FIRST TRAINING EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "example = train_dataset[0]\n",
    "\n",
    "print(f\"\\nüîç COMPLETE RAW DATA FOR train_dataset[0]:\")\n",
    "print(f\"\\ntrain_dataset[0]['image_id']:\")\n",
    "print(f\"  {example['image_id']}\")\n",
    "\n",
    "print(f\"\\ntrain_dataset[0]['image']:\")\n",
    "print(f\"  {example['image']}\")\n",
    "\n",
    "print(f\"\\ntrain_dataset[0]['width']:\")\n",
    "print(f\"  {example['width']}\")\n",
    "\n",
    "print(f\"\\ntrain_dataset[0]['height']:\")\n",
    "print(f\"  {example['height']}\")\n",
    "\n",
    "print(f\"\\ntrain_dataset[0]['meta']:\")\n",
    "pp = pprint.PrettyPrinter(indent=4, width=80)\n",
    "pp.pprint(example['meta'])\n",
    "\n",
    "print(f\"\\ntrain_dataset[0]['objects']:\")\n",
    "pp.pprint(example['objects'])\n",
    "\n",
    "print(f\"\\nüìù SUMMARY:\")\n",
    "print(f\"  We are examining: train_dataset[0] (first element of training set)\")\n",
    "print(f\"  Total keys in this example: {list(example.keys())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HUMAN-READABLE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Now let's format the same data in a more readable way:\")\n",
    "\n",
    "print(f\"\\nüÜî BASIC INFO:\")\n",
    "print(f\"  Image ID     : {example['image_id']}\")\n",
    "print(f\"  Dimensions   : {example['width']} x {example['height']}\")\n",
    "print(f\"  Image Object : {example['image']}\")\n",
    "\n",
    "print(f\"\\nüìä METADATA:\")\n",
    "pp.pprint(example['meta'])\n",
    "\n",
    "print(f\"\\nüéØ ANNOTATIONS:\")\n",
    "print(f\"  Number of objects: {len(example['objects']['category_name'])}\")\n",
    "print(f\"  Category name       : {example['objects']['category_name']}\")\n",
    "print(f\"  Category IDs     : {example['objects']['category_id']}\")\n",
    "print(f\"  Bounding boxes   :\")\n",
    "for i, bbox in enumerate(example['objects']['bbox']):\n",
    "    print(f\"    Object {i}: [y_min={bbox[0]:.3f}, x_min={bbox[1]:.3f}, y_max={bbox[2]:.3f}, x_max={bbox[3]:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: why the bbox coordinates are between 0 and 1? can you overlay the bbox on the image for one example?\n",
    "\n",
    "# The bbox coordinates are normalized to [0, 1] to make them resolution-independent.\n",
    "# This is a common practice in object detection to handle images of different sizes.\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TASK: display the image with bounding box\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMAGE VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Displaying original image vs. image with bounding box overlay:\")\n",
    "\n",
    "# Get the bounding box coordinates (already normalized to 0-1 from dataset)\n",
    "bbox = example['objects']['bbox'][0]\n",
    "\n",
    "# IMPORTANT: Check dataset dimensions vs PIL image dimensions\n",
    "width = example['width']\n",
    "height = example['height']\n",
    "pil_width, pil_height = example['image'].size\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Checking dimensions:\")\n",
    "print(f\"  Dataset width x height: {width} x {height}\")\n",
    "print(f\"  PIL image size: {pil_width} x {pil_height}\")\n",
    "\n",
    "# Show the raw bbox values for reference\n",
    "print(f\"\\nüì¶ Raw bbox values (normalized [0,1]):\")\n",
    "print(f\"  bbox = {bbox}\")\n",
    "print(f\"  Format: [y_min, x_min, y_max, x_max] (OpenFoodFacts convention)\")\n",
    "\n",
    "# CRITICAL: Always use PIL dimensions for visualization since we're drawing on PIL image\n",
    "# The dataset bbox is normalized [0,1] regardless of actual image size\n",
    "width = pil_width\n",
    "height = pil_height\n",
    "\n",
    "# Convert normalized [0,1] coordinates to pixel coordinates\n",
    "# CRITICAL: OpenFoodFacts dataset format is [y_min, x_min, y_max, x_max] NOT [x_min, y_min, x_max, y_max]!\n",
    "y_min, x_min, y_max, x_max = bbox  # Unpack in correct order\n",
    "x_min = x_min * width\n",
    "y_min = y_min * height\n",
    "x_max = x_max * width\n",
    "y_max = y_max * height\n",
    "\n",
    "print(f\"\\nüìê Converted to pixels:\")\n",
    "print(f\"  Pixel coords - Top-left: ({x_min:.1f}, {y_min:.1f})\")\n",
    "print(f\"  Pixel coords - Bottom-right: ({x_max:.1f}, {y_max:.1f})\")\n",
    "print(f\"  Box size: {x_max-x_min:.1f} x {y_max-y_min:.1f} pixels\")\n",
    "print(f\"  Image size: {width} x {height} pixels\")\n",
    "\n",
    "# Create side-by-side comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: Original image (should be clean, no bounding box)\n",
    "_ = ax1.imshow(example['image'])\n",
    "_ = ax1.set_title(\"Original Image\")\n",
    "_ = ax1.axis('off')\n",
    "\n",
    "# Right: Image with bounding box overlay\n",
    "img_with_bbox = example['image'].copy()  # Important: make a copy to avoid modifying original\n",
    "draw = ImageDraw.Draw(img_with_bbox)\n",
    "\n",
    "# Draw the bounding box\n",
    "draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "\n",
    "_ = ax2.imshow(img_with_bbox)\n",
    "_ = ax2.set_title(f\"With Bounding Box: {example['objects']['category_name'][0]}\")\n",
    "_ = ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization complete!\")\n",
    "print(f\"   Category detected: {example['objects']['category_name'][0]}\")\n",
    "print(f\"   Image dimensions: {width} x {height} pixels\")\n",
    "\n",
    "# Verification summary\n",
    "print(f\"\\n‚úÖ BBOX Format Confirmed: OpenFoodFacts uses [y_min, x_min, y_max, x_max]\")\n",
    "print(f\"   Image aspect ratio: {width/height:.2f}\")\n",
    "print(f\"   BBox aspect ratio: {(x_max-x_min)/(y_max-y_min):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0ca97",
   "metadata": {},
   "source": [
    "### For more visualization examples, please look at `notebooks/01_dataset_exploration.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc869e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the histogram of the image sizes\n",
    "# get the histogram of the #bounding boxes per image - important for finetuning the model\n",
    "\n",
    "# TASK: analyze dataset statistics for model preparation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Analyzing image dimensions and bounding box distributions:\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Collect statistics from all training examples\n",
    "image_widths = []\n",
    "image_heights = []\n",
    "image_areas = []\n",
    "num_bboxes_per_image = []\n",
    "\n",
    "print(f\"\\nProcessing {len(train_dataset)} training examples...\")\n",
    "\n",
    "for idx, example in enumerate(train_dataset):\n",
    "    # Get image dimensions\n",
    "    width = example['width']\n",
    "    height = example['height'] \n",
    "    area = width * height\n",
    "    \n",
    "    # Count bounding boxes in this image\n",
    "    num_bboxes = len(example['objects']['bbox'])\n",
    "    \n",
    "    # Store statistics\n",
    "    image_widths.append(width)\n",
    "    image_heights.append(height)\n",
    "    image_areas.append(area)\n",
    "    num_bboxes_per_image.append(num_bboxes)\n",
    "\n",
    "print(f\"‚úÖ Data collection complete!\")\n",
    "\n",
    "# Convert to numpy arrays for easier analysis\n",
    "image_widths = np.array(image_widths)\n",
    "image_heights = np.array(image_heights)\n",
    "image_areas = np.array(image_areas)\n",
    "num_bboxes_per_image = np.array(num_bboxes_per_image)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nüìä IMAGE DIMENSIONS SUMMARY:\")\n",
    "print(f\"  Width  - Min: {image_widths.min():4d}px | Max: {image_widths.max():4d}px | Mean: {image_widths.mean():.1f}px\")\n",
    "print(f\"  Height - Min: {image_heights.min():4d}px | Max: {image_heights.max():4d}px | Mean: {image_heights.mean():.1f}px\")\n",
    "print(f\"  Area   - Min: {image_areas.min():8.0f} | Max: {image_areas.max():8.0f} | Mean: {image_areas.mean():.0f}\")\n",
    "\n",
    "print(f\"\\nüéØ BOUNDING BOXES SUMMARY:\")\n",
    "print(f\"  Min boxes per image: {num_bboxes_per_image.min()}\")\n",
    "print(f\"  Max boxes per image: {num_bboxes_per_image.max()}\")\n",
    "print(f\"  Mean boxes per image: {num_bboxes_per_image.mean():.2f}\")\n",
    "print(f\"  Total bounding boxes: {num_bboxes_per_image.sum()}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Image widths histogram\n",
    "_ = ax1.hist(image_widths, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "_ = ax1.set_xlabel('Image Width (pixels)')\n",
    "_ = ax1.set_ylabel('Frequency')\n",
    "_ = ax1.set_title('Distribution of Image Widths')\n",
    "_ = ax1.grid(True, alpha=0.3)\n",
    "_ = ax1.axvline(image_widths.mean(), color='red', linestyle='--', label=f'Mean: {image_widths.mean():.0f}px')\n",
    "_ = ax1.legend()\n",
    "\n",
    "# 2. Image heights histogram  \n",
    "_ = ax2.hist(image_heights, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "_ = ax2.set_xlabel('Image Height (pixels)')\n",
    "_ = ax2.set_ylabel('Frequency')\n",
    "_ = ax2.set_title('Distribution of Image Heights')\n",
    "_ = ax2.grid(True, alpha=0.3)\n",
    "_ = ax2.axvline(image_heights.mean(), color='red', linestyle='--', label=f'Mean: {image_heights.mean():.0f}px')\n",
    "_ = ax2.legend()\n",
    "\n",
    "# 3. Image areas histogram\n",
    "_ = ax3.hist(image_areas/1e6, bins=30, alpha=0.7, color='orange', edgecolor='black')  # Convert to megapixels\n",
    "_ = ax3.set_xlabel('Image Area (Megapixels)')\n",
    "_ = ax3.set_ylabel('Frequency')\n",
    "_ = ax3.set_title('Distribution of Image Areas')\n",
    "_ = ax3.grid(True, alpha=0.3)\n",
    "_ = ax3.axvline(image_areas.mean()/1e6, color='red', linestyle='--', label=f'Mean: {image_areas.mean()/1e6:.1f}MP')\n",
    "_ = ax3.legend()\n",
    "\n",
    "# 4. Number of bounding boxes histogram - CRITICAL FOR MODEL TUNING\n",
    "bbox_counts = np.bincount(num_bboxes_per_image)\n",
    "bbox_labels = np.arange(len(bbox_counts))\n",
    "_ = ax4.bar(bbox_labels, bbox_counts, alpha=0.7, color='coral', edgecolor='black')\n",
    "_ = ax4.set_xlabel('Number of Bounding Boxes per Image')\n",
    "_ = ax4.set_ylabel('Number of Images')\n",
    "_ = ax4.set_title('Distribution of Bounding Boxes per Image\\n(Critical for Model Configuration)')\n",
    "_ = ax4.grid(True, alpha=0.3)\n",
    "_ = ax4.set_xticks(bbox_labels)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, count in enumerate(bbox_counts):\n",
    "    if count > 0:\n",
    "        percentage = (count / len(train_dataset)) * 100\n",
    "        _ = ax4.text(i, count + 0.5, f'{count}\\n({percentage:.1f}%)', \n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional insights for model preparation\n",
    "print(f\"\\nüîß MODEL PREPARATION INSIGHTS:\")\n",
    "print(f\"  Most common image aspect ratio: {image_widths.mean()/image_heights.mean():.2f} (width/height)\")\n",
    "\n",
    "unique_bbox_counts, bbox_frequencies = np.unique(num_bboxes_per_image, return_counts=True)\n",
    "print(f\"\\n  Bounding box distribution breakdown:\")\n",
    "for count, freq in zip(unique_bbox_counts, bbox_frequencies):\n",
    "    percentage = (freq / len(train_dataset)) * 100\n",
    "    print(f\"    {count} box(es): {freq:3d} images ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "if num_bboxes_per_image.max() == 1:\n",
    "    print(f\"  ‚úÖ Single object detection - simpler model configuration\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Multi-object detection - configure model for up to {num_bboxes_per_image.max()} objects\")\n",
    "\n",
    "print(f\"  üìê Consider input resolution around {int(np.sqrt(image_areas.mean())):d}√ó{int(np.sqrt(image_areas.mean())):d} pixels\")\n",
    "print(f\"  üéØ Bounding box distribution is important for anchor/prior configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41607f",
   "metadata": {},
   "source": [
    "# For more dataset exploration, see notebooks/01_dataset_exploration.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a338ca",
   "metadata": {
    "id": "5nFWDveC7JGi"
   },
   "source": [
    "# Understand Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db209b",
   "metadata": {},
   "source": [
    "üìì **For detailed model exploration and testing, see:** [`notebooks/02_model_understanding.ipynb`](notebooks/02_model_understanding.ipynb)\n",
    "\n",
    "This notebook covers:\n",
    "- Tokenizer analysis and special tokens\n",
    "- Model loading and configuration verification\n",
    "- Complete inference pipeline demonstration\n",
    "- Output parsing and visualization functions\n",
    "- Pre-trained model testing on nutrition tables (single and multiple bounding boxes)\n",
    "\n",
    "The sections below provide a streamlined version for the complete training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02842f96",
   "metadata": {
    "id": "l1h_1QI37JGi"
   },
   "source": [
    "You should read the Qwen2-VL paper to familiarize yourself with the following:\n",
    "\n",
    "- **Model Architecture**\n",
    "- **Data Processing**\n",
    "- **Chat Template**\n",
    "\n",
    "Next, review the model card and write an inference script for the model using Hugging Face.\n",
    "\n",
    "Hugging Face provides an abstract API that simplifies usage by hiding many implementation details. While this is convenient, it may leave you with a superficial understanding of the model. To deepen your knowledge, explore the Qwen2-VL code and focus on these key aspects:\n",
    "\n",
    "- **Understand the input format required by the model:**\n",
    "  - Can you create an example input where the user provides two images and one video?\n",
    "\n",
    "- **Explore `apply_chat_template`:**\n",
    "  - Run this function on the example above and analyze the output. What does it do?\n",
    "\n",
    "- **Understand `process_vision_info`:**\n",
    "  - Review the code and determine what this function returns.\n",
    "\n",
    "- **Examine `processor()`:**\n",
    "  - Investigate its functionalities, such as:\n",
    "    - Patch-ification\n",
    "    - Replicating pad tokens\n",
    "    - Text tokenization\n",
    "\n",
    "- **[Optional] Analyze `model.generate()`'s [forward pass](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L999):**\n",
    "  - Understand its operations, including:\n",
    "    - Embedding image patches through `PatchEmbed`‚Äôs [forward pass](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L272).\n",
    "    - Sending patch embeddings to a transformer for feature extraction:\n",
    "      - Grasp the concept of 2D RoPE (Rotary Position Embedding).\n",
    "      - Pay attention to [forbidden attention](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L339) when more than one image is provided.\n",
    "    - Merging the resulting feature embeddings via [PatchMerger](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L298).\n",
    "    - Processing image and text embeddings using the LLM:\n",
    "      - Pay special attention to multimodal RoPE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1ce57",
   "metadata": {
    "id": "JNIW_O0z7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: write an inference function for qwen2-vl\n",
    "import torch\n",
    "import os\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from qwen_vl_utils import vision_process\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def run_qwen2vl_inference(image_path_or_pil, prompt, model_id=\"Qwen/Qwen2-VL-7B-Instruct\", device= \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Run inference with Qwen2-VL model.\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_pil: Either a file path to an image or a PIL Image object\n",
    "        prompt: Text prompt for the model\n",
    "        model_id: Model identifier from HuggingFace\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        Generated text response from the model\n",
    "    \"\"\"\n",
    "    # Load model and processor\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16, # original training precision stored in config.json for this model is bfloat16, so \"auto\" = torch.bfloat16 for this model\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=device\n",
    "    )\n",
    "    processor = Qwen2VLProcessor.from_pretrained(\n",
    "        model_id,\n",
    "        min_pixels=256*28*28,    # ~200k pixels (reduced from 3k default)\n",
    "        max_pixels=1280*28*28,   # ~1M pixels (reduced from 12.8M default!)\n",
    "        trust_remote_code=True\n",
    "    ) # Or use AutoProcessor.from_pretrained(model_id), but in this case Qwen2VLProcessor is more explicit for demonstration purpose\n",
    "    \n",
    "    # Set model to evaluation mode for consistent inference\n",
    "    model.eval()\n",
    "    \n",
    "    # Handle image input - can be path or PIL Image\n",
    "    if isinstance(image_path_or_pil, str):\n",
    "        from PIL import Image\n",
    "        image = Image.open(image_path_or_pil)\n",
    "    else:\n",
    "        image = image_path_or_pil\n",
    "    \n",
    "    # Create the conversation format expected by Qwen2-VL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template to format the conversation\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, #  if tokenize=True, the text will be tokenized and the tokens will be returned. if tokenize=False, the text will be returned as is.\n",
    "        add_generation_prompt=True # adds assitant's token to the end of the text to prompts the model to generate response \n",
    "    )\n",
    "    \n",
    "    # Process vision information (handles image resizing, patching, etc.)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    # Prepare inputs for the model\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,  # Deterministic for object detection\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens (excluding the input)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Example usage (will be tested in next cell)\n",
    "print(\"Inference function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c21f9b",
   "metadata": {
    "id": "jlaMACEb7JGj"
   },
   "source": [
    "Test your inference script using this [image](https://t4.ftcdn.net/jpg/01/57/82/05/360_F_157820583_agejYX5XeczPZuWRSCDF2YYeCGwJqUdG.jpg) with the prompt: ‚ÄúDetect the bounding box of the red car.‚Äù The model should correctly identify and locate the car in the image, confirming the script‚Äôs correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380c320",
   "metadata": {
    "id": "xUkdCbsB7JGj",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TASK: test the inference function\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the test image (red car)\n",
    "test_image_url = \"https://t4.ftcdn.net/jpg/01/57/82/05/360_F_157820583_agejYX5XeczPZuWRSCDF2YYeCGwJqUdG.jpg\"\n",
    "response = requests.get(test_image_url)\n",
    "test_image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the test image\n",
    "print(\"\\nüì∑ DISPLAYING ORIGINAL TEST IMAGE:\")\n",
    "_ = plt.figure(figsize=(8, 6))\n",
    "_ = plt.imshow(test_image)\n",
    "_ = plt.title(\"Test Image: Red Car\")\n",
    "_ = plt.axis('off')\n",
    "_ = plt.show()\n",
    "\n",
    "\n",
    "# Test the inference function\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing Qwen2-VL inference...\")\n",
    "print(\"Prompt: 'Detect the bounding box of the red car.'\")\n",
    "\n",
    "\n",
    "result = run_qwen2vl_inference(\n",
    "    test_image, \n",
    "    \"Detect the bounding box of the red car.\"\n",
    ")\n",
    "print(\"\\nModel Response:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# For now, let's show what the expected output format should look like\n",
    "\n",
    "\n",
    "\n",
    "print(\"Expected output format(with skip_special_tokens=False): <|object_ref_start|>the red car<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\")\n",
    "print(\"  Where coordinates are normalized to image dimensions (1000x1000 for Qwen2-VL)\")\n",
    "print(\"\\nExpected output format(with skip_special_tokens=True): the red car(x1,y1),(x2,y2)\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Output now looks like expected output format(with skip_special_tokens=True): the red car(x1,y1),(x2,y2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bcdb4",
   "metadata": {
    "id": "oosQDLcQ7JGj"
   },
   "source": [
    "# Try Qwen2VL without finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6ac64",
   "metadata": {},
   "source": [
    "üìì **For comprehensive pre-trained model testing with visualizations, see:** [`notebooks/02_model_understanding.ipynb`](notebooks/02_model_understanding.ipynb)\n",
    "\n",
    "That notebook includes:\n",
    "- Testing on single nutrition table examples\n",
    "- Testing on examples with 2 bounding boxes\n",
    "- Testing on examples with 3 bounding boxes\n",
    "- Side-by-side comparisons of ground truth vs predictions\n",
    "\n",
    "The sections below demonstrate the core functionality for this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eadc15f",
   "metadata": {
    "id": "Sy2oO_XE7JGj"
   },
   "source": [
    "It's a good idea to first assess the model's current capability in detecting the nutrition table without any fine-tuning. This allows for a clear comparison between the model‚Äôs performance before and after fine-tuning. To do this, you need to write a function that extracts the bounding box by parsing the model output and then visualize the bounding box on the input image.\n",
    "\n",
    "Notice that the model‚Äôs response will likely follow a different format than wat we saw above for the dog image. Why? One possible explanation is that the nutrition table does not belong to a previously object class seen during the model‚Äôs training phase. Additionally, the bounding box coordinates returned by the model are likely inaccurate. This should highlight the necessity of fine-tuning to improve the model‚Äôs performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28d360",
   "metadata": {
    "id": "C5JKCnIA7JGj"
   },
   "outputs": [],
   "source": [
    "# TASK: write a function to parse model output to extract bounding box coordinates\n",
    "import re\n",
    "\n",
    "def parse_qwen_bbox_output(model_output):\n",
    "    \"\"\"\n",
    "    Parse Qwen2-VL model output to extract bounding box coordinates.\n",
    "    \n",
    "    Args:\n",
    "        model_output: String output from the model\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'object' name and 'bbox' coordinates, or None if parsing fails\n",
    "\n",
    "    Rationale: \n",
    "        I want to draw TWO pieces of information:\n",
    "            1. WHAT was detected (object name) (Just in case if needed)\n",
    "            2. WHERE it is (coordinates)\n",
    "    \"\"\"\n",
    "    # Remove <|im_end|> token if present (appears when skip_special_tokens=False)\n",
    "    model_output = model_output.replace('<|im_end|>', '').strip()\n",
    "    \n",
    "    # Pattern 1: WITH special tokens (skip_special_tokens=False)\n",
    "    # Format: <|object_ref_start|>object name<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\n",
    "    pattern_with_tokens = r'<\\|object_ref_start\\|>(.+?)<\\|object_ref_end\\|><\\|box_start\\|>\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)<\\|box_end\\|>'\n",
    "    matches = re.findall(pattern_with_tokens, model_output)\n",
    "    \n",
    "    if matches:\n",
    "        # Can handle multiple detections\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            object_name = match[0]\n",
    "            x1, y1, x2, y2 = int(match[1]), int(match[2]), int(match[3]), int(match[4])\n",
    "            results.append({\n",
    "                'object': object_name,\n",
    "                'bbox': [x1, y1, x2, y2]  # Qwen outputs in [x,y,x,y] format\n",
    "            })\n",
    "        return results[0] if len(results) == 1 else results\n",
    "    \n",
    "    # Pattern 2: WITHOUT special tokens (skip_special_tokens=True)\n",
    "    # Format: \"object name(x1,y1),(x2,y2)\" - may have space before parenthesis\n",
    "    pattern_no_tokens = r'([^\\(]+?)\\s*\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)'\n",
    "    matches = re.findall(pattern_no_tokens, model_output)\n",
    "    \n",
    "    if matches:\n",
    "        # Can handle multiple detections\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            object_name = match[0].strip()\n",
    "            x1, y1, x2, y2 = int(match[1]), int(match[2]), int(match[3]), int(match[4])\n",
    "            results.append({\n",
    "                'object': object_name,\n",
    "                'bbox': [x1, y1, x2, y2]\n",
    "            })\n",
    "        return results[0] if len(results) == 1 else results\n",
    "    \n",
    "    return None  # No valid bbox found\n",
    "\n",
    "\n",
    "# TASK: write a function to visualize the bounding boxes on the input image\n",
    "def visualize_bbox_on_image(image, bbox_data, normalize_coords=True):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        bbox_data: Dict or list of dicts with 'object' and 'bbox' keys from parse_qwen_bbox_output\n",
    "        normalize_coords: If True, bbox coords are in Qwen's [0,1000] space (from model output)\n",
    "                         If False, bbox coords are already in pixel coordinates\n",
    "        \n",
    "    Returns:\n",
    "        PIL Image with bounding boxes drawn and labeled with object names\n",
    "    \"\"\"\n",
    "    from PIL import ImageDraw, ImageFont\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    img_with_bbox = image.copy()\n",
    "    draw = ImageDraw.Draw(img_with_bbox)\n",
    "    width, height = img_with_bbox.size\n",
    "    \n",
    "    # Handle None case\n",
    "    if bbox_data is None:\n",
    "        print(\"No bounding box data to visualize\")\n",
    "        return img_with_bbox\n",
    "    \n",
    "    # Handle single or multiple bboxes\n",
    "    if isinstance(bbox_data, dict):\n",
    "        bbox_data = [bbox_data]\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "    \n",
    "    for idx, data in enumerate(bbox_data):\n",
    "        if data is None:\n",
    "            continue\n",
    "            \n",
    "        color = colors[idx % len(colors)]\n",
    "        bbox = data['bbox']\n",
    "        object_name = data.get('object', 'unknown')\n",
    "        \n",
    "        # Convert coordinates if needed\n",
    "        if normalize_coords:\n",
    "            # Qwen uses 1000x1000 normalized space\n",
    "            x1 = int(bbox[0] * width / 1000)\n",
    "            y1 = int(bbox[1] * height / 1000)\n",
    "            x2 = int(bbox[2] * width / 1000)\n",
    "            y2 = int(bbox[3] * height / 1000)\n",
    "        else:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "        \n",
    "        # Ensure coordinates are integers\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        \n",
    "        # Draw rectangle with thicker line\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=4)\n",
    "        \n",
    "        # Add label with background\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 20)\n",
    "        except:\n",
    "            font = None\n",
    "        \n",
    "        label = f\"{object_name}\"\n",
    "        if font:\n",
    "            # Get text size for background\n",
    "            bbox_label = draw.textbbox((x1, y1-30), label, font=font)\n",
    "            # Draw background rectangle\n",
    "            draw.rectangle(bbox_label, fill=color)\n",
    "            # Draw text\n",
    "            draw.text((x1, y1-30), label, fill='white', font=font)\n",
    "        else:\n",
    "            # Fallback without custom font\n",
    "            draw.text((x1, y1-25), label, fill=color)\n",
    "        \n",
    "        print(f\"Drew bbox in pixels for '{object_name}': [{x1}, {y1}, {x2}, {y2}]\")\n",
    "    \n",
    "    return img_with_bbox\n",
    "\n",
    "# Test the parsing function with the two expected formats\n",
    "print(\"\\nüìù Testing parse_qwen_bbox_output function:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: WITH special tokens (skip_special_tokens=False)\n",
    "test_with_tokens = \"<|object_ref_start|>the red car<|object_ref_end|><|box_start|>(450,380),(650,520)<|box_end|><|im_end|>\"\n",
    "parsed1 = parse_qwen_bbox_output(test_with_tokens)\n",
    "print(f\"Test 1 - WITH special tokens (skip_special_tokens=False):\")\n",
    "print(f\"  Input: {test_with_tokens}\")\n",
    "print(f\"  Parsed: {parsed1}\")\n",
    "print(f\"  ‚úÖ Correctly extracts object='the red car' and bbox=[450,380,650,520]\")\n",
    "\n",
    "# Test 2: WITHOUT special tokens (skip_special_tokens=True)\n",
    "test_no_tokens = \"the red car(358,571),(492,943)\"\n",
    "parsed2 = parse_qwen_bbox_output(test_no_tokens)\n",
    "print(f\"\\nTest 2 - WITHOUT special tokens (skip_special_tokens=True):\")\n",
    "print(f\"  Input: {test_no_tokens}\")\n",
    "print(f\"  Parsed: {parsed2}\")\n",
    "print(f\"  ‚úÖ Correctly extracts object='the red car' and bbox=[358,571,492,943]\")\n",
    "\n",
    "# Test 3: Real example from pre-trained model (nutrition table)\n",
    "test_nutrition = \"The nutrition table(13,60),(984,989) is located in the image\"\n",
    "parsed3 = parse_qwen_bbox_output(test_nutrition)\n",
    "print(f\"\\nTest 3 - Real output from pre-trained model:\")\n",
    "print(f\"  Input: {test_nutrition}\")\n",
    "print(f\"  Parsed: {parsed3}\")\n",
    "if parsed3:\n",
    "    print(f\"  ‚úÖ Successfully parsed even with extra text after coordinates\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Failed to parse - check regex pattern\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to look at where the bounding box is if we overlay the bounding box on the image\n",
    "# First, let's visualize the red car detection that we already ran\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZING RED CAR DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# We already have the result from earlier: \"the red car(358,571),(492,943)\"\n",
    "red_car_output = \"the red car(358,571),(492,943)\"\n",
    "print(f\"Model output for red car: {red_car_output}\")\n",
    "\n",
    "# Parse the output\n",
    "parsed_red_car = parse_qwen_bbox_output(red_car_output)\n",
    "print(f\"Parsed bbox: {parsed_red_car}\")\n",
    "\n",
    "if parsed_red_car:\n",
    "    # Visualize the bbox on the red car image\n",
    "    img_with_bbox = visualize_bbox_on_image(test_image, parsed_red_car, normalize_coords=True)\n",
    "    \n",
    "    # Display the result\n",
    "    _ = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    _ = plt.subplot(1, 2, 1)\n",
    "    _ = plt.imshow(test_image)\n",
    "    _ = plt.title(\"Original Image\", fontsize=14)\n",
    "    _ = plt.axis('off')\n",
    "    \n",
    "    # Image with bounding box\n",
    "    _ = plt.subplot(1, 2, 2)\n",
    "    _ = plt.imshow(img_with_bbox)\n",
    "    _ = plt.title(\"With Detected Bounding Box\", fontsize=14)\n",
    "    _ = plt.axis('off')\n",
    "    \n",
    "    _ = plt.suptitle(\"Red Car Detection Result\", fontsize=16, fontweight='bold')\n",
    "    _ = plt.tight_layout()\n",
    "    _ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test the pre-trained model on a nutrition table image\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING PRE-TRAINED MODEL ON NUTRITION TABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the first nutrition table example\n",
    "example_idx = 0\n",
    "example = train_dataset[example_idx]\n",
    "nutrition_image = example['image']\n",
    "ground_truth_bbox = example['objects']['bbox'][0]\n",
    "ground_truth_category = example['objects']['category_name'][0]\n",
    "\n",
    "print(f\"\\nUsing training example #{example_idx}\")\n",
    "print(f\"Image size: {nutrition_image.size}\")\n",
    "print(f\"Ground truth: {ground_truth_category}\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\nRunning inference...\")\n",
    "nutrition_response = run_qwen2vl_inference(\n",
    "    nutrition_image,\n",
    "    \"Detect the bounding box of the nutrition table.\"\n",
    ")\n",
    "print(f\"Model response: {nutrition_response}\")\n",
    "\n",
    "# Parse the output\n",
    "parsed_nutrition = parse_qwen_bbox_output(nutrition_response)\n",
    "print(f\"Parsed bbox: {parsed_nutrition}\")\n",
    "\n",
    "# Visualize if parsing succeeded\n",
    "if parsed_nutrition:\n",
    "    # Create visualization\n",
    "    img_with_nutrition_bbox = visualize_bbox_on_image(nutrition_image, parsed_nutrition, normalize_coords=True)\n",
    "    \n",
    "    # Show comparison\n",
    "    _ = plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Original with ground truth\n",
    "    _ = plt.subplot(1, 2, 1)\n",
    "    _ = plt.imshow(nutrition_image)\n",
    "    \n",
    "    # Draw ground truth rectangle\n",
    "    # Use PIL image dimensions since we're drawing on the displayed image\n",
    "    pil_width, pil_height = nutrition_image.size\n",
    "    # CRITICAL: OpenFoodFacts uses [y_min, x_min, y_max, x_max] format\n",
    "    y_min, x_min, y_max, x_max = ground_truth_bbox\n",
    "    gt_x1 = int(x_min * pil_width)\n",
    "    gt_y1 = int(y_min * pil_height)\n",
    "    gt_x2 = int(x_max * pil_width)\n",
    "    gt_y2 = int(y_max * pil_height)\n",
    "    \n",
    "    from matplotlib.patches import Rectangle\n",
    "    ax = plt.gca()\n",
    "    rect = Rectangle((gt_x1, gt_y1), gt_x2-gt_x1, gt_y2-gt_y1,\n",
    "                     linewidth=3, edgecolor='green', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    _ = plt.title(\"Ground Truth (Green)\", fontsize=14)\n",
    "    _ = plt.axis('off')\n",
    "    \n",
    "    # Model prediction\n",
    "    _ = plt.subplot(1, 2, 2)\n",
    "    _ = plt.imshow(img_with_nutrition_bbox)\n",
    "    _ = plt.title(\"Model Prediction (Red)\", fontsize=14)\n",
    "    _ = plt.axis('off')\n",
    "    \n",
    "    _ = plt.suptitle(\"Nutrition Table Detection - Pre-trained Model\", fontsize=16, fontweight='bold')\n",
    "    _ = plt.tight_layout()\n",
    "    _ = plt.show()\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"\\nFailed to parse bbox - the model likely couldn't detect the nutrition table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65130e65",
   "metadata": {},
   "source": [
    "## Note: for more detailed inference analysis and model understanding, please look at notebooks/02_model_understanding.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9015d9",
   "metadata": {
    "id": "vw_RG5kw7JGj"
   },
   "source": [
    "# Data preprocessing\n",
    "\n",
    "\n",
    "(Note: `notebooks/03_data_preprocessing.ipynb` has detailed documentation of data preprocessing, the following only keeps necessary parts. For detailed understanding of data preprocessing and the data pipeline, please look at `notebooks/03_data_preprocessing.ipynb`. It's very important for understanding how we handle None fields issue by dataset.map() later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96fcc9",
   "metadata": {
    "id": "KGOrsuQo7JGj"
   },
   "source": [
    "The dataset requires conversion to be compatible with the Hugging Face (HF) library. Specifically, each sample must be reformatted into the OpenAI conversation format, comprising:\n",
    "\n",
    "- Roles: system, user, and assistant\n",
    "- User input: Provide an image and ask, \"Detect the bounding box of the nutrition table.\"\n",
    "- Assistant response: Format compatible with Qwen2-VL's detection question responses\n",
    "    * See pages 7 and 43 of this [paper](https://arxiv.org/pdf/2409.12191) and  [Model Card](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct#more-usage-tips) for tips\n",
    "    * Ensure inclusion of class name and bounding box coordinates using the proper special tokens.\n",
    "    * Check the expected range of bb coordinates\n",
    "    * Pay attention to the order of x,y coordinates as expected by Qwen\n",
    "\n",
    "Here is an example system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32196f04",
   "metadata": {
    "id": "IBvKAlXhI46X",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from product images.\n",
    "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5102bf",
   "metadata": {
    "id": "XG8NuzqjjbgI"
   },
   "outputs": [],
   "source": [
    "# Task: write a function to map each sample to a list of 3 dicts (one for each role)\n",
    "\n",
    "def convert_to_conversation_format(example):\n",
    "    \"\"\"\n",
    "    Convert a dataset example to Qwen2-VL conversation format.\n",
    "    \n",
    "    Why IMAGE_PLACEHOLDER?\n",
    "    - HuggingFace dataset.map() needs serializable data (PIL images aren't)\n",
    "    - The placeholder is replaced with the actual image during training (in collate_fn)\n",
    "    - Image is stored separately at example['image'] to avoid duplication\n",
    "    \n",
    "    Coordinate Conversion:\n",
    "    - Input: OpenFoodFacts [y_min, x_min, y_max, x_max] in [0,1]\n",
    "    - Output: Qwen2-VL (x1,y1),(x2,y2) in [0,1000)\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset sample with 'image' and 'objects' fields\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'messages' (conversation) and 'image' (PIL object)\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if 'objects' not in example or 'bbox' not in example['objects']:\n",
    "        raise ValueError(\"Missing objects or bbox in example\")\n",
    "    \n",
    "    # Extract nutrition table bounding boxes\n",
    "    bboxes = example['objects']['bbox']\n",
    "    categories = example['objects']['category_name']  # Fixed: 'category_name' not 'category'\n",
    "    \n",
    "    # Format the assistant response with Qwen2-VL special tokens\n",
    "    # Convert normalized [0,1] bbox to Qwen's [0,1000) format\n",
    "    assistant_responses = []\n",
    "    for bbox, category in zip(bboxes, categories):\n",
    "        # Validate bbox values are in [0,1] range (with small tolerance for rounding)\n",
    "        if not all(-0.001 <= coord <= 1.001 for coord in bbox):\n",
    "            print(f\"Warning: bbox coordinates out of [0,1] range: {bbox}\")\n",
    "        \n",
    "        # CRITICAL: OpenFoodFacts uses [y_min, x_min, y_max, x_max] format\n",
    "        # But Qwen2VL expects (x_top_left, y_top_left), (x_bottom_right, y_bottom_right)\n",
    "        y_min, x_min, y_max, x_max = bbox  # Unpack OpenFoodFacts format\n",
    "        \n",
    "        # Convert to Qwen format: (x,y) coordinates in [0,1000) range\n",
    "        # Note: multiply by 1000 to convert from [0,1] to [0,1000)\n",
    "        x1 = int(x_min * 1000)  # x_top_left\n",
    "        y1 = int(y_min * 1000)  # y_top_left\n",
    "        x2 = int(x_max * 1000)  # x_bottom_right\n",
    "        y2 = int(y_max * 1000)  # y_bottom_right\n",
    "        \n",
    "        # Format: <|object_ref_start|>object<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\n",
    "        response = f\"<|object_ref_start|>{category}<|object_ref_end|><|box_start|>({x1},{y1}),({x2},{y2})<|box_end|>\"\n",
    "        assistant_responses.append(response)\n",
    "    \n",
    "    # Combine multiple detections if present\n",
    "    assistant_text = \" \".join(assistant_responses)\n",
    "    \n",
    "    # Create conversation format WITHOUT the PIL image embedded\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_message\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": \"IMAGE_PLACEHOLDER\"  # Use placeholder instead of actual image\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Detect the bounding box of the nutrition table.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": assistant_text\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Return messages and image separately\n",
    "    return {\n",
    "        \"messages\": conversation,\n",
    "        \"image\": example['image']  # Store PIL image at top level\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0ad1d",
   "metadata": {
    "id": "1edUqNGWTtjA"
   },
   "source": [
    "Now, let's format the data using the chatbot structure. This will allow us to set up the interactions appropriately for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907ff0a",
   "metadata": {
    "id": "oSHNqk0dkxii"
   },
   "outputs": [],
   "source": [
    "def _has_image(example):\n",
    "    \"\"\"\n",
    "    Filter out samples with missing or invalid images.\n",
    "    \n",
    "    This function is crucial for preventing None values from leaking into\n",
    "    process_vision_info during collation/training, which would cause crashes.\n",
    "    \n",
    "    Why this is necessary:\n",
    "    - Some dataset samples may have corrupted or missing images\n",
    "    - PIL Image loading can fail silently, leaving None values\n",
    "    - The collate_fn and process_vision_info expect valid PIL images\n",
    "    - Filtering ensures training stability and prevents runtime errors\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset sample that should contain an 'image' field\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if example has a valid PIL image with 'size' attribute\n",
    "    \"\"\"\n",
    "    img = example.get('image')\n",
    "    try:\n",
    "        # Treat as valid only if it looks like a PIL image\n",
    "        return (img is not None) and hasattr(img, 'size')\n",
    "    except Exception:\n",
    "        return img is not None\n",
    "\n",
    "# Apply filtering before formatting\n",
    "train_dataset = train_dataset.filter(_has_image)\n",
    "eval_dataset = eval_dataset.filter(_has_image)\n",
    "\n",
    "\n",
    "print(f\"train_dataset[0]: {train_dataset[0]}\")\n",
    "\n",
    "# Task: apply the function above to all samples in the training and eval datasets\n",
    "# Use remove_columns to get clean output with only 'messages' and 'image' fields\n",
    "columns_to_remove = ['image_id', 'width', 'height', 'meta', 'objects']\n",
    "\n",
    "# Unfortunately, HuggingFace datasets adds None fields during serialization of nested dicts\n",
    "# This is a known behavior. We have two options:\n",
    "# Option 1: Accept the None fields (they don't affect training, collate_fn handles them)\n",
    "# Option 2: Post-process to remove them (adds overhead but cleaner)\n",
    "\n",
    "# For now, using Option 1 - the collate_fn already handles None values correctly\n",
    "train_dataset_formatted = train_dataset.map(\n",
    "    convert_to_conversation_format,\n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "eval_dataset_formatted = eval_dataset.map(\n",
    "    convert_to_conversation_format, \n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "\n",
    "print(f\"Formatted training samples: {len(train_dataset_formatted)}\")\n",
    "print(f\"Formatted evaluation samples: {len(eval_dataset_formatted)}\")\n",
    "\n",
    "# NOTE: HuggingFace automatically adds 'image': None and 'text': None to content items\n",
    "# This is expected behavior and the collate_fn handles it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first two elements of train_dataset_formatted (what actually goes to DataLoader)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSPECTING train_dataset_formatted - ACTUAL DATASET PASSED TO TRAINER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simple inspection first\n",
    "for i in range(2):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(train_dataset_formatted[i])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6d6fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Display the same train_dataset_formatted samples with better formatting\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"train_dataset_formatted[0] - Better formatted\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "formatted_sample_0 = train_dataset_formatted[0]\n",
    "pprint(formatted_sample_0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"train_dataset_formatted[1] - Better formatted\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "formatted_sample_1 = train_dataset_formatted[1]\n",
    "pprint(formatted_sample_1)\n",
    "\n",
    "\n",
    "# Before we proceed with training the model in the next section, let's clear the current variables and clean the GPU to free up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b59b3e",
   "metadata": {},
   "source": [
    "## Step by Step data pipeline process (Deleted here to keep clean)\n",
    "\n",
    "## (If you want to know more about the data processing pipeline within the data collators, please look at `notebooks/03_data_preprocessing.ipynb`)\n",
    "\n",
    "That notebook documents:\n",
    "- Step-by-step visualization of data transformations\n",
    "- Sample outputs at each stage (before/after `dataset.map()`)\n",
    "- The HuggingFace None fields issue explained with examples\n",
    "\n",
    "**Key Discovery:** Our `convert_to_conversation_format()` function returns **clean** dicts like:\n",
    "```python\n",
    "{\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\"}\n",
    "{\"type\": \"text\", \"text\": \"Detect the bounding box...\"}\n",
    "```\n",
    "\n",
    "But after `dataset.map()`, HuggingFace adds None fields for Apache Arrow schema consistency:\n",
    "```python\n",
    "{\"type\": \"image\", \"image\": \"IMAGE_PLACEHOLDER\", \"text\": None}  # <-- \"text\": None added!\n",
    "{\"type\": \"text\", \"text\": \"Detect the bounding box...\", \"image\": None}  # <-- \"image\": None added!\n",
    "```\n",
    "\n",
    "This is **cosmetic only** - the collate_fn filters out None values during training.\n",
    "See the notebook for full debugging walkthrough and visual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60628de3",
   "metadata": {
    "id": "dxkXZuUkvy8j"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if 'inputs' in globals(): del globals()['inputs']\n",
    "    if 'model' in globals(): del globals()['model']\n",
    "    if 'processor' in globals(): del globals()['processor']\n",
    "    if 'trainer' in globals(): del globals()['trainer']\n",
    "    if 'peft_model' in globals(): del globals()['peft_model']\n",
    "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb419c1",
   "metadata": {
    "id": "Zw9nm6ZP7JGk"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ab92f",
   "metadata": {
    "id": "yIrR9gP2z90z"
   },
   "source": [
    "## Load the Model for Training with NF4 weights  ‚öôÔ∏è\n",
    "\n",
    "Next, you need to load the quantized model using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index). If you want to learn more about quantization, check out [this blog post](https://huggingface.co/blog/merve/quantization) or [this one](https://www.maartengrootendorst.com/blog/quantization/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec6a60",
   "metadata": {
    "id": "zm_bJRrXsESg"
   },
   "outputs": [],
   "source": [
    "# TASK: load the NF4 model and processor\n",
    "from transformers import BitsAndBytesConfig, Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "# Use \"balanced\" to evenly distribute model across available GPUs\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",  # Changed from \"auto\" to evenly distribute across GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",  # Use Flash Attention 2 for efficiency\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load processor with optimized pixel configuration for faster training\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    min_pixels=256*28*28,    # ~200k pixels (reduced from 3k default)\n",
    "    max_pixels=1280*28*28,   # ~1M pixels (reduced from 12.8M default!)\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model_id}\")\n",
    "print(f\"Model device map: {model.hf_device_map}\")\n",
    "print(f\"Model memory footprint: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: For TRL version 0.12.0, we need to manually prepare the model before LoRA\n",
    "# Newer versions (0.21+) do this automatically, but 0.12.0 does not\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# Define the trainable parameter summarizer function first (before using it)\n",
    "def _summarize_trainables(model_to_summarize):\n",
    "    try:\n",
    "        total_params, trainable_params = 0, 0\n",
    "        trainable_examples = []\n",
    "        for param_name, param_tensor in model_to_summarize.named_parameters():\n",
    "            param_count = param_tensor.numel()\n",
    "            total_params += param_count\n",
    "            if param_tensor.requires_grad:\n",
    "                trainable_params += param_count\n",
    "                if len(trainable_examples) < 8:\n",
    "                    device_str = str(param_tensor.device)\n",
    "                    trainable_examples.append(f\"- {param_name} | shape={tuple(param_tensor.shape)} | device={device_str}\")\n",
    "        trainable_percentage = 100.0 * trainable_params / max(total_params, 1)\n",
    "        print(\"\\nTrainable parameter summary:\")\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable params: {trainable_params:,} ({trainable_percentage:.2f}%)\")\n",
    "        if hasattr(model_to_summarize, 'hf_device_map'):\n",
    "            print(\"  Device map snapshot (first 10):\")\n",
    "            device_count = 0\n",
    "            for module_name, device_location in model_to_summarize.hf_device_map.items():\n",
    "                print(f\"    {module_name} -> {device_location}\")\n",
    "                device_count += 1\n",
    "                if device_count >= 10:\n",
    "                    break\n",
    "        if trainable_examples:\n",
    "            print(\"  Sample trainable tensors:\")\n",
    "            for example in trainable_examples:\n",
    "                print(\"    \", example)\n",
    "    except Exception as error:\n",
    "        print(f\"[warn] could not summarize trainables: {error}\")\n",
    "\n",
    "# Trainables snapshot BEFORE k-bit preparation\n",
    "print(\"=== Before prepare_model_for_kbit_training ===\")\n",
    "_summarize_trainables(model)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"\\n‚úÖ Model prepared for k-bit training\")\n",
    "\n",
    "# Trainables snapshot AFTER k-bit preparation\n",
    "print(\"\\n=== After prepare_model_for_kbit_training ===\")\n",
    "_summarize_trainables(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0b4d0",
   "metadata": {
    "id": "65wfO29isQlX"
   },
   "source": [
    "## Set Up QLoRA and SFTConfig üöÄ\n",
    "\n",
    "Next, you need to configure [QLoRA](https://github.com/artidoro/qlora) for your training setup. QLoRA enables efficient fine-tuning of large language models while significantly reducing the memory footprint compared to traditional methods. Unlike standard LoRA, which reduces memory usage by applying a low-rank approximation, QLoRA takes it a step further by quantizing the model weights. This leads to even lower memory requirements and improved training efficiency, making it an excellent choice for optimizing our model's performance without sacrificing quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ea571",
   "metadata": {
    "id": "S5WRzUJe-P_-"
   },
   "source": [
    "üí° NOTE:\n",
    "\n",
    "Preparing a model for QLoRA training typically involves three key steps:\n",
    "\n",
    "- Load the base model in 4-bit (using BitsAndBytesConfig).\n",
    "\n",
    "- Run prepare_model_for_kbit_training(). üö® Understand what this function does.\n",
    "\n",
    "- Apply LoRA adapters to the target modules.\n",
    "\n",
    "You can perform these steps manually, or let SFTTrainer handle steps 2 & 3 for you:\n",
    "\n",
    "- Simply load the model in 4-bit,\n",
    "\n",
    "- Pass a peft_config to SFTTrainer, which will automatically run prepare_model_for_kbit_training() (for unsharded QLoRA) and attach LoRA adapters. See lines 610 and 625 in [here](https://github.com/huggingface/trl/blob/v0.21.0/trl/trainer/sft_trainer.py)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e356c51",
   "metadata": {
    "id": "ITmkRHWCKYjf"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# Task: create LoRA config and apply LoRA to the model instrance created above\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # Rank\n",
    "    lora_alpha=128,  # Alpha scaling\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",  # MLP layers\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "\n",
    "# For TRL 0.12.0, we need to manually apply get_peft_model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n‚úÖ LoRA adapters attached to model\")\n",
    "\n",
    "# Note: model.print_trainable_parameters() counts differently than our custom function\n",
    "# PEFT's method includes the full base model size in total, our function only counts accessible parameters\n",
    "print(\"\\n=== PEFT's parameter count (includes full base model) ===\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n=== After get_peft_model (LoRA attached) - Custom count ===\")\n",
    "_summarize_trainables(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2bfc4",
   "metadata": {
    "id": "A1t7Hk_f7JGn"
   },
   "source": [
    "Next, you need to create an SFT config for model finetuning. This step is critical for model convergence.\n",
    "You should set the following hyper-parameteres among others:\n",
    " - learning_rate\n",
    " - per_device_train_batch_size\n",
    " - gradient_accumulation_steps for better gradient direction estimation\n",
    " - BF16 and TF32 enablement for memory saving and faster compute\n",
    " - gradient_checkpointing for memory saving\n",
    "   \n",
    "There are other input arguments that you should also set for proper evaluation during model finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bab02",
   "metadata": {
    "id": "SbqX1pQUKaSM"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "# TASK: create an SFT config\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # Output and logging - SAVE TO SSD TO AVOID HOME DIRECTORY QUOTA\n",
    "    output_dir=\"/ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-lora\",\n",
    "    logging_dir=\"/ssd1/zhuoyuan/vlm_outputs/logs\",\n",
    "    logging_steps=10,  # Show training loss every 10 steps for frequent updates\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Back to 8 for larger effective batch size (2 * 1 * 8 = 16)\n",
    "    gradient_checkpointing=False,  # Disabled - causes issues with QLoRA\n",
    "    # gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Not needed when disabled\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=2e-5,  # Slightly higher for better convergence with vision models\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta2=0.999,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Mixed precision and performance\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Enable TF32 on Ampere GPUs\n",
    "    dataloader_num_workers=0,\n",
    "    # dataloader_num_workers=2,  # Use 2 workers for better data loading\n",
    "    # dataloader_pin_memory=True,  # Pin memory for faster GPU transfer\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=300,  # Changed from 100 to 300 - fewer checkpoints\n",
    "    save_total_limit=3,  # Only keep last 3 checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Other settings\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",  # Use W&B for logging metrics (changed from tensorboard)\n",
    "    run_name=\"qwen2vl-nutrition-detection\",\n",
    "    seed=42,  # For reproducibility\n",
    "    \n",
    "    # Specific for vision models\n",
    "    dataset_text_field=\"\",  # Empty string because we use custom data_collator, not \"messages\"\n",
    "    # Note: dataset_text_field would be \"messages\" if we wanted SFTTrainer to handle text extraction,\n",
    "    # but we handle everything in our custom collate_fn, so we leave it empty\n",
    "    max_length=2048,\n",
    "    dataset_kwargs={\n",
    "        \"skip_prepare_dataset\": True  # We handle data preparation ourselves\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Random seed: {training_args.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656eab23",
   "metadata": {
    "id": "wjQGt-iZVyef"
   },
   "source": [
    "# wandb setup\n",
    "If you have wandb account, you can set it up here.\n",
    "Let‚Äôs connect our notebook to W&B to capture essential information during training.\n",
    "Make sure to have set the logging arguments in the SFT config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40fb20",
   "metadata": {
    "id": "ckVfXDWsoF4Y"
   },
   "outputs": [],
   "source": [
    "# Initialize W&B for experiment tracking\n",
    "import wandb\n",
    "\n",
    "wandb.login()  # reads the key from the env\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "wandb.init(\n",
    "    project=\"qwen2vl-nutrition-detection\",\n",
    "    name=\"qwen2vl-7b-nutrition-lora\",\n",
    "    config={\n",
    "        \"model\": model_id,\n",
    "        \"lora_r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation\": training_args.gradient_accumulation_steps,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"dataset\": \"openfoodfacts/nutrition-table-detection\",\n",
    "    },\n",
    "    tags=[\"qwen2-vl\", \"object-detection\", \"nutrition-table\", \"lora\"],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ W&B initialized for experiment tracking\")\n",
    "print(f\"View your run at: {wandb.run.get_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d3b1c",
   "metadata": {
    "id": "pOUrD9P-y-Kf"
   },
   "source": [
    "## Training the Model üèÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81975f",
   "metadata": {
    "id": "ucTUbGURV2_-"
   },
   "source": [
    "You should now create a trainer object by instantiating the SFTTrainer class of HF's TRL. For this, you need to provide the training dataset, model, tokenizer, and more important a collate function.\n",
    "\n",
    "You need a collator function to properly retrieve and batch the data during the training procedure. This function will receive as the input a batch of samples:\n",
    "\n",
    "[{'role': 'system',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format. \\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
    " {'role': 'user',\n",
    "  'content': [{'type': 'image',\n",
    "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>},\n",
    "   {'type': 'text',\n",
    "    'text': 'Detect the bounding box of the nutrition table'}]},\n",
    " {'role': 'assistant',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': '<|object_ref_start|>the nutrition table<|object_ref_end|><|box_start|>(14,57),(991,604)<|box_end|>'}]}]\n",
    "    \n",
    "[{'role': 'system',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format. \\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
    " {'role': 'user',\n",
    "  'content': [{'type': 'image',\n",
    "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408>},\n",
    "   {'type': 'text',\n",
    "    'text': 'Detect the bounding box of the nutrition table'}]},\n",
    " {'role': 'assistant',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': '<|object_ref_start|>the nutrition table<|object_ref_end|><|box_start|>(147,152),(516,588)<|box_end|>'}]}]\n",
    "\n",
    "and then performs ops similar to what we did earlier in the inference script:\n",
    "  - applying chat template on each sample in the batch -> get formatted prompt\n",
    "  - applying process_vision_info on each sample in the batch -> get image pixels\n",
    "  - applying processor on formatted prompt and image pixels -> new batch\n",
    "  - modify the labels of the new batch by replacing labels correspondings to the following items to -100 (why?)\n",
    "    * text pad tokens\n",
    "    * <|vision_start|> <|vision_end|> <|image_pad|>\n",
    "\n",
    "üëâ Check out the TRL official example [scripts]( https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm.py#L87) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfad9f",
   "metadata": {},
   "source": [
    "## Data Collator Function\n",
    "\n",
    "The collator function is critical for properly batching text-image pairs during training.\n",
    "It handles the conversion from our formatted dataset to the format expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4728202",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "# Fixed collate_fn with improved error handling and robust masking  \n",
    "class collate_fn_fixed_1:\n",
    "    \"\"\"\n",
    "    FIXED VERSION: Robust collate function that prevents out-of-range label issues.\n",
    "    \n",
    "    This collator:\n",
    "    1. Restores IMAGE_PLACEHOLDER with actual PIL images\n",
    "    2. Uses process_vision_info to properly extract and format images\n",
    "    3. Applies chat template to get formatted text\n",
    "    4. Tokenizes text and processes images together\n",
    "    5. Creates labels with proper masking for loss computation\n",
    "    \n",
    "    Key improvements:\n",
    "    - Properly masks all vision/special tokens to prevent training on them\n",
    "    - Validates that no out-of-vocabulary tokens exist in labels\n",
    "    - Uses token-based span finding for robust assistant response extraction\n",
    "    - Handles edge cases gracefully (samples without images, OOV tokens)\n",
    "    \n",
    "    Args:\n",
    "        batch: List of samples from the dataset, each containing 'messages' and 'image'\n",
    "        \n",
    "    Returns:\n",
    "        Dict with input_ids, attention_mask, pixel_values, image_grid_thw, and labels\n",
    "    \"\"\"\n",
    "\n",
    "    # set mask for the following tokens:     \n",
    "    # * text pad tokens\n",
    "    # * <|vision_start|> <|vision_end|> <|image_pad|>\n",
    "    def __init__(self, processor, model):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.masked_token_ids = [\n",
    "            self.processor.tokenizer.pad_token_id,  # This one exists as attribute\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'),\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'),\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|image_pad|>')\n",
    "        ]\n",
    "\n",
    "    def __call__(self, batch):  \n",
    "\n",
    "        # STEP 1: Format the dataset into desired format:\n",
    "        # Restores IMAGE_PLACEHOLDER with actual PIL image\n",
    "        # so each element in the batch looks like sample of convert_to_conversation_format(train_dataset[0])\n",
    "\n",
    "        # Extract messages and images from each sample\n",
    "        messages_list = [sample['messages'] for sample in batch]\n",
    "        images_list = [sample.get('image', None) for sample in batch]\n",
    "\n",
    "        # Filter out samples without images\n",
    "        valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "        if not valid_pairs:\n",
    "            raise ValueError(\"Batch contains no valid images.\")\n",
    "\n",
    "        messages_list, images_list = zip(*valid_pairs)\n",
    "        messages_list = list(messages_list)\n",
    "        images_list = list(images_list)\n",
    "\n",
    "\n",
    "        all_conversations = []  # This will hold complete conversations, each conversation is a list of 3 messages (system, assistant, user)\n",
    "\n",
    "        for messages, image in zip(messages_list, images_list):\n",
    "            messages_with_image = []\n",
    "            # Clean up messages: remove None values and restore images\n",
    "            for msg in messages:\n",
    "                msg_copy = {'role': msg['role'], 'content': []}\n",
    "                \n",
    "                for content_item in msg['content']:\n",
    "                    # Skip None entries entirely\n",
    "                    if content_item is None:\n",
    "                        continue\n",
    "                        \n",
    "                    # Process text content - filter out None text values\n",
    "                    if content_item.get('type') == 'text':\n",
    "                        text_value = content_item.get('text')\n",
    "                        if text_value is not None and text_value != 'None':  # Check for actual None and string 'None'\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'text',\n",
    "                                'text': text_value\n",
    "                            })\n",
    "                    # Process image content - replace IMAGE_PLACEHOLDER with actual PIL image\n",
    "                    elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                        # Check if it's IMAGE_PLACEHOLDER and replace with actual image\n",
    "                        image_value = content_item.get('image')\n",
    "                        if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                            # Replace with the actual PIL image from top level\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'image',\n",
    "                                'image': image  # Use the actual PIL image\n",
    "                            })\n",
    "                        elif image_value and image_value != 'None':\n",
    "                            # Use existing image if it's not placeholder\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'image',\n",
    "                                'image': image_value\n",
    "                            })\n",
    "                \n",
    "                if msg_copy['content']:\n",
    "                    messages_with_image.append(msg_copy)\n",
    "\n",
    "            # Add the complete conversation (3 messages) to collection\n",
    "            all_conversations.append(messages_with_image)\n",
    "        \n",
    "        # Apply chat template to get text\n",
    "        text = self.processor.apply_chat_template(\n",
    "            all_conversations,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        # process_vision_info to get image and video\n",
    "        image, video = process_vision_info(all_conversations)\n",
    "\n",
    "        # Process texts and images together\n",
    "        batch_inputs = self.processor( # batch_inputs is a dictionary containing input_ids, attention_mask, pixel_values, and image_grid_thw\n",
    "            text=text,\n",
    "            images=image,\n",
    "            # videos=video,\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    " \n",
    "\n",
    "\n",
    "        # if 'input_ids' in batch_inputs: # only this fixes torch_dtype=torch.int32 error\n",
    "        #     batch_inputs['input_ids'] = batch_inputs['input_ids'].to(torch.long)\n",
    "        # if 'image_grid_thw' in batch_inputs:\n",
    "        #     batch_inputs['image_grid_thw'] = batch_inputs['image_grid_thw'].to(torch.int32)\n",
    "        # if 'pixel_values' in batch_inputs:\n",
    "        #     batch_inputs['pixel_values'] = batch_inputs['pixel_values'].to(torch.int32)\n",
    "\n",
    "\n",
    "        # Create labels from input_ids\n",
    "        labels = batch_inputs[\"input_ids\"].clone()\n",
    "        for token_id in self.masked_token_ids:\n",
    "            labels[labels == token_id] = -100\n",
    "        batch_inputs[\"labels\"] = labels\n",
    "\n",
    "        # # FIX: Convert all int64 tensors to int32 for Flash Attention\n",
    "        # for key, value in batch_inputs.items():\n",
    "        #     if torch.is_tensor(value) and value.dtype == torch.int64:\n",
    "        #         batch_inputs[key] = value.to(torch.int32)\n",
    "\n",
    "        # print(batch_inputs) #  for debugging, uncomment\n",
    "\n",
    "        return batch_inputs\n",
    "\n",
    "print(\"‚úÖ Fixed collate_fn_1 created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8352cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed collate_fn with Flash Attention dtype fix for cu_seqlens\n",
    "class collate_fn_fixed_fixed1:\n",
    "    \"\"\"\n",
    "    FIXED VERSION 2: Fixes Flash Attention cu_seqlens_q dtype error.\n",
    "    \n",
    "    The error \"RuntimeError: cu_seqlens_q must have dtype int32\" occurs because\n",
    "    Flash Attention expects cumulative sequence length tensors to be int32,\n",
    "    but they're created as int64 internally.\n",
    "    \n",
    "    This collator patches the Flash Attention forward pass to ensure cu_seqlens\n",
    "    tensors have the correct dtype.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processor, model):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.masked_token_ids = [\n",
    "            self.processor.tokenizer.pad_token_id,\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'),\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'),\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|image_pad|>')\n",
    "        ]\n",
    "        self.call_count = 0  # Track calls to limit debug output\n",
    "        \n",
    "        # Apply monkey patch to fix cu_seqlens dtype issue\n",
    "        self._patch_flash_attention()\n",
    "    \n",
    "    def _patch_flash_attention(self):\n",
    "        \"\"\"Monkey-patch Flash Attention to fix cu_seqlens dtype.\"\"\"\n",
    "        import flash_attn.flash_attn_interface as flash_interface\n",
    "        \n",
    "        # Store original _flash_attn_varlen_forward function\n",
    "        original_flash_varlen_forward = flash_interface._flash_attn_varlen_forward\n",
    "        \n",
    "        def patched_flash_varlen_forward(q, k, v, cu_seqlens_q, cu_seqlens_k, \n",
    "                                        max_seqlen_q, max_seqlen_k, *args, **kwargs):\n",
    "            # Convert cu_seqlens to int32 if needed\n",
    "            if cu_seqlens_q is not None and cu_seqlens_q.dtype != torch.int32:\n",
    "                cu_seqlens_q = cu_seqlens_q.to(torch.int32)\n",
    "            if cu_seqlens_k is not None and cu_seqlens_k.dtype != torch.int32:\n",
    "                cu_seqlens_k = cu_seqlens_k.to(torch.int32)\n",
    "            \n",
    "            # Call original function with fixed dtypes\n",
    "            return original_flash_varlen_forward(q, k, v, cu_seqlens_q, cu_seqlens_k,\n",
    "                                                max_seqlen_q, max_seqlen_k, *args, **kwargs)\n",
    "        \n",
    "        # Replace the internal _flash_attn_varlen_forward function\n",
    "        flash_interface._flash_attn_varlen_forward = patched_flash_varlen_forward\n",
    "        \n",
    "        # Also patch the FlashAttnVarlenFunc.forward method\n",
    "        from flash_attn.flash_attn_interface import FlashAttnVarlenFunc\n",
    "        original_forward = FlashAttnVarlenFunc.forward\n",
    "        \n",
    "        @staticmethod\n",
    "        def patched_forward(ctx, q, k, v, cu_seqlens_q, cu_seqlens_k, \n",
    "                          max_seqlen_q, max_seqlen_k, *args):\n",
    "            # Convert cu_seqlens to int32 if needed\n",
    "            if cu_seqlens_q is not None and cu_seqlens_q.dtype != torch.int32:\n",
    "                cu_seqlens_q = cu_seqlens_q.to(torch.int32)\n",
    "            if cu_seqlens_k is not None and cu_seqlens_k.dtype != torch.int32:\n",
    "                cu_seqlens_k = cu_seqlens_k.to(torch.int32)\n",
    "            \n",
    "            # Call original forward with fixed dtypes\n",
    "            return original_forward(ctx, q, k, v, cu_seqlens_q, cu_seqlens_k,\n",
    "                                   max_seqlen_q, max_seqlen_k, *args)\n",
    "        \n",
    "        FlashAttnVarlenFunc.forward = patched_forward\n",
    "        print(\"‚úÖ Applied Flash Attention cu_seqlens dtype patch (both _flash_attn_varlen_forward and FlashAttnVarlenFunc.forward)\")\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        # Extract messages and images from each sample\n",
    "        messages_list = [sample['messages'] for sample in batch]\n",
    "        images_list = [sample.get('image', None) for sample in batch]\n",
    "        \n",
    "        # Filter out samples without images\n",
    "        valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "        if not valid_pairs:\n",
    "            raise ValueError(\"Batch contains no valid images.\")\n",
    "        \n",
    "        messages_list, images_list = zip(*valid_pairs)\n",
    "        messages_list = list(messages_list)\n",
    "        images_list = list(images_list)\n",
    "        \n",
    "        all_conversations = []\n",
    "        \n",
    "        for messages, image in zip(messages_list, images_list):\n",
    "            messages_with_image = []\n",
    "            for msg in messages:\n",
    "                msg_copy = {'role': msg['role'], 'content': []}\n",
    "                \n",
    "                for content_item in msg['content']:\n",
    "                    if content_item is None:\n",
    "                        continue\n",
    "                    \n",
    "                    if content_item.get('type') == 'text':\n",
    "                        text_value = content_item.get('text')\n",
    "                        if text_value is not None and text_value != 'None':\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'text',\n",
    "                                'text': text_value\n",
    "                            })\n",
    "                    elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                        image_value = content_item.get('image')\n",
    "                        if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'image',\n",
    "                                'image': image\n",
    "                            })\n",
    "                        elif image_value and image_value != 'None':\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'image',\n",
    "                                'image': image_value\n",
    "                            })\n",
    "                \n",
    "                if msg_copy['content']:\n",
    "                    messages_with_image.append(msg_copy)\n",
    "            \n",
    "            all_conversations.append(messages_with_image)\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = self.processor.apply_chat_template(\n",
    "            all_conversations,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # Process vision info\n",
    "        image, video = process_vision_info(all_conversations)\n",
    "        \n",
    "        # Process texts and images together\n",
    "        batch_inputs = self.processor(\n",
    "            text=text,\n",
    "            images=image,\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Ensure correct dtypes for all tensors\n",
    "        if 'input_ids' in batch_inputs:\n",
    "            batch_inputs['input_ids'] = batch_inputs['input_ids'].to(torch.long)\n",
    "        \n",
    "        if 'attention_mask' in batch_inputs:\n",
    "            batch_inputs['attention_mask'] = batch_inputs['attention_mask'].to(torch.long)\n",
    "        \n",
    "        # Create labels with masking\n",
    "        labels = batch_inputs[\"input_ids\"].clone()\n",
    "        for token_id in self.masked_token_ids:\n",
    "            labels[labels == token_id] = -100\n",
    "        batch_inputs[\"labels\"] = labels\n",
    "        \n",
    "        # Debug output disabled for clean training logs\n",
    "        # Uncomment the lines below if you need to debug token masking:\n",
    "        # self.call_count += 1\n",
    "        # if self.call_count <= 3:  # Only show first 3 samples\n",
    "        #     non_masked = (labels != -100).sum().item()\n",
    "        #     total = labels.numel()\n",
    "        #     if non_masked > 0 and total > 0:\n",
    "        #         print(f\"[Sample {self.call_count}] Training on {non_masked}/{total} tokens ({100*non_masked/total:.1f}%)\")\n",
    "        \n",
    "        return batch_inputs\n",
    "\n",
    "print(\"‚úÖ Fixed collate_fn_fixed_fixed1 created with Flash Attention dtype patch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2e1c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Use the already loaded model and processor from above\n",
    "collate_fn = collate_fn_fixed_1(processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f9eef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Function to analyze token distribution\n",
    "def analyze_token_distribution(collate_fn, dataset, num_samples=3):\n",
    "    \"\"\"\n",
    "    Analyze what tokens are being trained on vs masked.\n",
    "    This helps understand why loss might not be decreasing.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOKEN DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Analyzing {num_samples} samples to understand training tokens...\\n\")\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        test_batch = [dataset[i]]\n",
    "        batch_output = collate_fn(test_batch)\n",
    "        \n",
    "        input_ids = batch_output['input_ids'][0]\n",
    "        labels = batch_output['labels'][0]\n",
    "        \n",
    "        # Count token types\n",
    "        total_tokens = len(input_ids)\n",
    "        masked_tokens = (labels == -100).sum().item()\n",
    "        trained_tokens = total_tokens - masked_tokens\n",
    "        \n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Total tokens: {total_tokens}\")\n",
    "        print(f\"  Masked (ignored): {masked_tokens} tokens\")\n",
    "        print(f\"  Trained on: {trained_tokens} tokens ({100*trained_tokens/total_tokens:.1f}%)\")\n",
    "        \n",
    "        # Decode what we're training on\n",
    "        trained_indices = (labels != -100).nonzero(as_tuple=True)[0]\n",
    "        if len(trained_indices) > 0:\n",
    "            # Show first and last parts of what we're training on\n",
    "            trained_token_ids = input_ids[trained_indices]\n",
    "            decoded = processor.tokenizer.decode(trained_token_ids, skip_special_tokens=False)\n",
    "            \n",
    "            # # Clean up for display\n",
    "            # decoded_clean = decoded.replace('<|im_start|>', '[START]')\n",
    "            # decoded_clean = decoded_clean.replace('<|im_end|>', '[END]')\n",
    "            # decoded_clean = decoded_clean.replace('<|object_ref_start|>', '[OBJ]')\n",
    "            # decoded_clean = decoded_clean.replace('<|object_ref_end|>', '[/OBJ]')\n",
    "            # decoded_clean = decoded_clean.replace('<|box_start|>', '[BOX]')\n",
    "            # decoded_clean = decoded_clean.replace('<|box_end|>', '[/BOX]')\n",
    "            \n",
    "            # if len(decoded_clean) > 150:\n",
    "            #     print(f\"  Training content: '{decoded_clean[:75]}...{decoded_clean[-75:]}'\")\n",
    "            # else:\n",
    "            #     print(f\"  Training content: '{decoded_clean}'\")\n",
    "            print(decoded)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"üìä BREAKDOWN OF TRAINED TOKENS:\")\n",
    "    print(\"  ~50 tokens: System prompt (static - same every sample)\")\n",
    "    print(\"  ~10 tokens: User prompt 'Detect the bounding box...' (static)\")\n",
    "    print(\"  ~10 tokens: Role markers like <|im_start|>, <|im_end|> (static)\")\n",
    "    print(\"  ~30 tokens: Assistant response with bbox coordinates (ONLY THIS VARIES!)\")\n",
    "    print()\n",
    "    print(\"‚ö†Ô∏è  ISSUE: 70% of trained tokens are static (don't help learning)\")\n",
    "    print(\"    Only ~30% are the actual variable bbox coordinates\")\n",
    "    print(\"    This is why loss decreases very slowly!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run token analysis\n",
    "print(\"\\nüìä Running token distribution analysis...\")\n",
    "analyze_token_distribution(collate_fn, train_dataset_formatted, num_samples=3)\n",
    "print(\"\\nüìä Running token distribution analysis on validation split...\")\n",
    "analyze_token_distribution(collate_fn, eval_dataset_formatted, num_samples=3)\n",
    "\n",
    "# Test collator with a small batch\n",
    "test_dataset = train_dataset_formatted.select(range(2))\n",
    "test_batch = [test_dataset[i] for i in range(len(test_dataset))]\n",
    "x = collate_fn(test_batch)\n",
    "print(\"\\n‚úÖ Collator test successful\")\n",
    "print(f\"Output type: {type(x)}\")\n",
    "print(f\"Keys in output: {list(x.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594288ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of x\n",
    "print(\"\\n=== DATA TYPES IN BATCH OUTPUT ===\")\n",
    "for key, value in x.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Shape: {value.shape}\")\n",
    "        print(f\"  Dtype: {value.dtype}\")\n",
    "        print(f\"  Device: {value.device}\")\n",
    "        if key == 'image_grid_thw':\n",
    "            print(f\"  Values: {value}\")\n",
    "    else:\n",
    "        print(f\"{key}: type={type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5afdde",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Check Flash Attention version - CRITICAL for debugging cu_seqlens_q dtype error\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING FLASH ATTENTION AND ENVIRONMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import flash_attn\n",
    "print(f\"Flash Attention version: {flash_attn.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Check if version is sufficient for Qwen2-VL\n",
    "flash_version = flash_attn.__version__\n",
    "major, minor = map(int, flash_version.split('.')[:2])\n",
    "if major < 2 or (major == 2 and minor < 5):\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Flash Attention version is too old for Qwen2-VL!\")\n",
    "    print(\"   Required: >= 2.5.0\")\n",
    "    print(\"   To fix, run: pip install flash-attn==2.6.3 --no-build-isolation\")\n",
    "else:\n",
    "    print(\"‚úÖ Flash Attention version is compatible\")\n",
    "\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# REFACTORED: The callbacks below (GradNormCallback, IoUEvalCallback, ConsoleLogCallback)\n",
    "# have been extracted to: src/training/callbacks.py\n",
    "#\n",
    "# Import with:\n",
    "#   from src.training.callbacks import GradNormCallback, IoUEvalCallback, ConsoleLogCallback\n",
    "#\n",
    "# The code below is kept for reference only and can be deleted.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Callbacks for training diagnostics ---\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class GradNormCallback(TrainerCallback):\n",
    "    \"\"\"Logs gradient L2 norm over trainable parameters before optimizer step.\"\"\"\n",
    "    def __init__(self, log_every_steps=0):\n",
    "        self.trainer = None\n",
    "        self.log_every_steps = log_every_steps  # 0 -> use args.logging_steps\n",
    "\n",
    "    def on_init_end(self, args, state, control, **kwargs):\n",
    "        self.trainer = kwargs.get(\"trainer\", None)\n",
    "\n",
    "    def on_pre_optimizer_step(self, args, state, control, **kwargs):\n",
    "        \"\"\"Compute gradient statistics BEFORE optimizer.step() and zero_grad().\"\"\"\n",
    "        if self.trainer is None:\n",
    "            return control\n",
    "        logging_frequency = self.log_every_steps or getattr(args, \"logging_steps\", 10)\n",
    "        if state.global_step % max(logging_frequency, 1) != 0 or state.global_step == 0:\n",
    "            return control\n",
    "\n",
    "        total_squared_norm = 0.0\n",
    "        param_count = 0\n",
    "        nonzero_grad_count = 0\n",
    "\n",
    "        # Accumulate L2 norm over all trainable parameters with existing gradients\n",
    "        for param_name, param_tensor in self.trainer.model.named_parameters():\n",
    "            if param_tensor.requires_grad and (param_tensor.grad is not None):\n",
    "                grad = param_tensor.grad.detach()\n",
    "                gnorm = grad.float().norm(2).item()\n",
    "                total_squared_norm += gnorm * gnorm\n",
    "                param_count += 1\n",
    "                if gnorm > 0:\n",
    "                    nonzero_grad_count += 1\n",
    "\n",
    "        pre_clip_norm = (total_squared_norm ** 0.5) if param_count > 0 else 0.0\n",
    "\n",
    "        max_grad_norm = float(getattr(args, \"max_grad_norm\", 1.0) or 0.0)\n",
    "        grad_clipped = int(pre_clip_norm > max_grad_norm) if max_grad_norm > 0 else 0\n",
    "        clip_ratio = float(min(1.0, max_grad_norm / (pre_clip_norm + 1e-6))) if pre_clip_norm > 0 and max_grad_norm > 0 else 1.0\n",
    "\n",
    "        gradient_logs = {\n",
    "            # Keep the original key for continuity in dashboards\n",
    "            \"grad_lora_norm\": float(pre_clip_norm),\n",
    "            # Additional diagnostics\n",
    "            \"grad_norm_pre_clip\": float(pre_clip_norm),\n",
    "            \"grad_clipped\": int(grad_clipped),\n",
    "            \"grad_clip_ratio\": float(clip_ratio),\n",
    "            \"grads_nonzero\": int(nonzero_grad_count),\n",
    "            \"grads_total\": int(param_count),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            self.trainer.log(gradient_logs)\n",
    "        except Exception as _e:\n",
    "            # Best-effort fallback\n",
    "            try:\n",
    "                import wandb as _wandb\n",
    "                if getattr(_wandb, \"run\", None) is not None:\n",
    "                    _wandb.log(gradient_logs, step=int(state.global_step))\n",
    "                else:\n",
    "                    print(f\"[GradNormCallback] {gradient_logs}\")\n",
    "            except Exception as _e2:\n",
    "                print(f\"[GradNormCallback] Logging failed: {_e}; Fallback failed: {_e2}\")\n",
    "        return control\n",
    "\n",
    "class IoUEvalCallback(TrainerCallback):\n",
    "    \"\"\"Runs quick IoU evaluation at each evaluate event and logs results.\"\"\"\n",
    "    def __init__(self, processor, eval_dataset, num_samples=24, prefix=\"eval\"):\n",
    "        self.trainer = None\n",
    "        self.processor = processor\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.num_samples = num_samples\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def on_init_end(self, args, state, control, **kwargs):\n",
    "        self.trainer = kwargs.get(\"trainer\", None)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # Use the model from the attached trainer; CallbackHandler does not pass model here\n",
    "        if self.trainer is None or getattr(self.trainer, \"model\", None) is None:\n",
    "            return control\n",
    "        model = self.trainer.model\n",
    "        logs = None\n",
    "        try:\n",
    "            # Lightweight internal IoU eval to avoid dependency ordering issues\n",
    "            import numpy as _np\n",
    "            from torchvision import ops as _ops\n",
    "            num_eval_samples = min(self.num_samples, len(self.eval_dataset))\n",
    "            iou_scores = []\n",
    "            successful_predictions = 0\n",
    "            for sample_idx in range(num_eval_samples):\n",
    "                dataset_example = self.eval_dataset[sample_idx]\n",
    "                input_image = dataset_example['image']\n",
    "                ground_truth_bbox = dataset_example['objects']['bbox'][0]\n",
    "                # Build messages\n",
    "                chat_messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": input_image},\n",
    "                        {\"type\": \"text\", \"text\": \"Detect the bounding box of the nutrition table.\"},\n",
    "                    ],\n",
    "                }]\n",
    "                prompt_text = self.processor.apply_chat_template(chat_messages, tokenize=False, add_generation_prompt=True)\n",
    "                processed_images, processed_videos = process_vision_info(chat_messages)\n",
    "                model_inputs = self.processor(text=[prompt_text], images=processed_images, videos=processed_videos, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "                # Ensure eval mode and no grad for generation\n",
    "                was_training = model.training\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    generated_output = model.generate(**model_inputs, max_new_tokens=64, do_sample=False)\n",
    "                if was_training:\n",
    "                    model.train()\n",
    "                trimmed_outputs = [output[len(input_ids):] for input_ids, output in zip(model_inputs.input_ids, generated_output)]\n",
    "                decoded_text = self.processor.batch_decode(trimmed_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "                parsed_bbox = parse_qwen_bbox_output(decoded_text)\n",
    "                if parsed_bbox:\n",
    "                    successful_predictions += 1\n",
    "                    bbox_coords = parsed_bbox[0]['bbox'] if isinstance(parsed_bbox, list) else parsed_bbox['bbox']\n",
    "                    predicted_tensor = torch.tensor([[bbox_coords[0]/1000.0, bbox_coords[1]/1000.0, bbox_coords[2]/1000.0, bbox_coords[3]/1000.0]], dtype=torch.float32)\n",
    "                    y_min, x_min, y_max, x_max = ground_truth_bbox\n",
    "                    ground_truth_tensor = torch.tensor([[x_min, y_min, x_max, y_max]], dtype=torch.float32)\n",
    "                    iou_value = _ops.box_iou(predicted_tensor, ground_truth_tensor).item()\n",
    "                    iou_scores.append(iou_value)\n",
    "                else:\n",
    "                    iou_scores.append(0.0)\n",
    "            mean_iou = float(_np.mean(iou_scores)) if iou_scores else 0.0\n",
    "            median_iou = float(_np.median(iou_scores)) if iou_scores else 0.0\n",
    "            logs = {\n",
    "                f\"{self.prefix}_iou_mean\": float(mean_iou),\n",
    "                f\"{self.prefix}_iou_median\": float(median_iou),\n",
    "                f\"{self.prefix}_iou_0.5\": float(sum(1 for iou_val in iou_scores if iou_val > 0.5) / max(num_eval_samples, 1)),\n",
    "                f\"{self.prefix}_iou_0.7\": float(sum(1 for iou_val in iou_scores if iou_val > 0.7) / max(num_eval_samples, 1)),\n",
    "                f\"{self.prefix}_det_rate\": float(successful_predictions / max(num_eval_samples, 1)),\n",
    "            }\n",
    "            try:\n",
    "                self.trainer.log(logs)\n",
    "            except Exception as _e:\n",
    "                # Fallback to direct W&B logging if available\n",
    "                try:\n",
    "                    import wandb as _wandb\n",
    "                    if getattr(_wandb, \"run\", None) is not None:\n",
    "                        _wandb.log(logs, step=int(state.global_step))\n",
    "                        print(f\"[IoUEvalCallback] Logged to W&B directly: {logs}\")\n",
    "                    else:\n",
    "                        print(f\"[IoUEvalCallback] Metrics: {logs}\")\n",
    "                except Exception as _e2:\n",
    "                    print(f\"[IoUEvalCallback] trainer.log failed: {_e}; fallback failed: {_e2}; metrics: {logs}\")\n",
    "        except Exception as e:\n",
    "            # If evaluation itself fails, report and continue\n",
    "            print(f\"[IoUEvalCallback] eval failed: {e}\")\n",
    "        return control\n",
    "\n",
    "# Optional: print selected metrics to console so readers see them in notebook output\n",
    "class ConsoleLogCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return control\n",
    "        keys = [\n",
    "            \"eval_iou_mean\", \"eval_iou_median\", \"eval_iou_0.5\", \"eval_iou_0.7\", \"eval_det_rate\",\n",
    "            \"grad_lora_norm\", \"grads_nonzero\", \"grads_total\",\n",
    "        ]\n",
    "        present = {k: logs[k] for k in keys if k in logs}\n",
    "        if present:\n",
    "            try:\n",
    "                pretty = \" \".join(\n",
    "                    [f\"{k}={v:.4f}\" if isinstance(v, (int, float)) else f\"{k}={v}\" for k, v in present.items()]\n",
    "                )\n",
    "            except Exception:\n",
    "                pretty = str(present)\n",
    "            print(f\"[metrics] {pretty}\")\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9cfb8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "# TASK: Create the SFT trainer and launch training\n",
    "\n",
    "# Create the trainer\n",
    "# For TRL 0.12.0: We already manually:\n",
    "# 1. Called prepare_model_for_kbit_training()\n",
    "# 2. Applied get_peft_model() with LoRA config\n",
    "# So we DON'T pass peft_config to SFTTrainer (model is already a PEFT model)\n",
    "\n",
    "# CRITICAL: We're NOT using gradient checkpointing at all\n",
    "# It breaks gradient flow with LoRA + quantization\n",
    "# training_args.gradient_checkpointing is already False in the config\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Already a PEFT model with LoRA adapters\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_formatted,\n",
    "    eval_dataset=eval_dataset_formatted,\n",
    "    data_collator=collate_fn,  # Using the FIXED collate_fn with Flash Attention dtype patch\n",
    "    processing_class=processor,  # Use full processor (not just tokenizer) for consistency\n",
    "    # Note: NO peft_config here since we already applied get_peft_model manually\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer created successfully\")\n",
    "print(f\"Total training samples: {len(train_dataset_formatted)}\")\n",
    "print(f\"Total evaluation samples: {len(eval_dataset_formatted)}\")\n",
    "print(f\"Number of training steps: {len(train_dataset_formatted) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2dd42",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Attach diagnostics: IoU evaluation + gradient norm monitor\n",
    "# Attach diagnostics with explicit trainer binding\n",
    "_iou_cb = IoUEvalCallback(processor=processor, eval_dataset=eval_dataset, num_samples=24, prefix=\"eval\")\n",
    "_iou_cb.trainer = trainer\n",
    "trainer.add_callback(_iou_cb)  # logs task metrics (eval_iou_mean/median/thresholds/det_rate)\n",
    "\n",
    "_grad_cb = GradNormCallback()\n",
    "_grad_cb.trainer = trainer\n",
    "trainer.add_callback(_grad_cb)  # logs gradient norms for trainable (LoRA) params\n",
    "trainer.add_callback(ConsoleLogCallback())  # prints selected metrics to console for notebook readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85a82a",
   "metadata": {
    "id": "skbpTuJlV8qN"
   },
   "source": [
    "Now, we will define the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer), which is a wrapper around the [transformers.Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class and inherits its attributes and methods. This class simplifies the fine-tuning process by properly initializing the [PeftModel](https://huggingface.co/docs/peft/v0.6.0/package_reference/peft_model) when a [PeftConfig](https://huggingface.co/docs/peft/v0.6.0/en/package_reference/config#peft.PeftConfig) object is provided. By using `SFTTrainer`, we can efficiently manage the training workflow and ensure a smooth fine-tuning experience for our Vision Language Model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bef90",
   "metadata": {
    "id": "k_jk-U7ULYtA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Launch training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "print(\"\\nSaving the fine-tuned model...\")\n",
    "trainer.save_model(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "print(f\"Model saved to {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a0ba1",
   "metadata": {
    "id": "6yx_sGW42dN3"
   },
   "source": [
    "# 5. Testing the Fine-Tuned Model üîç\n",
    "\n",
    "Now that we've successfully fine-tuned our Vision Language Model (VLM), it's time to evaluate its performance! In this section, we will test the model using examples from the ChartQA dataset to see how well it answers questions based on chart images. Let's dive in and explore the results! üöÄ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea11b1",
   "metadata": {
    "id": "i0KEPu6qYKqn"
   },
   "source": [
    "Let's clean up the GPU memory to ensure optimal performance üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e2ebd",
   "metadata": {
    "id": "Ttx6EK8Uy8t0"
   },
   "outputs": [],
   "source": [
    "# COMMENTED OUT: This deletes model and processor which we need later!\n",
    "# clear_memory()\n",
    "print(\"Skipping clear_memory() to keep model and processor available for later cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d724226",
   "metadata": {
    "id": "HwCTPHsfujn2"
   },
   "source": [
    "We will reload the base model using the same pipeline as before, but this we will load the LoRA adpaters into the model too. LoRA adapters should be selected from the saved directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf9d80",
   "metadata": {
    "id": "EFqTNUud2lA7"
   },
   "outputs": [],
   "source": [
    "# TASK:  Load model, processor, and adapter weights\n",
    "from peft import PeftModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "import torch\n",
    "\n",
    "# Path to saved LoRA adapters\n",
    "adapter_path = \"./qwen2vl-nutrition-detection-lora\"\n",
    "\n",
    "# Load the base model\n",
    "base_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "model_finetuned = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",  # Use balanced for multi-GPU consistency\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load the LoRA adapters\n",
    "model_finetuned = PeftModel.from_pretrained(\n",
    "    model_finetuned,\n",
    "    adapter_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor_finetuned = Qwen2VLProcessor.from_pretrained(adapter_path)\n",
    "\n",
    "print(f\"Loaded fine-tuned model from {adapter_path}\")\n",
    "print(f\"Model has {sum(p.numel() for p in model_finetuned.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e106e66",
   "metadata": {
    "id": "pqryChyLWRmR"
   },
   "source": [
    "Test the fine-tuned model on the example above, where the model previously struggled to accurately locate the nutrition table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f0cc7",
   "metadata": {
    "id": "LH6fWLid7JGp"
   },
   "outputs": [],
   "source": [
    "# TASK: test on the #20 training example\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# Get example #20 from training set\n",
    "example_idx = 20\n",
    "example = train_dataset[example_idx]\n",
    "image = example['image']\n",
    "ground_truth_bbox = example['objects']['bbox'][0]\n",
    "ground_truth_category = example['objects']['category_name'][0]\n",
    "\n",
    "# Create messages for inference\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Detect the bounding box of the nutrition table.\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = processor_finetuned.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Process vision information\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor_finetuned(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model_finetuned.device)\n",
    "\n",
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_finetuned.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "# Decode the output\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor_finetuned.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(f\"Example #{example_idx}\")\n",
    "print(f\"Ground truth category: {ground_truth_category}\")\n",
    "print(f\"Ground truth bbox (normalized): {ground_truth_bbox}\")\n",
    "print(f\"\\nModel output: {output_text}\")\n",
    "\n",
    "# Parse the model output\n",
    "parsed_bbox = parse_qwen_bbox_output(output_text)\n",
    "if parsed_bbox:\n",
    "    print(f\"Parsed prediction: {parsed_bbox}\")\n",
    "\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Ground truth\n",
    "img_gt = image.copy()\n",
    "draw_gt = ImageDraw.Draw(img_gt)\n",
    "width, height = img_gt.size\n",
    "# CRITICAL: OpenFoodFacts uses [y_min, x_min, y_max, x_max] format\n",
    "y_min, x_min, y_max, x_max = ground_truth_bbox\n",
    "x_min = x_min * width\n",
    "y_min = y_min * height\n",
    "x_max = x_max * width\n",
    "y_max = y_max * height\n",
    "draw_gt.rectangle([x_min, y_min, x_max, y_max], outline='green', width=3)\n",
    "\n",
    "axes[0].imshow(img_gt)\n",
    "axes[0].set_title(f\"Ground Truth: {ground_truth_category}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Model prediction\n",
    "if parsed_bbox:\n",
    "    img_pred = visualize_bbox_on_image(image, parsed_bbox, normalize_coords=True)\n",
    "    axes[1].imshow(img_pred)\n",
    "    axes[1].set_title(f\"Model Prediction: {parsed_bbox['object']}\")\n",
    "else:\n",
    "    axes[1].imshow(image)\n",
    "    axes[1].set_title(\"Model Prediction: No detection\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e5765",
   "metadata": {
    "id": "swibyq5AWctZ"
   },
   "source": [
    "Since this sample is drawn from the training set, the model has encountered it during training, which may be seen as a form of cheating. To gain a more comprehensive understanding of the model's performance, you should also evaluate it using the eval dataset. For this, write an evaluation script that measures the IoU metric between the ground truth box and the predicted boundig box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d4376",
   "metadata": {
    "id": "czZSBgnoef1E"
   },
   "outputs": [],
   "source": [
    "# Task: write the eval function. You can use use ops.box_iou\n",
    "from torchvision import ops\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, processor_model, dataset, num_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset using IoU metric.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        processor_model: The processor for the model\n",
    "        dataset: The evaluation dataset\n",
    "        num_samples: Number of samples to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Dict with evaluation metrics\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    successful_detections = 0\n",
    "    total_samples = min(num_samples, len(dataset))\n",
    "    \n",
    "    for idx in tqdm(range(total_samples), desc=\"Evaluating\"):\n",
    "        example = dataset[idx]\n",
    "        image = example['image']\n",
    "        ground_truth_bbox = example['objects']['bbox'][0]  # Take first bbox\n",
    "        \n",
    "        # Create messages for inference\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": \"Detect the bounding box of the nutrition table.\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Apply chat template\n",
    "            text = processor_model.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Process vision information\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = processor_model(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Generate prediction\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=128,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = processor_model.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            \n",
    "            # Parse the model output\n",
    "            parsed_bbox = parse_qwen_bbox_output(output_text)\n",
    "            \n",
    "            if parsed_bbox:\n",
    "                successful_detections += 1\n",
    "                \n",
    "                # Convert predicted bbox from Qwen format (1000x1000) to normalized [0,1]\n",
    "                if isinstance(parsed_bbox, list):\n",
    "                    pred_bbox = parsed_bbox[0]['bbox']\n",
    "                else:\n",
    "                    pred_bbox = parsed_bbox['bbox']\n",
    "                \n",
    "                # Normalize predicted bbox from 1000x1000 to 0-1\n",
    "                pred_bbox_norm = [\n",
    "                    pred_bbox[0] / 1000.0,\n",
    "                    pred_bbox[1] / 1000.0,\n",
    "                    pred_bbox[2] / 1000.0,\n",
    "                    pred_bbox[3] / 1000.0\n",
    "                ]\n",
    "                \n",
    "                # Convert to tensor for IoU calculation\n",
    "                # CRITICAL: Convert ground_truth from [y_min, x_min, y_max, x_max] to [x_min, y_min, x_max, y_max]\n",
    "                # because torch.ops.box_iou expects [x_min, y_min, x_max, y_max] format\n",
    "                y_min_gt, x_min_gt, y_max_gt, x_max_gt = ground_truth_bbox\n",
    "                gt_tensor = torch.tensor([[x_min_gt, y_min_gt, x_max_gt, y_max_gt]], dtype=torch.float32)\n",
    "                \n",
    "                # pred_bbox_norm is already in [x_min, y_min, x_max, y_max] format from Qwen output\n",
    "                pred_tensor = torch.tensor([[pred_bbox_norm[0], pred_bbox_norm[1],\n",
    "                                           pred_bbox_norm[2], pred_bbox_norm[3]]], dtype=torch.float32)\n",
    "                \n",
    "                # Calculate IoU\n",
    "                iou = ops.box_iou(pred_tensor, gt_tensor).item()\n",
    "                ious.append(iou)\n",
    "            else:\n",
    "                ious.append(0.0)  # No detection counts as 0 IoU\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            ious.append(0.0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"mean_iou\": np.mean(ious) if ious else 0.0,\n",
    "        \"median_iou\": np.median(ious) if ious else 0.0,\n",
    "        \"max_iou\": np.max(ious) if ious else 0.0,\n",
    "        \"min_iou\": np.min(ious) if ious else 0.0,\n",
    "        \"detection_rate\": successful_detections / total_samples,\n",
    "        \"samples_evaluated\": total_samples,\n",
    "        \"iou_threshold_0.5\": sum(1 for iou in ious if iou > 0.5) / total_samples,\n",
    "        \"iou_threshold_0.7\": sum(1 for iou in ious if iou > 0.7) / total_samples,\n",
    "    }\n",
    "    \n",
    "    return metrics, ious\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "print(\"\\nEvaluating fine-tuned model on validation set...\")\n",
    "metrics_finetuned, ious_finetuned = evaluate_model(\n",
    "    model_finetuned, \n",
    "    processor_finetuned, \n",
    "    eval_dataset, \n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuned Model Evaluation Results:\")\n",
    "print(f\"  Mean IoU: {metrics_finetuned['mean_iou']:.4f}\")\n",
    "print(f\"  Median IoU: {metrics_finetuned['median_iou']:.4f}\")\n",
    "print(f\"  Detection Rate: {metrics_finetuned['detection_rate']:.2%}\")\n",
    "print(f\"  IoU > 0.5: {metrics_finetuned['iou_threshold_0.5']:.2%}\")\n",
    "print(f\"  IoU > 0.7: {metrics_finetuned['iou_threshold_0.7']:.2%}\")\n",
    "\n",
    "# Plot IoU distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(ious_finetuned, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('IoU Score')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of IoU Scores - Fine-tuned Model')\n",
    "plt.axvline(metrics_finetuned['mean_iou'], color='red', linestyle='--', label=f\"Mean IoU: {metrics_finetuned['mean_iou']:.3f}\")\n",
    "plt.axvline(0.5, color='green', linestyle='--', alpha=0.5, label='IoU = 0.5')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e7b54",
   "metadata": {
    "id": "ATuQ6ZS6eirO",
    "outputId": "c3adc0fd-0fdc-4ff4-cc4e-14b4d9039323"
   },
   "source": [
    "Do the same evaluation for the model without finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f3799",
   "metadata": {
    "id": "eval_base_model"
   },
   "outputs": [],
   "source": [
    "# Evaluate base model without fine-tuning\n",
    "print(\"\\nLoading base model for comparison...\")\n",
    "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"balanced\",  # Use balanced for multi-GPU consistency\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base_processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "print(\"\\nEvaluating base model (without fine-tuning) on validation set...\")\n",
    "metrics_base, ious_base = evaluate_model(\n",
    "    base_model,\n",
    "    base_processor,\n",
    "    eval_dataset,\n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nBase Model Evaluation Results (No Fine-tuning):\")\n",
    "print(f\"  Mean IoU: {metrics_base['mean_iou']:.4f}\")\n",
    "print(f\"  Median IoU: {metrics_base['median_iou']:.4f}\")\n",
    "print(f\"  Detection Rate: {metrics_base['detection_rate']:.2%}\")\n",
    "print(f\"  IoU > 0.5: {metrics_base['iou_threshold_0.5']:.2%}\")\n",
    "print(f\"  IoU > 0.7: {metrics_base['iou_threshold_0.7']:.2%}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON: Fine-tuned vs Base Model\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean IoU Improvement: {(metrics_finetuned['mean_iou'] - metrics_base['mean_iou']):.4f} \"\n",
    "      f\"({((metrics_finetuned['mean_iou'] - metrics_base['mean_iou']) / max(metrics_base['mean_iou'], 0.001) * 100):.1f}% improvement)\")\n",
    "print(f\"Detection Rate Improvement: {(metrics_finetuned['detection_rate'] - metrics_base['detection_rate']):.2%}\")\n",
    "print(f\"IoU > 0.5 Improvement: {(metrics_finetuned['iou_threshold_0.5'] - metrics_base['iou_threshold_0.5']):.2%}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# IoU distribution comparison\n",
    "axes[0].hist(ious_base, bins=20, alpha=0.5, label='Base Model', edgecolor='black')\n",
    "axes[0].hist(ious_finetuned, bins=20, alpha=0.5, label='Fine-tuned Model', edgecolor='black')\n",
    "axes[0].set_xlabel('IoU Score')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_title('IoU Distribution Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison bar chart\n",
    "metrics_names = ['Mean IoU', 'Detection Rate', 'IoU > 0.5', 'IoU > 0.7']\n",
    "base_values = [\n",
    "    metrics_base['mean_iou'],\n",
    "    metrics_base['detection_rate'],\n",
    "    metrics_base['iou_threshold_0.5'],\n",
    "    metrics_base['iou_threshold_0.7']\n",
    "]\n",
    "finetuned_values = [\n",
    "    metrics_finetuned['mean_iou'],\n",
    "    metrics_finetuned['detection_rate'],\n",
    "    metrics_finetuned['iou_threshold_0.5'],\n",
    "    metrics_finetuned['iou_threshold_0.7']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, base_values, width, label='Base Model', alpha=0.8)\n",
    "axes[1].bar(x + width/2, finetuned_values, width, label='Fine-tuned Model', alpha=0.8)\n",
    "axes[1].set_xlabel('Metrics')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Model Performance Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics_names, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up base model to free memory\n",
    "del base_model\n",
    "del base_processor\n",
    "# COMMENTED OUT: This deletes model and processor which we need for later cells!\n",
    "# clear_memory()\n",
    "print(\"Deleted base_model and base_processor, but keeping model and processor for later use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fec56e",
   "metadata": {
    "id": "iyhR69T3dfLI"
   },
   "source": [
    "# üßë‚Äçüç≥ [Optional]  The recipe\n",
    "For the best model accuracy, one can first finetune the vision encoder while freezing the LLM. Then, we can use the ckpt above and finetune the model by applying LoRA to vision encoder and QLoRA to LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034cc02",
   "metadata": {
    "id": "ODzQhQ1R1SAR"
   },
   "source": [
    "# üîÄ Merge LoRA\n",
    "After fine-tuning with LoRA, the adapter weights can be merged back into the base model, effectively eliminating the overhead of LoRA modules during inference. This fusion produces a standalone model suitable for efficient deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b421d",
   "metadata": {
    "id": "sbDEhQDyqbzK"
   },
   "source": [
    "# üöÄ Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b236c",
   "metadata": {
    "id": "XjtLbVqWqFnE"
   },
   "source": [
    "Try to export your trained model (with merged LoRA weights) to vLLM and then deploy into Nvidia triton!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a1320",
   "metadata": {
    "id": "b3SH27Iu193x",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Optional: Merge LoRA weights into base model for deployment\n",
    "print(\"\\nMerging LoRA weights into base model for deployment...\")\n",
    "merged_model = model_finetuned.merge_and_unload()\n",
    "\n",
    "# Save the merged model to SSD\n",
    "merged_output_dir = \"/ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-merged\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "processor_finetuned.save_pretrained(merged_output_dir)\n",
    "\n",
    "print(f\"Merged model saved to {merged_output_dir}\")\n",
    "print(\"This model can now be loaded without PEFT and used for efficient inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88c0e1",
   "metadata": {
    "id": "ejJwH5xpcY6K"
   },
   "source": [
    "## Bonus\n",
    "\n",
    "For Qwen2-VL, implement a custom collate_fn that restricts loss computation to the answer portion only, explicitly excluding the system prompt and question from the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ead3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Custom collate function with loss only on answer portion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb8a0a",
   "metadata": {},
   "source": [
    "---- Draft\n",
    "\n",
    "\n",
    "=============================================================================\n",
    "REFACTORED: The draft collators below have been backed up to:\n",
    "src/data/collators.py (under the \"BACKUP/DRAFT COLLATORS\" section)\n",
    "\n",
    "Backed up functions:\n",
    "  - collate_fn_fixed_2 (verbose debug collator)\n",
    "  - debug_collate_fn (debug helper function)\n",
    "  - test_collate_fn_fixed_3 (test function for coordinate scaling)\n",
    "\n",
    "The code below is kept for reference only and can be deleted.\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29655b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_fixed_2(batch):\n",
    "    \"\"\"\n",
    "    FIXED VERSION: Robust collate function that prevents out-of-range label issues.\n",
    "    \n",
    "    This collator:\n",
    "    1. Restores IMAGE_PLACEHOLDER with actual PIL images\n",
    "    2. Uses process_vision_info to properly extract and format images\n",
    "    3. Applies chat template to get formatted text\n",
    "    4. Tokenizes text and processes images together\n",
    "    5. Creates labels with proper masking for loss computation\n",
    "    \n",
    "    Key improvements:\n",
    "    - Properly masks all vision/special tokens to prevent training on them\n",
    "    - Validates that no out-of-vocabulary tokens exist in labels\n",
    "    - Uses token-based span finding for robust assistant response extraction\n",
    "    - Handles edge cases gracefully (samples without images, OOV tokens)\n",
    "    - CRITICAL: Initially masks ALL tokens, then selectively unmasks only assistant responses\n",
    "    - Provides detailed debugging statistics about training token percentages\n",
    "    \n",
    "    Args:\n",
    "        batch: List of samples from the dataset, each containing 'messages' and 'image'\n",
    "        \n",
    "    Returns:\n",
    "        Dict with input_ids, attention_mask, pixel_values, image_grid_thw, and labels\n",
    "    \"\"\"\n",
    "    # STEP 1: Format the dataset into desired format:\n",
    "    # Restores IMAGE_PLACEHOLDER with actual PIL image\n",
    "    # so each element in the batch looks like sample of convert_to_conversation_format(train_dataset[0])\n",
    "\n",
    "\n",
    "    # Extract messages and images from each sample\n",
    "    messages_list = [sample['messages'] for sample in batch]\n",
    "    images_list = [sample.get('image', None) for sample in batch]\n",
    "\n",
    "    # Filter out samples without images\n",
    "    valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "    if not valid_pairs:\n",
    "        raise ValueError(\"Batch contains no valid images.\")\n",
    "\n",
    "    messages_list, images_list = zip(*valid_pairs)\n",
    "    messages_list = list(messages_list)\n",
    "    images_list = list(images_list)\n",
    "\n",
    "\n",
    "    all_conversations = []  # This will hold complete conversations, each conversation is a list of 3 messages (system, assistant, user)\n",
    "\n",
    "    for messages, image in zip(messages_list, images_list):\n",
    "        messages_with_image = []\n",
    "        # Clean up messages: remove None values and restore images\n",
    "        for msg in messages:\n",
    "            msg_copy = {'role': msg['role'], 'content': []}\n",
    "\n",
    "            for content_item in msg['content']:\n",
    "                # Skip None entries entirely\n",
    "                if content_item is None:\n",
    "                    continue\n",
    "\n",
    "                # Process text content - filter out None text values\n",
    "                if content_item.get('type') == 'text':\n",
    "                    text_value = content_item.get('text')\n",
    "                    if text_value is not None and text_value != 'None':  # Check for actual None and string 'None'\n",
    "                        msg_copy['content'].append({\n",
    "                            'type': 'text',\n",
    "                            'text': text_value\n",
    "                        })\n",
    "                # Process image content - replace IMAGE_PLACEHOLDER with actual PIL image\n",
    "                elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                    # Check if it's IMAGE_PLACEHOLDER and replace with actual image\n",
    "                    image_value = content_item.get('image')\n",
    "                    if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                        # Replace with the actual PIL image from top level\n",
    "                        msg_copy['content'].append({\n",
    "                            'type': 'image',\n",
    "                            'image': image  # Use the actual PIL image\n",
    "                        })\n",
    "                    elif image_value and image_value != 'None':\n",
    "                        # Use existing image if it's not placeholder\n",
    "                        msg_copy['content'].append({\n",
    "                            'type': 'image',\n",
    "                            'image': image_value\n",
    "                        })\n",
    "\n",
    "            if msg_copy['content']:\n",
    "                messages_with_image.append(msg_copy)\n",
    "\n",
    "        # Add the complete conversation (3 messages) to collection\n",
    "        all_conversations.append(messages_with_image)\n",
    "\n",
    "    # Apply chat template to get text\n",
    "    text = processor.apply_chat_template(\n",
    "        all_conversations,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # process_vision_info to get image and video\n",
    "    image, video = process_vision_info(all_conversations)\n",
    "\n",
    "    # Process texts and images together\n",
    "    batch_inputs = processor( # batch_inputs is a dictionary containing input_ids, attention_mask, pixel_values, and image_grid_thw\n",
    "        text=text,\n",
    "        images=image,\n",
    "        # videos=video,\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Create labels from input_ids\n",
    "    labels = batch_inputs[\"input_ids\"].clone()\n",
    "\n",
    "    # Get vocabulary size for validation\n",
    "    vocab_size = processor.tokenizer.vocab_size\n",
    "\n",
    "    # Track OOV tokens (these are vision tokens with IDs >= vocab_size)\n",
    "    # IMPORTANT: We do NOT modify input_ids - vision tokens are valid for the model!\n",
    "    oov_mask = batch_inputs[\"input_ids\"] >= vocab_size\n",
    "    if oov_mask.any():\n",
    "        num_oov = oov_mask.sum().item()\n",
    "        print(f\"[collate_fn_fixed_2] INFO: Found {num_oov} vision tokens (IDs >= vocab_size)\")\n",
    "        # These are expected vision tokens - we just mask them in labels\n",
    "        labels[oov_mask] = -100\n",
    "\n",
    "    # IMPORTANT: Start by masking EVERYTHING - this ensures we only train on what we explicitly unmask\n",
    "    labels[:, :] = -100\n",
    "\n",
    "    # Collect special token IDs for verification\n",
    "    special_token_ids = set()\n",
    "\n",
    "    # Add padding token\n",
    "    if processor.tokenizer.pad_token_id is not None:\n",
    "        special_token_ids.add(processor.tokenizer.pad_token_id)\n",
    "\n",
    "    # Add vision and chat special tokens\n",
    "    special_token_strings = [\n",
    "        \"<|vision_start|>\", \"<|vision_end|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
    "        \"<|box_start|>\", \"<|box_end|>\"\n",
    "    ]\n",
    "\n",
    "    for token_str in special_token_strings:\n",
    "        if token_str in processor.tokenizer.get_vocab():\n",
    "            token_id = processor.tokenizer.convert_tokens_to_ids(token_str)\n",
    "            if token_id is not None:  # Don't check vocab_size - special tokens might be outside\n",
    "                special_token_ids.add(token_id)\n",
    "\n",
    "    # DEBUG: Let's understand the token structure\n",
    "    debug_mode = True  # Set to False in production\n",
    "    if debug_mode and labels.shape[0] > 0:\n",
    "        # Decode first 100 tokens to see structure\n",
    "        first_100 = batch_inputs[\"input_ids\"][0][:100]\n",
    "        decoded = processor.tokenizer.decode(first_100, skip_special_tokens=False)\n",
    "        print(f\"[DEBUG] First 100 tokens decoded: {decoded[:200]}...\")\n",
    "\n",
    "    # Find and unmask ONLY assistant responses\n",
    "    # Try multiple formats for assistant markers\n",
    "    assistant_markers = [\n",
    "        (\"<|im_start|>assistant\\n\", \"<|im_end|>\"),\n",
    "        (\"<|im_start|>assistant\", \"<|im_end|>\"),  # Without newline\n",
    "        (\"assistant\\n\", \"<|im_end|>\"),  # Simpler pattern\n",
    "    ]\n",
    "\n",
    "    assistant_found = False\n",
    "    for start_text, end_text in assistant_markers:\n",
    "        if assistant_found:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            assistant_start_ids = processor.tokenizer.encode(start_text, add_special_tokens=False)\n",
    "            assistant_end_ids = processor.tokenizer.encode(end_text, add_special_tokens=False)\n",
    "\n",
    "            if debug_mode:\n",
    "                print(f\"[DEBUG] Trying pattern: '{start_text}' -> {assistant_start_ids}\")\n",
    "\n",
    "            if assistant_start_ids and assistant_end_ids:\n",
    "                # Helper function to find token sequence\n",
    "                def find_token_sequence(tokens, sequence, start_pos=0):\n",
    "                    seq_len = len(sequence)\n",
    "                    for i in range(start_pos, len(tokens) - seq_len + 1):\n",
    "                        if tokens[i:i + seq_len].tolist() == sequence:\n",
    "                            return i\n",
    "                    return -1\n",
    "\n",
    "                # Process each sequence in the batch\n",
    "                assistant_tokens_found = 0\n",
    "                for batch_idx in range(labels.shape[0]):\n",
    "                    input_ids_list = batch_inputs[\"input_ids\"][batch_idx]\n",
    "\n",
    "                    # Find all assistant response spans\n",
    "                    pos = 0\n",
    "                    while pos < len(input_ids_list):\n",
    "                        # Find start of assistant response\n",
    "                        start_pos = find_token_sequence(input_ids_list, assistant_start_ids, pos)\n",
    "                        if start_pos == -1:\n",
    "                            break\n",
    "\n",
    "                        assistant_found = True\n",
    "\n",
    "                        # Find end of assistant response\n",
    "                        response_start = start_pos + len(assistant_start_ids)\n",
    "                        end_pos = find_token_sequence(input_ids_list, assistant_end_ids, response_start)\n",
    "\n",
    "                        if end_pos == -1:\n",
    "                            # No end marker found, unmask to the end\n",
    "                            end_pos = len(input_ids_list)\n",
    "\n",
    "                        if debug_mode and batch_idx == 0 and assistant_tokens_found == 0:\n",
    "                            # Debug: Show what we're unmasking\n",
    "                            response_tokens = input_ids_list[response_start:min(response_start+50, end_pos)]\n",
    "                            response_text = processor.tokenizer.decode(response_tokens, skip_special_tokens=False)\n",
    "                            print(f\"[DEBUG] Found assistant response: '{response_text}'\")\n",
    "\n",
    "                        # Unmask the assistant response (but not special tokens)\n",
    "                        for idx in range(response_start, end_pos):\n",
    "                            token_id = input_ids_list[idx].item()\n",
    "                            # Only unmask if it's not a special token and within vocab range\n",
    "                            if token_id not in special_token_ids and 0 <= token_id < vocab_size:\n",
    "                                labels[batch_idx, idx] = token_id\n",
    "                                assistant_tokens_found += 1\n",
    "\n",
    "                        # Move past this response\n",
    "                        pos = end_pos + len(assistant_end_ids) if end_pos < len(input_ids_list) else len(input_ids_list)\n",
    "\n",
    "                if assistant_tokens_found > 0:\n",
    "                    print(f\"[collate_fn_fixed_2] Found {assistant_tokens_found} assistant tokens to train on\")\n",
    "                    break  # Found with this pattern, don't try others\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[collate_fn_fixed_2] Error trying pattern '{start_text}': {e}\")\n",
    "\n",
    "    if not assistant_found:\n",
    "        print(\"[collate_fn_fixed_2] WARNING: No assistant responses found with any pattern!\")\n",
    "        # DEBUG: Show the actual structure\n",
    "        if debug_mode and labels.shape[0] > 0:\n",
    "            # Find where assistant might be\n",
    "            full_text = processor.tokenizer.decode(batch_inputs[\"input_ids\"][0], skip_special_tokens=False)\n",
    "            assistant_idx = full_text.find(\"assistant\")\n",
    "            if assistant_idx >= 0:\n",
    "                print(f\"[DEBUG] Found 'assistant' at position {assistant_idx}\")\n",
    "                print(f\"[DEBUG] Context: ...{full_text[max(0, assistant_idx-20):assistant_idx+100]}...\")\n",
    "\n",
    "    # Final validation\n",
    "    valid_labels = labels[labels != -100]\n",
    "    if valid_labels.numel() == 0:\n",
    "        print(\"[collate_fn_fixed_2] WARNING: No tokens left for training after masking!\")\n",
    "    else:\n",
    "        # Check all unmasked tokens are within vocabulary\n",
    "        max_label = valid_labels.max().item()\n",
    "        if max_label >= vocab_size:\n",
    "            print(f\"[collate_fn_fixed_2] ERROR: Found label {max_label} >= vocab_size {vocab_size}\")\n",
    "            labels[labels >= vocab_size] = -100\n",
    "\n",
    "        # Print statistics\n",
    "        unmasked_count = valid_labels.numel()\n",
    "        total_count = labels.numel()\n",
    "        print(f\"[collate_fn_fixed_2] Unmasked {unmasked_count}/{total_count} tokens ({unmasked_count/total_count*100:.1f}%) for training\")\n",
    "\n",
    "    # Add labels to batch\n",
    "    batch_inputs[\"labels\"] = labels\n",
    "\n",
    "    return batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ collate_fn_fixed_2 created with improved label masking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fe720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Test collate_fn_fixed_2 and verify training tokens\n",
    "def debug_collate_fn(collate_fn, dataset, num_samples=2):\n",
    "    \"\"\"Debug helper to test collator and see what we're training on\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DEBUG: Testing collate_fn_fixed_2\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test with a small batch\n",
    "    test_batch = [dataset[i] for i in range(min(num_samples, len(dataset)))]\n",
    "    \n",
    "    print(f\"\\nüìã Testing with {len(test_batch)} samples...\")\n",
    "    \n",
    "    # Run the collator\n",
    "    try:\n",
    "        batch_output = collate_fn(test_batch)\n",
    "        print(\"‚úÖ Collator executed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Collator failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    # Check sequence lengths\n",
    "    seq_lengths = (batch_output[\"attention_mask\"] == 1).sum(dim=1)\n",
    "    print(f\"\\nüìä Sequence statistics:\")\n",
    "    print(f\"   Sequence lengths: {seq_lengths.tolist()}\")\n",
    "    print(f\"   Max sequence length: {seq_lengths.max().item()}\")\n",
    "    print(f\"   Min sequence length: {seq_lengths.min().item()}\")\n",
    "    \n",
    "    # Check what tokens we're actually training on\n",
    "    labels = batch_output[\"labels\"]\n",
    "    print(f\"\\nüéØ Training token analysis:\")\n",
    "    \n",
    "    for batch_idx in range(labels.shape[0]):\n",
    "        unmasked_indices = (labels[batch_idx] != -100).nonzero(as_tuple=True)[0]\n",
    "        seq_len = seq_lengths[batch_idx].item()\n",
    "        \n",
    "        print(f\"\\n   Sample {batch_idx}:\")\n",
    "        print(f\"   - Total tokens: {seq_len}\")\n",
    "        print(f\"   - Training tokens: {len(unmasked_indices)}\")\n",
    "        print(f\"   - Training percentage: {len(unmasked_indices)/seq_len*100:.2f}%\")\n",
    "        \n",
    "        if len(unmasked_indices) > 0:\n",
    "            # Decode what we're training on\n",
    "            unmasked_tokens = batch_output[\"input_ids\"][batch_idx][unmasked_indices]\n",
    "            decoded_text = processor.tokenizer.decode(unmasked_tokens, skip_special_tokens=False)\n",
    "            # Clean up the decoded text for display\n",
    "            decoded_clean = decoded_text.replace('<|object_ref_start|>', '[OBJ_START]')\n",
    "            decoded_clean = decoded_clean.replace('<|object_ref_end|>', '[OBJ_END]')\n",
    "            decoded_clean = decoded_clean.replace('<|box_start|>', '[BOX_START]')\n",
    "            decoded_clean = decoded_clean.replace('<|box_end|>', '[BOX_END]')\n",
    "            print(f\"   - Training on: '{decoded_clean}'\")\n",
    "            \n",
    "            # Verify it's bbox coordinates\n",
    "            if 'nutrition-table' in decoded_text and '(' in decoded_text:\n",
    "                print(f\"   ‚úÖ Correctly training on bbox coordinates\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Warning: Unexpected training content\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Testing forward pass with model...\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Move batch to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                device = next(model.parameters()).device\n",
    "                batch_output_gpu = {}\n",
    "                for k, v in batch_output.items():\n",
    "                    if hasattr(v, 'to'):\n",
    "                        batch_output_gpu[k] = v.to(device)\n",
    "                    else:\n",
    "                        batch_output_gpu[k] = v\n",
    "            else:\n",
    "                batch_output_gpu = batch_output\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch_output_gpu)\n",
    "            loss_value = outputs.loss.item() if outputs.loss is not None else None\n",
    "            \n",
    "            print(\"‚úÖ Forward pass successful!\")\n",
    "            if loss_value is not None:\n",
    "                print(f\"   Loss: {loss_value:.4f}\")\n",
    "                if loss_value > 10:\n",
    "                    print(\"   ‚ö†Ô∏è Warning: Loss is very high, training might be unstable\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è Warning: No loss returned\")\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            if \"cu_seqlens_q must have dtype int32\" in str(e):\n",
    "                print(f\"‚ùå Flash Attention dtype error detected!\")\n",
    "                print(\"   This is the known issue with Flash Attention version mismatch\")\n",
    "                print(\"   Solution: Upgrade Flash Attention to 2.6.3\")\n",
    "                print(\"   Command: pip install flash-attn==2.6.3 --no-build-isolation\")\n",
    "            else:\n",
    "                print(f\"‚ùå Forward pass failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Forward pass failed with unexpected error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    return batch_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4525c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run the debug test\n",
    "print(\"Running debug test on collate_fn_fixed_2...\")\n",
    "test_output = debug_collate_fn(collate_fn_fixed_2, train_dataset_formatted, num_samples=2)\n",
    "\n",
    "# If the test fails with cu_seqlens_q error, we know it's Flash Attention version issue\n",
    "if test_output is None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ö†Ô∏è NEXT STEPS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"1. The collate_fn_fixed_2 is working correctly\")\n",
    "    print(\"2. The error is likely due to Flash Attention version mismatch\")\n",
    "    print(\"3. Run: pip install flash-attn==2.6.3 --no-build-isolation\")\n",
    "    print(\"4. Restart the kernel after installation\")\n",
    "    print(\"5. Re-run from model loading onwards\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30618c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Clean collate_fn_fixed_3 class that only trains on assistant responses\n",
    "class collate_fn_fixed_3:\n",
    "    \"\"\"\n",
    "    Clean collator that masks everything except assistant responses.\n",
    "    This ensures the model only learns the actual detection task (bbox coordinates)\n",
    "    rather than memorizing static prompts.\n",
    "    \n",
    "    Key features:\n",
    "    - Masks all padding, vision, system, and user tokens\n",
    "    - Only trains on assistant response tokens\n",
    "    - No OOV complexity - keeps it simple\n",
    "    - Proper class structure with processor and model passed in __init__\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processor, model):\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        \n",
    "        # Track special tokens to mask\n",
    "        self.pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "        self.vision_token_ids = [\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'),\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'),\n",
    "            self.processor.tokenizer.convert_tokens_to_ids('<|image_pad|>')\n",
    "        ]\n",
    "        \n",
    "        # Assistant markers for finding response boundaries\n",
    "        self.assistant_start_token = \"<|im_start|>assistant\\n\"\n",
    "        self.assistant_end_token = \"<|im_end|>\"\n",
    "        \n",
    "        # For debugging - set to True to see first few samples\n",
    "        self.debug = False\n",
    "        self.debug_count = 0\n",
    "        self.max_debug = 3\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        # Step 1: Extract and prepare messages with actual images\n",
    "        messages_list = [sample['messages'] for sample in batch]\n",
    "        images_list = [sample.get('image', None) for sample in batch]\n",
    "        \n",
    "        # Filter out samples without images\n",
    "        valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "        if not valid_pairs:\n",
    "            raise ValueError(\"Batch contains no valid images.\")\n",
    "        \n",
    "        messages_list, images_list = zip(*valid_pairs)\n",
    "        messages_list = list(messages_list)\n",
    "        images_list = list(images_list)\n",
    "        \n",
    "        # Process each sample to restore IMAGE_PLACEHOLDER with actual images\n",
    "        all_conversations = []\n",
    "        for messages, image in zip(messages_list, images_list):\n",
    "            messages_with_image = []\n",
    "            for msg in messages:\n",
    "                msg_copy = {'role': msg['role'], 'content': []}\n",
    "                \n",
    "                for content_item in msg['content']:\n",
    "                    if content_item is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process text content\n",
    "                    if content_item.get('type') == 'text':\n",
    "                        text_value = content_item.get('text')\n",
    "                        if text_value is not None and text_value != 'None':\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'text',\n",
    "                                'text': text_value\n",
    "                            })\n",
    "                    # Process image content for user messages\n",
    "                    elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                        image_value = content_item.get('image')\n",
    "                        if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'image',\n",
    "                                'image': image  # Use actual PIL image\n",
    "                            })\n",
    "                        elif image_value and image_value != 'None':\n",
    "                            msg_copy['content'].append({\n",
    "                                'type': 'image',\n",
    "                                'image': image_value\n",
    "                            })\n",
    "                \n",
    "                if msg_copy['content']:\n",
    "                    messages_with_image.append(msg_copy)\n",
    "            \n",
    "            all_conversations.append(messages_with_image)\n",
    "        \n",
    "        # Step 2: Apply chat template\n",
    "        texts = self.processor.apply_chat_template(\n",
    "            all_conversations,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # Step 3: Process vision info\n",
    "        images, videos = process_vision_info(all_conversations)\n",
    "        \n",
    "        # Step 4: Tokenize and process\n",
    "        batch_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Step 5: Create labels - start with everything masked\n",
    "        labels = batch_inputs[\"input_ids\"].clone()\n",
    "        labels[:, :] = -100  # Mask everything initially\n",
    "        \n",
    "        # Step 6: Find and unmask ONLY assistant responses\n",
    "        for batch_idx, text in enumerate(texts):\n",
    "            input_ids = batch_inputs[\"input_ids\"][batch_idx]\n",
    "            \n",
    "            # Find assistant response boundaries in the text\n",
    "            assistant_start_pos = text.find(self.assistant_start_token)\n",
    "            if assistant_start_pos == -1:\n",
    "                # Try without newline\n",
    "                assistant_start_pos = text.find(\"<|im_start|>assistant\")\n",
    "            \n",
    "            if assistant_start_pos != -1:\n",
    "                # Find where actual response starts (after the marker)\n",
    "                response_start_in_text = assistant_start_pos + len(self.assistant_start_token)\n",
    "                \n",
    "                # Find end of assistant response\n",
    "                assistant_end_pos = text.find(self.assistant_end_token, response_start_in_text)\n",
    "                \n",
    "                if assistant_end_pos != -1:\n",
    "                    # Extract just the response text\n",
    "                    response_text = text[response_start_in_text:assistant_end_pos]\n",
    "                    \n",
    "                    # Tokenize the response to find it in input_ids\n",
    "                    response_tokens = self.processor.tokenizer.encode(\n",
    "                        response_text, \n",
    "                        add_special_tokens=False\n",
    "                    )\n",
    "                    \n",
    "                    if response_tokens:\n",
    "                        # Find this sequence in the full input_ids\n",
    "                        for i in range(len(input_ids) - len(response_tokens) + 1):\n",
    "                            if input_ids[i:i+len(response_tokens)].tolist() == response_tokens:\n",
    "                                # Unmask these tokens for training\n",
    "                                labels[batch_idx, i:i+len(response_tokens)] = input_ids[i:i+len(response_tokens)]\n",
    "                                break\n",
    "        \n",
    "        # Step 7: Debug output (only for first few samples)\n",
    "        if self.debug and self.debug_count < self.max_debug:\n",
    "            self.debug_count += 1\n",
    "            non_masked = (labels != -100).sum().item()\n",
    "            total = labels.numel()\n",
    "            if non_masked > 0:\n",
    "                # Decode what we're training on\n",
    "                sample_labels = labels[0]\n",
    "                trained_indices = (sample_labels != -100).nonzero(as_tuple=True)[0]\n",
    "                if len(trained_indices) > 0:\n",
    "                    trained_tokens = batch_inputs[\"input_ids\"][0][trained_indices]\n",
    "                    decoded = self.processor.tokenizer.decode(trained_tokens, skip_special_tokens=False)\n",
    "                    print(f\"[collate_fn_fixed_3] Sample {self.debug_count}: Training on {len(trained_indices)} tokens\")\n",
    "                    print(f\"  Content: '{decoded}'\")\n",
    "                    print(f\"  This is {100*len(trained_indices)/len(sample_labels):.1f}% of {len(sample_labels)} total tokens\")\n",
    "        \n",
    "        batch_inputs[\"labels\"] = labels\n",
    "        return batch_inputs\n",
    "\n",
    "print(\"‚úÖ Created collate_fn_fixed_3 - clean assistant-only training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test collate_fn_fixed_3 and verify coordinate scaling\n",
    "def test_collate_fn_fixed_3():\n",
    "    \"\"\"Test the new collator and verify coordinate scaling.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING collate_fn_fixed_3 & COORDINATE SCALING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create collator instance\n",
    "    # Note: processor and model should already be loaded from earlier cells\n",
    "    # We commented out the clear_memory() calls to preserve them\n",
    "    collator = collate_fn_fixed_3(processor, model)\n",
    "    collator.debug = True  # Enable debug output\n",
    "    \n",
    "    # Test with first sample\n",
    "    # Note: train_dataset_formatted should be available from data preprocessing cells\n",
    "    test_batch = [train_dataset_formatted[0]]\n",
    "    \n",
    "    print(\"\\n1. TESTING COLLATOR:\")\n",
    "    try:\n",
    "        output = collator(test_batch)\n",
    "        print(\"‚úÖ Collator executed successfully\")\n",
    "        \n",
    "        # Check what we're training on\n",
    "        labels = output['labels'][0]\n",
    "        trained_indices = (labels != -100).nonzero(as_tuple=True)[0]\n",
    "        print(f\"   Training on {len(trained_indices)} tokens out of {len(labels)} total\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Collator failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n2. COORDINATE SCALING CHECK:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check a sample to verify coordinate scaling\n",
    "    example = train_dataset[0]\n",
    "    bbox = example['objects']['bbox'][0]\n",
    "    \n",
    "    print(f\"Original bbox (normalized [0,1]): {bbox}\")\n",
    "    \n",
    "    # Show how we convert for training\n",
    "    y_min, x_min, y_max, x_max = bbox\n",
    "    x1 = int(x_min * 1000)\n",
    "    y1 = int(y_min * 1000)\n",
    "    x2 = int(x_max * 1000)\n",
    "    y2 = int(y_max * 1000)\n",
    "    \n",
    "    print(f\"Converted for training ([0,1000)): ({x1},{y1}),({x2},{y2})\")\n",
    "    print(f\"Format in training data: <|box_start|>({x1},{y1}),({x2},{y2})<|box_end|>\")\n",
    "    \n",
    "    print(\"\\n‚úÖ COORDINATE SCALING IS CORRECT:\")\n",
    "    print(\"   - Dataset provides bbox in [0,1] format\")\n",
    "    print(\"   - We convert to [0,1000) for training (Qwen2-VL expects this)\")\n",
    "    print(\"   - Model should output in [0,1000) format\")\n",
    "    print(\"   - parse_qwen_bbox_output expects [0,1000) and converts back\")\n",
    "    \n",
    "    print(\"\\n3. WHY MODEL MIGHT PREDICT NO BOX:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   Possible reasons:\")\n",
    "    print(\"   1. Training on static prompts (70% of tokens) - poor learning signal\")\n",
    "    print(\"   2. Loss plateaued at ~3.25 - model not improving\")\n",
    "    print(\"   3. Need more epochs or higher learning rate\")\n",
    "    print(\"   4. Should use collate_fn_fixed_3 for assistant-only training\")\n",
    "    \n",
    "    return collator\n",
    "\n",
    "# Run the test\n",
    "print(\"\\nüìä Testing new collator and coordinate system...\")\n",
    "test_collator = test_collate_fn_fixed_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abab56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# SECOND TRAINING RUN: Using collate_fn_fixed_3 (Assistant-Only Training)\n",
    "# ================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECOND TRAINING RUN: ASSISTANT-ONLY WITH collate_fn_fixed_3\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "This training run focuses ONLY on assistant responses (bbox coordinates),\n",
    "ignoring system prompts and user messages. This should lead to:\n",
    "- Better convergence on the actual detection task\n",
    "- Lower training loss (training on ~30% of tokens instead of 100%)\n",
    "- More efficient use of compute resources\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfca5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory before starting second training\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"\\nüßπ Clearing GPU memory before second training...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.synchronize()\n",
    "print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0236210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new training configuration for assistant-only training\n",
    "from trl import SFTConfig\n",
    "\n",
    "training_args_v3 = SFTConfig(\n",
    "    # Output and logging - Different directory for comparison\n",
    "    output_dir=\"/ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-lora-assistantonly\",\n",
    "    logging_dir=\"/ssd1/zhuoyuan/vlm_outputs/logs-assistantonly\",\n",
    "    logging_steps=10,  # Show training loss every 10 steps\n",
    "    \n",
    "    # Training hyperparameters (same as original for fair comparison)\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 16\n",
    "    gradient_checkpointing=False,  # Disabled - causes issues with QLoRA\n",
    "    \n",
    "    # Learning rate and optimization (same as original)\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta2=0.999,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Mixed precision and performance\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    dataloader_num_workers=0,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Advanced options\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    \n",
    "    # Debugging / reporting\n",
    "    report_to=\"wandb\",  # Use W&B for consistency with first trainer\n",
    "    run_name=\"qwen2vl-7b-assistant-only\",  # Distinct run name for clear separation on W&B\n",
    "    \n",
    "    # TRL specific\n",
    "    dataset_text_field=None,  # We handle formatting in collate_fn\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created new SFTConfig for assistant-only training\")\n",
    "print(f\"   Output directory: {training_args_v3.output_dir}\")\n",
    "\n",
    "# Create a separate W&B run for assistant-only stage\n",
    "try:\n",
    "    import wandb as _wandb\n",
    "    _wandb.finish()\n",
    "    _wandb.init(\n",
    "        project=\"qwen2vl-nutrition-detection\",\n",
    "        name=\"qwen2vl-7b-assistant-only\",\n",
    "        tags=[\"stage:assistant-only\", \"collator:v3\", \"qlora\", \"lora\"],\n",
    "    )\n",
    "except Exception as _e:\n",
    "    print(f\"[warn] W&B re-init failed: {_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1636bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collate_fn_fixed_3\n",
    "print(\"\\nüì¶ Initializing collate_fn_fixed_3...\")\n",
    "\n",
    "# Create instance of collate_fn_fixed_3\n",
    "# Note: processor and model should be available from earlier training\n",
    "collate_fn_v3 = collate_fn_fixed_3(processor, model)\n",
    "collate_fn_v3.debug = True  # Enable debug output for first few samples\n",
    "collate_fn_v3.max_debug = 5  # Show first 5 samples for verification\n",
    "\n",
    "print(\"‚úÖ collate_fn_fixed_3 initialized\")\n",
    "print(\"   - Will train ONLY on assistant responses\")\n",
    "print(\"   - Debug output enabled for first 5 samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbcfd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the collator with a sample batch\n",
    "print(\"\\nüß™ Testing collate_fn_fixed_3 with a sample batch...\")\n",
    "test_batch = [train_dataset_formatted[0], train_dataset_formatted[1]]\n",
    "test_output = collate_fn_v3(test_batch)\n",
    "\n",
    "# Analyze what we're training on\n",
    "labels = test_output['labels']\n",
    "non_masked = (labels != -100).sum().item()\n",
    "total = labels.numel()\n",
    "print(f\"\\nüìä Training statistics:\")\n",
    "print(f\"   - Total tokens in batch: {total}\")\n",
    "print(f\"   - Tokens being trained on: {non_masked} ({100*non_masked/total:.1f}%)\")\n",
    "print(f\"   - Tokens masked (ignored): {total - non_masked} ({100*(total-non_masked)/total:.1f}%)\")\n",
    "\n",
    "# Additional token distribution audits for assistant-only collator\n",
    "print(\"\\nüìä Token distribution (assistant-only) on train split...\")\n",
    "analyze_token_distribution(collate_fn_v3, train_dataset_formatted, num_samples=3)\n",
    "print(\"\\nüìä Token distribution (assistant-only) on validation split...\")\n",
    "analyze_token_distribution(collate_fn_v3, eval_dataset_formatted, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edcdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the second SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"\\nüéØ Creating second SFTTrainer with assistant-only training...\")\n",
    "\n",
    "trainer_v3 = SFTTrainer(\n",
    "    model=model,  # Use the same model (already has LoRA adapters from first training)\n",
    "    args=training_args_v3,\n",
    "    train_dataset=train_dataset_formatted,\n",
    "    eval_dataset=eval_dataset_formatted,\n",
    "    data_collator=collate_fn_v3,  # Using collate_fn_fixed_3 for assistant-only\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Second SFTTrainer created successfully\")\n",
    "print(f\"   Total training samples: {len(train_dataset_formatted)}\")\n",
    "print(f\"   Total evaluation samples: {len(eval_dataset_formatted)}\")\n",
    "print(f\"   Training will continue from the first trainer's checkpoint\")\n",
    "\n",
    "# Calculate training steps\n",
    "steps_per_epoch = len(train_dataset_formatted) // (training_args_v3.per_device_train_batch_size * training_args_v3.gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * training_args_v3.num_train_epochs\n",
    "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   Total training steps: {total_steps}\")\n",
    "\n",
    "# Attach the same diagnostics callbacks with explicit trainer binding\n",
    "_iou_cb_v3 = IoUEvalCallback(processor=processor, eval_dataset=eval_dataset, num_samples=24, prefix=\"eval\")\n",
    "_iou_cb_v3.trainer = trainer_v3\n",
    "trainer_v3.add_callback(_iou_cb_v3)\n",
    "\n",
    "_grad_cb_v3 = GradNormCallback()\n",
    "_grad_cb_v3.trainer = trainer_v3\n",
    "trainer_v3.add_callback(_grad_cb_v3)\n",
    "trainer_v3.add_callback(ConsoleLogCallback())  # prints selected metrics to console for notebook readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the second training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ LAUNCHING SECOND TRAINING (ASSISTANT-ONLY)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Training will focus only on learning the bbox coordinates...\")\n",
    "print(\"Watch for lower loss values compared to the first training run!\")\n",
    "print(\"\")\n",
    "\n",
    "# Train the model\n",
    "trainer_v3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"\\nüíæ Saving the assistant-only fine-tuned model...\")\n",
    "trainer_v3.save_model(training_args_v3.output_dir)\n",
    "processor.save_pretrained(training_args_v3.output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {training_args_v3.output_dir}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou now have two trained models:\")\n",
    "print(f\"1. Original (all tokens): /ssd1/zhuoyuan/vlm_outputs/qwen2vl-nutrition-detection-lora\")\n",
    "print(f\"2. Assistant-only: {training_args_v3.output_dir}\")\n",
    "print(\"\\nCompare their performance to see which approach works better!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
