{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKadZFQ2IdJb"
   },
   "source": [
    "# Fine-Tuning Qwen2-VL-7B for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdc7yvCQ7JGf"
   },
   "source": [
    "## üåü WHAT?\n",
    "\n",
    "In this notebook, you will learn how to fine-tune [Qwen2-VL-7B](https://qwenlm.github.io/blog/qwen2-vl/) for for detecting nutrition tables from product images using Hugging Face.\n",
    "\n",
    "![Image of nutrition table detection](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*tcy5oCWmHT3jeVN7Lw-FpQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ84pmL57JGf"
   },
   "source": [
    "üí° You can execute this Jupyter Notebook on a remote machine and then access and interact with it in your local web browser, leveraging the remote machine's computational resources.\n",
    "- On remote: jupyter notebook --no-browser --port=8080\n",
    "- On local: ssh -L 8080:localhost:8080 ntajbakhsh@workstation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JATmSI8mcyW2"
   },
   "source": [
    "üö® **WARNING**: Please note that QWEN2-VL-7B is a relatively large model, requiring significant computational resources for fine-tuning. I recommend using either 2x A6000 or 1x A100 GPUs to ensure sufficient memory and processing power. While I haven't experimented with other GPUs, you're welcome to try alternative options. However, please be aware that other GPUs may not have enough memory to accommodate the model and optimizer states during training.\n",
    "\n",
    "üö® **WARNING**: Training transformers can be significantly more memory-efficient with Flash Attention (FA) compared to traditional attention mechanisms. However, FA support is currently limited to Nvidia's Ampere series of GPUs (A100, A6000, etc.) or better. If you're using an older GPU generation, please note that you'll need to disable FA to avoid error messages. Keep in mind that disabling FA may require using additional GPUs to compensate for the reduced memory efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSHmDKNFoqjC"
   },
   "source": [
    "# 1. Install Dependencies\n",
    "\n",
    "Let‚Äôs start by installing the essential libraries we‚Äôll need for fine-tuning! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCMhPmFdIGSb"
   },
   "outputs": [],
   "source": [
    "!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
    "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1\n",
    "!pip install  matplotlib IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4pAvoQaOJ1M"
   },
   "source": [
    "We‚Äôll also need to install an earlier version of *PyTorch*, as the latest version has an issue that currently prevents this notebook from running correctly. You can learn more about the issue [here](https://github.com/pytorch/pytorch/issues/138340) and consider updating to the latest version once it‚Äôs resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8iRteA4oXVj"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upZjD4bH7JGh"
   },
   "source": [
    "# 2. HF Login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0-2Lso6wkIh"
   },
   "source": [
    "Log in to Hugging Face to upload your fine-tuned model! üóùÔ∏è\n",
    "\n",
    "You‚Äôll need to authenticate with your Hugging Face account to save and share your model directly from this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xcL4-bwGIoaR"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HF_TOKEN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m login(token\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHF_TOKEN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;66;03m# export your HF_TOKEN first. You can add this to your ~/.bashrc.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm_Qwen2VL_object_detection/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HF_TOKEN'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.environ['HF_TOKEN']) # export your HF_TOKEN first. You can add this to your ~/.bashrc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMZD2Wv57JGi"
   },
   "source": [
    "## Optional Settings for an Improved Jupyter Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2BTm9Ug7JGi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9QXwbJ7ovM5"
   },
   "source": [
    "# 2. Load and Understand Dataset üìÅ\n",
    "\n",
    "In this section, you should load the [openfoodfacts/nutrition-table-detection](https://huggingface.co/datasets/openfoodfacts/nutrition-table-detection) dataset. This dataset contains product images, the extracted bar codes, and bounding boxes for the nutrition tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKIHSAHX7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: load the dataset into training and evaluation sets\n",
    "from datasets import load_dataset\n",
    "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
    "\n",
    "# Load the dataset with train and validation splits\n",
    "ds = load_dataset(dataset_id)\n",
    "train_dataset = ds['train']\n",
    "eval_dataset = ds['validation']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Dataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J4Y1d-l7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: inspect the content of a training example\n",
    "# Let's look at the first training example\n",
    "example = train_dataset[0]\n",
    "print(\"Example keys:\", example.keys())\n",
    "print(\"\\nBarcode:\", example['barcode'])\n",
    "print(\"Image size:\", example['image'].size if hasattr(example['image'], 'size') else 'N/A')\n",
    "print(\"Number of bounding boxes:\", len(example['objects']['bbox']))\n",
    "print(\"Bounding box:\", example['objects']['bbox'][0])\n",
    "print(\"Category:\", example['objects']['category'][0])\n",
    "\n",
    "# Display the image with bounding box\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = example['image']\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Get the bounding box coordinates (normalized to 0-1)\n",
    "bbox = example['objects']['bbox'][0]\n",
    "width, height = img.size\n",
    "\n",
    "# Convert normalized coordinates to pixel coordinates\n",
    "x_min = bbox[0] * width\n",
    "y_min = bbox[1] * height\n",
    "x_max = bbox[2] * width\n",
    "y_max = bbox[3] * height\n",
    "\n",
    "# Draw the bounding box\n",
    "draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Category: {example['objects']['category'][0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQvNOB-57JGi"
   },
   "outputs": [],
   "source": [
    "# Q: why the bbox coordinates are between 0 and 1? can you overlay the bbox on the image for one example?\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The bbox coordinates are normalized to [0, 1] to make them resolution-independent.\n",
    "# This is a common practice in object detection to handle images of different sizes.\n",
    "\n",
    "# Let's visualize multiple examples with their bounding boxes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(6):\n",
    "    example = train_dataset[idx]\n",
    "    img = example['image'].copy()  # Make a copy to avoid modifying original\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width, height = img.size\n",
    "    \n",
    "    # Draw all bounding boxes for this image\n",
    "    for bbox, category in zip(example['objects']['bbox'], example['objects']['category']):\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        x_min = bbox[0] * width\n",
    "        y_min = bbox[1] * height\n",
    "        x_max = bbox[2] * width\n",
    "        y_max = bbox[3] * height\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "        draw.text((x_min, y_min-10), category, fill='red')\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Image {idx} - Size: {width}x{height}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlr2lSwM7JGi"
   },
   "outputs": [],
   "source": [
    "# get the histogram of the image sizes\n",
    "# get the histogram of the #bounding boxes per image - important for finetuning the model\n",
    "import numpy as np\n",
    "\n",
    "# Collect image sizes and bbox counts\n",
    "widths = []\n",
    "heights = []\n",
    "bbox_counts = []\n",
    "\n",
    "for example in train_dataset:\n",
    "    img = example['image']\n",
    "    widths.append(img.size[0])\n",
    "    heights.append(img.size[1])\n",
    "    bbox_counts.append(len(example['objects']['bbox']))\n",
    "\n",
    "# Create histograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Image widths histogram\n",
    "axes[0].hist(widths, bins=30, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Image Widths')\n",
    "axes[0].set_xlabel('Width (pixels)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].axvline(np.mean(widths), color='red', linestyle='--', label=f'Mean: {np.mean(widths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Image heights histogram\n",
    "axes[1].hist(heights, bins=30, edgecolor='black')\n",
    "axes[1].set_title('Distribution of Image Heights')\n",
    "axes[1].set_xlabel('Height (pixels)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].axvline(np.mean(heights), color='red', linestyle='--', label=f'Mean: {np.mean(heights):.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Bounding boxes per image histogram\n",
    "axes[2].hist(bbox_counts, bins=range(1, max(bbox_counts)+2), edgecolor='black', align='left')\n",
    "axes[2].set_title('Distribution of Bounding Boxes per Image')\n",
    "axes[2].set_xlabel('Number of Bounding Boxes')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_xticks(range(1, max(bbox_counts)+1))\n",
    "axes[2].axvline(np.mean(bbox_counts), color='red', linestyle='--', label=f'Mean: {np.mean(bbox_counts):.1f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size stats:\")\n",
    "print(f\"  Width: min={min(widths)}, max={max(widths)}, mean={np.mean(widths):.0f}, std={np.std(widths):.0f}\")\n",
    "print(f\"  Height: min={min(heights)}, max={max(heights)}, mean={np.mean(heights):.0f}, std={np.std(heights):.0f}\")\n",
    "print(f\"\\nBounding boxes per image:\")\n",
    "print(f\"  Min: {min(bbox_counts)}, Max: {max(bbox_counts)}, Mean: {np.mean(bbox_counts):.2f}\")\n",
    "print(f\"  Most common: {max(set(bbox_counts), key=bbox_counts.count)} boxes (appears {bbox_counts.count(max(set(bbox_counts), key=bbox_counts.count))} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nFWDveC7JGi"
   },
   "source": [
    "# Understand Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1h_1QI37JGi"
   },
   "source": [
    "You should read the Qwen2-VL paper to familiarize yourself with the following:\n",
    "\n",
    "- **Model Architecture**\n",
    "- **Data Processing**\n",
    "- **Chat Template**\n",
    "\n",
    "Next, review the model card and write an inference script for the model using Hugging Face.\n",
    "\n",
    "Hugging Face provides an abstract API that simplifies usage by hiding many implementation details. While this is convenient, it may leave you with a superficial understanding of the model. To deepen your knowledge, explore the Qwen2-VL code and focus on these key aspects:\n",
    "\n",
    "- **Understand the input format required by the model:**\n",
    "  - Can you create an example input where the user provides two images and one video?\n",
    "\n",
    "- **Explore `apply_chat_template`:**\n",
    "  - Run this function on the example above and analyze the output. What does it do?\n",
    "\n",
    "- **Understand `process_vision_info`:**\n",
    "  - Review the code and determine what this function returns.\n",
    "\n",
    "- **Examine `processor()`:**\n",
    "  - Investigate its functionalities, such as:\n",
    "    - Patch-ification\n",
    "    - Replicating pad tokens\n",
    "    - Text tokenization\n",
    "\n",
    "- **[Optional] Analyze `model.generate()`'s [forward pass](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L999):**\n",
    "  - Understand its operations, including:\n",
    "    - Embedding image patches through `PatchEmbed`‚Äôs [forward pass](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L272).\n",
    "    - Sending patch embeddings to a transformer for feature extraction:\n",
    "      - Grasp the concept of 2D RoPE (Rotary Position Embedding).\n",
    "      - Pay attention to [forbidden attention](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L339) when more than one image is provided.\n",
    "    - Merging the resulting feature embeddings via [PatchMerger](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L298).\n",
    "    - Processing image and text embeddings using the LLM:\n",
    "      - Pay special attention to multimodal RoPE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNIW_O0z7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: write an inference function for qwen2-vl\n",
    "import torch\n",
    "import os\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def run_qwen2vl_inference(image_path_or_pil, prompt, model_id=\"Qwen/Qwen2-VL-7B-Instruct\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Run inference with Qwen2-VL model.\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_pil: Either a file path to an image or a PIL Image object\n",
    "        prompt: Text prompt for the model\n",
    "        model_id: Model identifier from HuggingFace\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        Generated text response from the model\n",
    "    \"\"\"\n",
    "    # Load model and processor\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device\n",
    "    )\n",
    "    processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    # Handle image input - can be path or PIL Image\n",
    "    if isinstance(image_path_or_pil, str):\n",
    "        from PIL import Image\n",
    "        image = Image.open(image_path_or_pil)\n",
    "    else:\n",
    "        image = image_path_or_pil\n",
    "    \n",
    "    # Create the conversation format expected by Qwen2-VL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template to format the conversation\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process vision information (handles image resizing, patching, etc.)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    # Prepare inputs for the model\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,  # Deterministic for object detection\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens (excluding the input)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Example usage (will be tested in next cell)\n",
    "print(\"Inference function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlaMACEb7JGj"
   },
   "source": [
    "Test your inference script using this [image](https://t4.ftcdn.net/jpg/01/57/82/05/360_F_157820583_agejYX5XeczPZuWRSCDF2YYeCGwJqUdG.jpg) with the prompt: ‚ÄúDetect the bounding box of the red car.‚Äù The model should correctly identify and locate the car in the image, confirming the script‚Äôs correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUkdCbsB7JGj"
   },
   "outputs": [],
   "source": [
    "# TASK: test the inference function\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the test image (red car)\n",
    "test_image_url = \"https://t4.ftcdn.net/jpg/01/57/82/05/360_F_157820583_agejYX5XeczPZuWRSCDF2YYeCGwJqUdG.jpg\"\n",
    "response = requests.get(test_image_url)\n",
    "test_image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the test image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(test_image)\n",
    "plt.title(\"Test Image: Red Car\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Test the inference function\n",
    "print(\"Testing Qwen2-VL inference...\")\n",
    "print(\"Prompt: 'Detect the bounding box of the red car.'\")\n",
    "print(\"\\nModel Response:\")\n",
    "\n",
    "# Uncomment below when you have GPU available\n",
    "# result = run_qwen2vl_inference(\n",
    "#     test_image, \n",
    "#     \"Detect the bounding box of the red car.\"\n",
    "# )\n",
    "# print(result)\n",
    "\n",
    "# For now, let's show what the expected output format should look like\n",
    "print(\"Expected format: <|object_ref_start|>the red car<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\")\n",
    "print(\"Where coordinates are normalized to image dimensions (1000x1000 for Qwen2-VL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oosQDLcQ7JGj"
   },
   "source": [
    "# Try Qwen2VL without finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy2oO_XE7JGj"
   },
   "source": [
    "It‚Äôs a good idea to first assess the model‚Äôs current capability in detecting the nutrition table without any fine-tuning. This allows for a clear comparison between the model‚Äôs performance before and after fine-tuning. To do this, you need to write a function that extracts the bounding box by parsing the model output and then visualize the bounding box on the input image.\n",
    "\n",
    "Notice that the model‚Äôs response will likely follow a different format than wat we saw above for the dog image. Why? One possible explanation is that the nutrition table does not belong to a previously object class seen during the model‚Äôs training phase. Additionally, the bounding box coordinates returned by the model are likely inaccurate. This should highlight the necessity of fine-tuning to improve the model‚Äôs performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5JKCnIA7JGj"
   },
   "outputs": [],
   "source": [
    "# TASK: write a function to parse model output to extract bounding box coordinates\n",
    "import re\n",
    "\n",
    "def parse_qwen_bbox_output(model_output):\n",
    "    \"\"\"\n",
    "    Parse Qwen2-VL model output to extract bounding box coordinates.\n",
    "    \n",
    "    Args:\n",
    "        model_output: String output from the model\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'object' name and 'bbox' coordinates, or None if parsing fails\n",
    "    \"\"\"\n",
    "    # Pattern for Qwen2-VL detection format:\n",
    "    # <|object_ref_start|>object name<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\n",
    "    pattern = r'<\\|object_ref_start\\|>(.+?)<\\|object_ref_end\\|><\\|box_start\\|>\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)<\\|box_end\\|>'\n",
    "    \n",
    "    matches = re.findall(pattern, model_output)\n",
    "    \n",
    "    if not matches:\n",
    "        # Try alternative format without special tokens (for non-finetuned model)\n",
    "        # Look for patterns like \"bounding box: (x1,y1,x2,y2)\" or \"[x1,y1,x2,y2]\"\n",
    "        alt_pattern = r'[\\[\\(]?\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*[\\]\\)]?'\n",
    "        alt_matches = re.findall(alt_pattern, model_output)\n",
    "        \n",
    "        if alt_matches:\n",
    "            # Take the first match\n",
    "            coords = alt_matches[0]\n",
    "            return {\n",
    "                'object': 'detected object',\n",
    "                'bbox': [int(coords[0]), int(coords[1]), int(coords[2]), int(coords[3])]\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    # Parse all detected objects\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        object_name = match[0]\n",
    "        x1, y1, x2, y2 = int(match[1]), int(match[2]), int(match[3]), int(match[4])\n",
    "        results.append({\n",
    "            'object': object_name,\n",
    "            'bbox': [x1, y1, x2, y2]\n",
    "        })\n",
    "    \n",
    "    return results[0] if len(results) == 1 else results\n",
    "\n",
    "\n",
    "# TASK: write a function to visualize the bounding boxes on the input image\n",
    "def visualize_bbox_on_image(image, bbox_data, normalize_coords=True):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        bbox_data: Dict or list of dicts with 'object' and 'bbox' keys\n",
    "        normalize_coords: If True, bbox coords are in Qwen's 1000x1000 space\n",
    "        \n",
    "    Returns:\n",
    "        PIL Image with bounding boxes drawn\n",
    "    \"\"\"\n",
    "    from PIL import ImageDraw, ImageFont\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    img_with_bbox = image.copy()\n",
    "    draw = ImageDraw.Draw(img_with_bbox)\n",
    "    width, height = img_with_bbox.size\n",
    "    \n",
    "    # Handle single or multiple bboxes\n",
    "    if isinstance(bbox_data, dict):\n",
    "        bbox_data = [bbox_data]\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "    \n",
    "    for idx, data in enumerate(bbox_data):\n",
    "        if data is None:\n",
    "            continue\n",
    "            \n",
    "        color = colors[idx % len(colors)]\n",
    "        bbox = data['bbox']\n",
    "        object_name = data.get('object', 'unknown')\n",
    "        \n",
    "        # Convert coordinates if needed\n",
    "        if normalize_coords:\n",
    "            # Qwen uses 1000x1000 normalized space\n",
    "            x1 = int(bbox[0] * width / 1000)\n",
    "            y1 = int(bbox[1] * height / 1000)\n",
    "            x2 = int(bbox[2] * width / 1000)\n",
    "            y2 = int(bbox[3] * height / 1000)\n",
    "        else:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "        \n",
    "        # Draw rectangle\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # Add label\n",
    "        try:\n",
    "            # Try to use a better font if available\n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 16)\n",
    "        except:\n",
    "            font = None\n",
    "        \n",
    "        label = f\"{object_name}\"\n",
    "        if font:\n",
    "            bbox_label = draw.textbbox((x1, y1-25), label, font=font)\n",
    "            draw.rectangle(bbox_label, fill=color)\n",
    "            draw.text((x1, y1-25), label, fill='white', font=font)\n",
    "        else:\n",
    "            draw.text((x1, y1-20), label, fill=color)\n",
    "    \n",
    "    return img_with_bbox\n",
    "\n",
    "# Test the parsing function\n",
    "test_output = \"<|object_ref_start|>the red car<|object_ref_end|><|box_start|>(450,380),(650,520)<|box_end|>\"\n",
    "parsed = parse_qwen_bbox_output(test_output)\n",
    "print(\"Test parsing:\")\n",
    "print(f\"Input: {test_output}\")\n",
    "print(f\"Parsed: {parsed}\")\n",
    "\n",
    "# Alternative format test\n",
    "alt_output = \"The bounding box for the object is [100, 200, 300, 400]\"\n",
    "parsed_alt = parse_qwen_bbox_output(alt_output)\n",
    "print(f\"\\nAlternative format parsing:\")\n",
    "print(f\"Input: {alt_output}\")\n",
    "print(f\"Parsed: {parsed_alt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw_RG5kw7JGj"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGOrsuQo7JGj"
   },
   "source": [
    "The dataset requires conversion to be compatible with the Hugging Face (HF) library. Specifically, each sample must be reformatted into the OpenAI conversation format, comprising:\n",
    "\n",
    "- Roles: system, user, and assistant\n",
    "- User input: Provide an image and ask, \"Detect the bounding box of the nutrition table.\"\n",
    "- Assistant response: Format compatible with Qwen2-VL's detection question responses\n",
    "    * See pages 7 and 43 of this [paper](https://arxiv.org/pdf/2409.12191) and  [Model Card](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct#more-usage-tips) for tips\n",
    "    * Ensure inclusion of class name and bounding box coordinates using the proper special tokens.\n",
    "    * Check the expected range of bb coordinates\n",
    "    * Pay attention to the order of x,y coordinates as expected by Qwen\n",
    "\n",
    "Here is an example system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBvKAlXhI46X",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from product images.\n",
    "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG8NuzqjjbgI"
   },
   "outputs": [],
   "source": [
    "# Task: write a function to map each sample to a list of 3 dicts (one for each role)\n",
    "\n",
    "def convert_to_conversation_format(example):\n",
    "    \"\"\"\n",
    "    Convert a dataset example to Qwen2-VL conversation format.\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset sample with 'image', 'objects' containing bbox and category\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'messages' key containing list of message dicts with roles\n",
    "    \"\"\"\n",
    "    # Extract nutrition table bounding boxes\n",
    "    bboxes = example['objects']['bbox']\n",
    "    categories = example['objects']['category']\n",
    "    \n",
    "    # Format the assistant response with Qwen2-VL special tokens\n",
    "    # Convert normalized [0,1] bbox to Qwen's [0,1000] format\n",
    "    assistant_responses = []\n",
    "    for bbox, category in zip(bboxes, categories):\n",
    "        # Convert from [x_min, y_min, x_max, y_max] normalized to Qwen format\n",
    "        x1 = int(bbox[0] * 1000)\n",
    "        y1 = int(bbox[1] * 1000)\n",
    "        x2 = int(bbox[2] * 1000)\n",
    "        y2 = int(bbox[3] * 1000)\n",
    "        \n",
    "        # Format: <|object_ref_start|>object<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\n",
    "        response = f\"<|object_ref_start|>{category}<|object_ref_end|><|box_start|>({x1},{y1}),({x2},{y2})<|box_end|>\"\n",
    "        assistant_responses.append(response)\n",
    "    \n",
    "    # Combine multiple detections if present\n",
    "    assistant_text = \" \".join(assistant_responses)\n",
    "    \n",
    "    # Create conversation format\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_message\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": example['image']\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Detect the bounding box of the nutrition table.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": assistant_text\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\": conversation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1edUqNGWTtjA"
   },
   "source": [
    "Now, let‚Äôs format the data using the chatbot structure. This will allow us to set up the interactions appropriately for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSHNqk0dkxii",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Task: apply the function above to all samples in the training and eval datasets\n",
    "train_dataset_formatted = train_dataset.map(convert_to_conversation_format)\n",
    "eval_dataset_formatted = eval_dataset.map(convert_to_conversation_format)\n",
    "\n",
    "print(f\"Formatted training samples: {len(train_dataset_formatted)}\")\n",
    "print(f\"Formatted evaluation samples: {len(eval_dataset_formatted)}\")\n",
    "\n",
    "# Check a sample\n",
    "sample_formatted = train_dataset_formatted[0]\n",
    "print(\"\\nSample formatted message structure:\")\n",
    "for msg in sample_formatted['messages']:\n",
    "    print(f\"  Role: {msg['role']}\")\n",
    "    for content in msg['content']:\n",
    "        if content['type'] == 'text':\n",
    "            print(f\"    Text: {content['text'][:100]}...\" if len(content['text']) > 100 else f\"    Text: {content['text']}\")\n",
    "        elif content['type'] == 'image':\n",
    "            print(f\"    Image: {content['image'].size if hasattr(content['image'], 'size') else 'PIL Image'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3b76rawti6"
   },
   "source": [
    "# Model finetuning\n",
    "**Remove Model and Clean GPU**\n",
    "\n",
    "Before we proceed with training the model in the next section, let's clear the current variables and clean the GPU to free up resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxkXZuUkvy8j"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if 'inputs' in globals(): del globals()['inputs']\n",
    "    if 'model' in globals(): del globals()['model']\n",
    "    if 'processor' in globals(): del globals()['processor']\n",
    "    if 'trainer' in globals(): del globals()['trainer']\n",
    "    if 'peft_model' in globals(): del globals()['peft_model']\n",
    "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw9nm6ZP7JGk"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIrR9gP2z90z"
   },
   "source": [
    "## Load the Model for Training with NF4 weights  ‚öôÔ∏è\n",
    "\n",
    "Next, you need to load the quantized model using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index). If you want to learn more about quantization, check out [this blog post](https://huggingface.co/blog/merve/quantization) or [this one](https://www.maartengrootendorst.com/blog/quantization/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm_bJRrXsESg",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TASK: load the NF4 model and processor\n",
    "from transformers import BitsAndBytesConfig, Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",  # Use Flash Attention 2 for efficiency\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model_id}\")\n",
    "print(f\"Model device map: {model.hf_device_map}\")\n",
    "print(f\"Model memory footprint: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65wfO29isQlX"
   },
   "source": [
    "## Set Up QLoRA and SFTConfig üöÄ\n",
    "\n",
    "Next, you need to configure [QLoRA](https://github.com/artidoro/qlora) for your training setup. QLoRA enables efficient fine-tuning of large language models while significantly reducing the memory footprint compared to traditional methods. Unlike standard LoRA, which reduces memory usage by applying a low-rank approximation, QLoRA takes it a step further by quantizing the model weights. This leads to even lower memory requirements and improved training efficiency, making it an excellent choice for optimizing our model's performance without sacrificing quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5WRzUJe-P_-"
   },
   "source": [
    "üí° NOTE:\n",
    "\n",
    "Preparing a model for QLoRA training typically involves three key steps:\n",
    "\n",
    "- Load the base model in 4-bit (using BitsAndBytesConfig).\n",
    "\n",
    "- Run prepare_model_for_kbit_training(). üö® Understand what this function does.\n",
    "\n",
    "- Apply LoRA adapters to the target modules.\n",
    "\n",
    "You can perform these steps manually, or let SFTTrainer handle steps 2 & 3 for you:\n",
    "\n",
    "- Simply load the model in 4-bit,\n",
    "\n",
    "- Pass a peft_config to SFTTrainer, which will automatically run prepare_model_for_kbit_training() (for unsharded QLoRA) and attach LoRA adapters. See lines 610 and 625 in [here](https://github.com/huggingface/trl/blob/v0.21.0/trl/trainer/sft_trainer.py)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITmkRHWCKYjf"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# Task: create LoRA config and apply LoRA to the model instrance created above\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # Rank\n",
    "    lora_alpha=128,  # Alpha scaling\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",  # MLP layers\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1t7Hk_f7JGn"
   },
   "source": [
    "Next, you need to create an SFT config for model finetuning. This step is critical for model convergence.\n",
    "You should set the following hyper-parameteres among others:\n",
    " - learning_rate\n",
    " - per_device_train_batch_size\n",
    " - gradient_accumulation_steps for better gradient direction estimation\n",
    " - BF16 and TF32 enablement for memory saving and faster compute\n",
    " - gradient_checkpointing for memory saving\n",
    "   \n",
    "There are other input arguments that you should also set for proper evaluation during model finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbqX1pQUKaSM"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "# TASK: create an SFT config\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # Output and logging\n",
    "    output_dir=\"./qwen2vl-nutrition-detection-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 2 * 8 = 16\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta2=0.999,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Mixed precision and performance\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Enable TF32 on Ampere GPUs\n",
    "    dataloader_num_workers=4,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Other settings\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",  # Enable wandb logging\n",
    "    run_name=\"qwen2vl-nutrition-detection\",\n",
    "    \n",
    "    # Specific for vision models\n",
    "    dataset_text_field=\"\",  # We'll use a custom data collator\n",
    "    max_seq_length=2048,\n",
    "    dataset_kwargs={\n",
    "        \"skip_prepare_dataset\": True  # We handle data preparation ourselves\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Number of epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjQGt-iZVyef"
   },
   "source": [
    "# wandb setup\n",
    "If you have wandb account, you can set it up here.\n",
    "Let‚Äôs connect our notebook to W&B to capture essential information during training.\n",
    "Make sure to have set the logging arguments in the SFT config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckVfXDWsoF4Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "# TASK: set up wand.init\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "wandb.init(\n",
    "    project=\"qwen2vl-nutrition-detection\",\n",
    "    name=\"qwen2vl-7b-nutrition-lora\",\n",
    "    config={\n",
    "        \"model\": model_id,\n",
    "        \"lora_r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation\": training_args.gradient_accumulation_steps,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"dataset\": \"openfoodfacts/nutrition-table-detection\",\n",
    "    },\n",
    "    tags=[\"qwen2-vl\", \"object-detection\", \"nutrition-table\", \"lora\"],\n",
    ")\n",
    "\n",
    "print(\"W&B initialized for experiment tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOUrD9P-y-Kf"
   },
   "source": [
    "## Training the Model üèÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucTUbGURV2_-"
   },
   "source": [
    "You should now create a trainer object by instantiating the SFTTrainer class of HF's TRL. For this, you need to provide the training dataset, model, tokenizer, and more important a collate function.\n",
    "\n",
    "You need a collator function to properly retrieve and batch the data during the training procedure. This function will receive as the input a batch of samples:\n",
    "\n",
    "[{'role': 'system',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format. \\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
    " {'role': 'user',\n",
    "  'content': [{'type': 'image',\n",
    "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>},\n",
    "   {'type': 'text',\n",
    "    'text': 'Detect the bounding box of the nutrition table'}]},\n",
    " {'role': 'assistant',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': '<|object_ref_start|>the nutrition table<|object_ref_end|><|box_start|>(14,57),(991,604)<|box_end|>'}]}]\n",
    "    \n",
    "[{'role': 'system',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format. \\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
    " {'role': 'user',\n",
    "  'content': [{'type': 'image',\n",
    "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408>},\n",
    "   {'type': 'text',\n",
    "    'text': 'Detect the bounding box of the nutrition table'}]},\n",
    " {'role': 'assistant',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': '<|object_ref_start|>the nutrition table<|object_ref_end|><|box_start|>(147,152),(516,588)<|box_end|>'}]}]\n",
    "\n",
    "and then performs ops similar to what we did earlier in the inference script:\n",
    "  - applying chat template on each sample in the batch -> get formatted prompt\n",
    "  - applying process_vision_info on each sample in the batch -> get image pixels\n",
    "  - applying processor on formatted prompt and image pixels -> new batch\n",
    "  - modify the labels of the new batch by replacing labels correspondings to the following items to -100 (why?)\n",
    "    * text pad tokens\n",
    "    * <|vision_start|> <|vision_end|> <|image_pad|>\n",
    "\n",
    "üëâ Check out the TRL official example [scripts]( https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm.py#L87) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAzDovzylQeZ"
   },
   "outputs": [],
   "source": [
    "# TASK: Create a data collator to encode text and image pairs\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Data collator for Qwen2-VL that processes text-image pairs for training.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of samples from the dataset, each containing 'messages'\n",
    "        \n",
    "    Returns:\n",
    "        Dict with input_ids, attention_mask, pixel_values, and labels\n",
    "    \"\"\"\n",
    "    # Extract messages from each sample\n",
    "    messages_list = [sample['messages'] for sample in batch]\n",
    "    \n",
    "    # Apply chat template to each conversation\n",
    "    texts = []\n",
    "    images_list = []\n",
    "    \n",
    "    for messages in messages_list:\n",
    "        # Apply chat template\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "        \n",
    "        # Process vision information\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        images_list.append(image_inputs)\n",
    "    \n",
    "    # Process all texts and images together\n",
    "    batch_inputs = processor(\n",
    "        text=texts,\n",
    "        images=images_list,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels for training (copy input_ids)\n",
    "    labels = batch_inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask padding tokens in labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Mask image tokens in labels to not compute loss on them\n",
    "    # Find and mask special vision tokens\n",
    "    if hasattr(processor, \"image_token_id\"):\n",
    "        labels[labels == processor.image_token_id] = -100\n",
    "    \n",
    "    # Find and mask vision start/end tokens\n",
    "    for special_token in [\"<|vision_start|>\", \"<|vision_end|>\", \"<|image_pad|>\"]:\n",
    "        if special_token in processor.tokenizer.special_tokens_map.values():\n",
    "            token_id = processor.tokenizer.convert_tokens_to_ids(special_token)\n",
    "            labels[labels == token_id] = -100\n",
    "    \n",
    "    # Also mask the system and user parts, keeping only assistant response for loss\n",
    "    # This is done by finding the assistant token positions\n",
    "    for i, text in enumerate(texts):\n",
    "        # Find where assistant response starts\n",
    "        assistant_start = text.find(\"assistant\") \n",
    "        if assistant_start != -1:\n",
    "            # Tokenize just the part before assistant response\n",
    "            prefix_tokens = processor.tokenizer(\n",
    "                text[:assistant_start],\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            prefix_len = prefix_tokens[\"input_ids\"].shape[1]\n",
    "            \n",
    "            # Mask everything before the assistant response\n",
    "            if prefix_len < labels.shape[1]:\n",
    "                labels[i, :prefix_len] = -100\n",
    "    \n",
    "    batch_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "print(\"Data collator function created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skbpTuJlV8qN"
   },
   "source": [
    "Now, we will define the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer), which is a wrapper around the [transformers.Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class and inherits its attributes and methods. This class simplifies the fine-tuning process by properly initializing the [PeftModel](https://huggingface.co/docs/peft/v0.6.0/package_reference/peft_model) when a [PeftConfig](https://huggingface.co/docs/peft/v0.6.0/en/package_reference/config#peft.PeftConfig) object is provided. By using `SFTTrainer`, we can efficiently manage the training workflow and ensure a smooth fine-tuning experience for our Vision Language Model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_jk-U7ULYtA"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "# TASK: Create the SFT trainer and launch training\n",
    "\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_formatted,\n",
    "    eval_dataset=eval_dataset_formatted,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer created successfully\")\n",
    "print(f\"Total training samples: {len(train_dataset_formatted)}\")\n",
    "print(f\"Total evaluation samples: {len(eval_dataset_formatted)}\")\n",
    "print(f\"Number of training steps: {len(train_dataset_formatted) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "\n",
    "# Launch training\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "print(\"\\nSaving the fine-tuned model...\")\n",
    "trainer.save_model(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "print(f\"Model saved to {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yx_sGW42dN3"
   },
   "source": [
    "# 5. Testing the Fine-Tuned Model üîç\n",
    "\n",
    "Now that we've successfully fine-tuned our Vision Language Model (VLM), it's time to evaluate its performance! In this section, we will test the model using examples from the ChartQA dataset to see how well it answers questions based on chart images. Let's dive in and explore the results! üöÄ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0KEPu6qYKqn"
   },
   "source": [
    "Let's clean up the GPU memory to ensure optimal performance üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ttx6EK8Uy8t0"
   },
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwCTPHsfujn2"
   },
   "source": [
    "We will reload the base model using the same pipeline as before, but this we will load the LoRA adpaters into the model too. LoRA adapters should be selected from the saved directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFqTNUud2lA7"
   },
   "outputs": [],
   "source": [
    "# TASK:  Load model, processor, and adapter weights\n",
    "from peft import PeftModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "import torch\n",
    "\n",
    "# Path to saved LoRA adapters\n",
    "adapter_path = \"./qwen2vl-nutrition-detection-lora\"\n",
    "\n",
    "# Load the base model\n",
    "base_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "model_finetuned = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load the LoRA adapters\n",
    "model_finetuned = PeftModel.from_pretrained(\n",
    "    model_finetuned,\n",
    "    adapter_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor_finetuned = Qwen2VLProcessor.from_pretrained(adapter_path)\n",
    "\n",
    "print(f\"Loaded fine-tuned model from {adapter_path}\")\n",
    "print(f\"Model has {sum(p.numel() for p in model_finetuned.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqryChyLWRmR"
   },
   "source": [
    "Test the fine-tuned model on the example above, where the model previously struggled to accurately locate the nutrition table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LH6fWLid7JGp"
   },
   "outputs": [],
   "source": [
    "# TASK: test on the #20 training example\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# Get example #20 from training set\n",
    "example_idx = 20\n",
    "example = train_dataset[example_idx]\n",
    "image = example['image']\n",
    "ground_truth_bbox = example['objects']['bbox'][0]\n",
    "ground_truth_category = example['objects']['category'][0]\n",
    "\n",
    "# Create messages for inference\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Detect the bounding box of the nutrition table.\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = processor_finetuned.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Process vision information\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor_finetuned(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model_finetuned.device)\n",
    "\n",
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_finetuned.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "# Decode the output\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor_finetuned.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(f\"Example #{example_idx}\")\n",
    "print(f\"Ground truth category: {ground_truth_category}\")\n",
    "print(f\"Ground truth bbox (normalized): {ground_truth_bbox}\")\n",
    "print(f\"\\nModel output: {output_text}\")\n",
    "\n",
    "# Parse the model output\n",
    "parsed_bbox = parse_qwen_bbox_output(output_text)\n",
    "if parsed_bbox:\n",
    "    print(f\"Parsed prediction: {parsed_bbox}\")\n",
    "\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Ground truth\n",
    "img_gt = image.copy()\n",
    "draw_gt = ImageDraw.Draw(img_gt)\n",
    "width, height = img_gt.size\n",
    "x_min = ground_truth_bbox[0] * width\n",
    "y_min = ground_truth_bbox[1] * height\n",
    "x_max = ground_truth_bbox[2] * width\n",
    "y_max = ground_truth_bbox[3] * height\n",
    "draw_gt.rectangle([x_min, y_min, x_max, y_max], outline='green', width=3)\n",
    "\n",
    "axes[0].imshow(img_gt)\n",
    "axes[0].set_title(f\"Ground Truth: {ground_truth_category}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Model prediction\n",
    "if parsed_bbox:\n",
    "    img_pred = visualize_bbox_on_image(image, parsed_bbox, normalize_coords=True)\n",
    "    axes[1].imshow(img_pred)\n",
    "    axes[1].set_title(f\"Model Prediction: {parsed_bbox['object']}\")\n",
    "else:\n",
    "    axes[1].imshow(image)\n",
    "    axes[1].set_title(\"Model Prediction: No detection\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swibyq5AWctZ"
   },
   "source": [
    "Since this sample is drawn from the training set, the model has encountered it during training, which may be seen as a form of cheating. To gain a more comprehensive understanding of the model's performance, you should also evaluate it using the eval dataset. For this, write an evaluation script that measures the IoU metric between the ground truth box and the predicted boundig box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czZSBgnoef1E"
   },
   "outputs": [],
   "source": [
    "# Task: write the eval function. You can use use ops.box_iou\n",
    "from torchvision import ops\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, processor_model, dataset, num_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset using IoU metric.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        processor_model: The processor for the model\n",
    "        dataset: The evaluation dataset\n",
    "        num_samples: Number of samples to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Dict with evaluation metrics\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    successful_detections = 0\n",
    "    total_samples = min(num_samples, len(dataset))\n",
    "    \n",
    "    for idx in tqdm(range(total_samples), desc=\"Evaluating\"):\n",
    "        example = dataset[idx]\n",
    "        image = example['image']\n",
    "        ground_truth_bbox = example['objects']['bbox'][0]  # Take first bbox\n",
    "        \n",
    "        # Create messages for inference\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": \"Detect the bounding box of the nutrition table.\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            # Apply chat template\n",
    "            text = processor_model.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Process vision information\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = processor_model(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Generate prediction\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=128,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = processor_model.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            \n",
    "            # Parse the model output\n",
    "            parsed_bbox = parse_qwen_bbox_output(output_text)\n",
    "            \n",
    "            if parsed_bbox:\n",
    "                successful_detections += 1\n",
    "                \n",
    "                # Convert predicted bbox from Qwen format (1000x1000) to normalized [0,1]\n",
    "                if isinstance(parsed_bbox, list):\n",
    "                    pred_bbox = parsed_bbox[0]['bbox']\n",
    "                else:\n",
    "                    pred_bbox = parsed_bbox['bbox']\n",
    "                \n",
    "                # Normalize predicted bbox from 1000x1000 to 0-1\n",
    "                pred_bbox_norm = [\n",
    "                    pred_bbox[0] / 1000.0,\n",
    "                    pred_bbox[1] / 1000.0,\n",
    "                    pred_bbox[2] / 1000.0,\n",
    "                    pred_bbox[3] / 1000.0\n",
    "                ]\n",
    "                \n",
    "                # Convert to tensor for IoU calculation\n",
    "                gt_tensor = torch.tensor([[ground_truth_bbox[0], ground_truth_bbox[1], \n",
    "                                          ground_truth_bbox[2], ground_truth_bbox[3]]], dtype=torch.float32)\n",
    "                pred_tensor = torch.tensor([[pred_bbox_norm[0], pred_bbox_norm[1],\n",
    "                                           pred_bbox_norm[2], pred_bbox_norm[3]]], dtype=torch.float32)\n",
    "                \n",
    "                # Calculate IoU\n",
    "                iou = ops.box_iou(pred_tensor, gt_tensor).item()\n",
    "                ious.append(iou)\n",
    "            else:\n",
    "                ious.append(0.0)  # No detection counts as 0 IoU\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            ious.append(0.0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"mean_iou\": np.mean(ious) if ious else 0.0,\n",
    "        \"median_iou\": np.median(ious) if ious else 0.0,\n",
    "        \"max_iou\": np.max(ious) if ious else 0.0,\n",
    "        \"min_iou\": np.min(ious) if ious else 0.0,\n",
    "        \"detection_rate\": successful_detections / total_samples,\n",
    "        \"samples_evaluated\": total_samples,\n",
    "        \"iou_threshold_0.5\": sum(1 for iou in ious if iou > 0.5) / total_samples,\n",
    "        \"iou_threshold_0.7\": sum(1 for iou in ious if iou > 0.7) / total_samples,\n",
    "    }\n",
    "    \n",
    "    return metrics, ious\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "print(\"\\nEvaluating fine-tuned model on validation set...\")\n",
    "metrics_finetuned, ious_finetuned = evaluate_model(\n",
    "    model_finetuned, \n",
    "    processor_finetuned, \n",
    "    eval_dataset, \n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuned Model Evaluation Results:\")\n",
    "print(f\"  Mean IoU: {metrics_finetuned['mean_iou']:.4f}\")\n",
    "print(f\"  Median IoU: {metrics_finetuned['median_iou']:.4f}\")\n",
    "print(f\"  Detection Rate: {metrics_finetuned['detection_rate']:.2%}\")\n",
    "print(f\"  IoU > 0.5: {metrics_finetuned['iou_threshold_0.5']:.2%}\")\n",
    "print(f\"  IoU > 0.7: {metrics_finetuned['iou_threshold_0.7']:.2%}\")\n",
    "\n",
    "# Plot IoU distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(ious_finetuned, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('IoU Score')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of IoU Scores - Fine-tuned Model')\n",
    "plt.axvline(metrics_finetuned['mean_iou'], color='red', linestyle='--', label=f\"Mean IoU: {metrics_finetuned['mean_iou']:.3f}\")\n",
    "plt.axvline(0.5, color='green', linestyle='--', alpha=0.5, label='IoU = 0.5')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATuQ6ZS6eirO",
    "outputId": "c3adc0fd-0fdc-4ff4-cc4e-14b4d9039323"
   },
   "source": [
    "Do the same evaluation for the model without finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62439755",
   "metadata": {
    "id": "eval_base_model"
   },
   "outputs": [],
   "source": [
    "# Evaluate base model without fine-tuning\n",
    "print(\"\\nLoading base model for comparison...\")\n",
    "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base_processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "print(\"\\nEvaluating base model (without fine-tuning) on validation set...\")\n",
    "metrics_base, ious_base = evaluate_model(\n",
    "    base_model,\n",
    "    base_processor,\n",
    "    eval_dataset,\n",
    "    num_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nBase Model Evaluation Results (No Fine-tuning):\")\n",
    "print(f\"  Mean IoU: {metrics_base['mean_iou']:.4f}\")\n",
    "print(f\"  Median IoU: {metrics_base['median_iou']:.4f}\")\n",
    "print(f\"  Detection Rate: {metrics_base['detection_rate']:.2%}\")\n",
    "print(f\"  IoU > 0.5: {metrics_base['iou_threshold_0.5']:.2%}\")\n",
    "print(f\"  IoU > 0.7: {metrics_base['iou_threshold_0.7']:.2%}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON: Fine-tuned vs Base Model\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean IoU Improvement: {(metrics_finetuned['mean_iou'] - metrics_base['mean_iou']):.4f} \"\n",
    "      f\"({((metrics_finetuned['mean_iou'] - metrics_base['mean_iou']) / max(metrics_base['mean_iou'], 0.001) * 100):.1f}% improvement)\")\n",
    "print(f\"Detection Rate Improvement: {(metrics_finetuned['detection_rate'] - metrics_base['detection_rate']):.2%}\")\n",
    "print(f\"IoU > 0.5 Improvement: {(metrics_finetuned['iou_threshold_0.5'] - metrics_base['iou_threshold_0.5']):.2%}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# IoU distribution comparison\n",
    "axes[0].hist(ious_base, bins=20, alpha=0.5, label='Base Model', edgecolor='black')\n",
    "axes[0].hist(ious_finetuned, bins=20, alpha=0.5, label='Fine-tuned Model', edgecolor='black')\n",
    "axes[0].set_xlabel('IoU Score')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_title('IoU Distribution Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison bar chart\n",
    "metrics_names = ['Mean IoU', 'Detection Rate', 'IoU > 0.5', 'IoU > 0.7']\n",
    "base_values = [\n",
    "    metrics_base['mean_iou'],\n",
    "    metrics_base['detection_rate'],\n",
    "    metrics_base['iou_threshold_0.5'],\n",
    "    metrics_base['iou_threshold_0.7']\n",
    "]\n",
    "finetuned_values = [\n",
    "    metrics_finetuned['mean_iou'],\n",
    "    metrics_finetuned['detection_rate'],\n",
    "    metrics_finetuned['iou_threshold_0.5'],\n",
    "    metrics_finetuned['iou_threshold_0.7']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, base_values, width, label='Base Model', alpha=0.8)\n",
    "axes[1].bar(x + width/2, finetuned_values, width, label='Fine-tuned Model', alpha=0.8)\n",
    "axes[1].set_xlabel('Metrics')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Model Performance Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics_names, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up base model to free memory\n",
    "del base_model\n",
    "del base_processor\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyhR69T3dfLI"
   },
   "source": [
    "# üßë‚Äçüç≥ [Optional]  The recipe\n",
    "For the best model accuracy, one can first finetune the vision encoder while freezing the LLM. Then, we can use the ckpt above and finetune the model by applying LoRA to vision encoder and QLoRA to LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODzQhQ1R1SAR"
   },
   "source": [
    "# üîÄ Merge LoRA\n",
    "After fine-tuning with LoRA, the adapter weights can be merged back into the base model, effectively eliminating the overhead of LoRA modules during inference. This fusion produces a standalone model suitable for efficient deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbDEhQDyqbzK"
   },
   "source": [
    "# üöÄ Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjtLbVqWqFnE"
   },
   "source": [
    "Try to export your trained model (with merged LoRA weights) to vLLM and then deploy into Nvidia triton!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3SH27Iu193x",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Optional: Merge LoRA weights into base model for deployment\n",
    "print(\"\\nMerging LoRA weights into base model for deployment...\")\n",
    "merged_model = model_finetuned.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_output_dir = \"./qwen2vl-nutrition-detection-merged\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "processor_finetuned.save_pretrained(merged_output_dir)\n",
    "\n",
    "print(f\"Merged model saved to {merged_output_dir}\")\n",
    "print(\"This model can now be loaded without PEFT and used for efficient inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejJwH5xpcY6K"
   },
   "source": [
    "## Bonus\n",
    "\n",
    "For Qwen2-VL, implement a custom collate_fn that restricts loss computation to the answer portion only, explicitly excluding the system prompt and question from the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxFyhptrccdr"
   },
   "outputs": [],
   "source": [
    "# Bonus: Custom collate function with loss only on answer portion\n",
    "def collate_fn_answer_only(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that computes loss only on the assistant's answer,\n",
    "    excluding system prompt and user question from loss computation.\n",
    "    \"\"\"\n",
    "    messages_list = [sample['messages'] for sample in batch]\n",
    "    \n",
    "    texts = []\n",
    "    images_list = []\n",
    "    \n",
    "    for messages in messages_list:\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "        \n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        images_list.append(image_inputs)\n",
    "    \n",
    "    batch_inputs = processor(\n",
    "        text=texts,\n",
    "        images=images_list,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    labels = batch_inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask all tokens except assistant's response\n",
    "    for i, text in enumerate(texts):\n",
    "        # Find the assistant response boundaries\n",
    "        # Look for the pattern that indicates start of assistant's actual answer\n",
    "        assistant_token = \"<|im_start|>assistant\"\n",
    "        assistant_end_token = \"<|im_end|>\"\n",
    "        \n",
    "        # Tokenize the full text to get token positions\n",
    "        encoding = processor.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Find assistant response start and end positions\n",
    "        assistant_start_idx = text.find(assistant_token)\n",
    "        if assistant_start_idx != -1:\n",
    "            # Find the actual content start (after the assistant tag)\n",
    "            content_start = assistant_start_idx + len(assistant_token)\n",
    "            \n",
    "            # Find where assistant response ends\n",
    "            assistant_end_idx = text.find(assistant_end_token, content_start)\n",
    "            \n",
    "            # Convert character positions to token positions\n",
    "            offset_mapping = encoding['offset_mapping']\n",
    "            \n",
    "            # Mask everything before assistant's actual response\n",
    "            for j, (start, end) in enumerate(offset_mapping):\n",
    "                if start < content_start:\n",
    "                    labels[i, j] = -100\n",
    "                elif assistant_end_idx != -1 and start >= assistant_end_idx:\n",
    "                    labels[i, j] = -100\n",
    "    \n",
    "    # Also mask padding and special vision tokens\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    batch_inputs[\"labels\"] = labels\n",
    "    return batch_inputs\n",
    "\n",
    "print(\"Custom collate function for answer-only loss computation created successfully\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
