{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKadZFQ2IdJb"
   },
   "source": [
    "# Fine-Tuning Qwen2-VL-7B for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdc7yvCQ7JGf"
   },
   "source": [
    "## üåü WHAT?\n",
    "\n",
    "In this notebook, you will learn how to fine-tune [Qwen2-VL-7B](https://qwenlm.github.io/blog/qwen2-vl/) for for detecting nutrition tables from product images using Hugging Face.\n",
    "\n",
    "![Image of nutrition table detection](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*tcy5oCWmHT3jeVN7Lw-FpQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ84pmL57JGf"
   },
   "source": [
    "üí° You can execute this Jupyter Notebook on a remote machine and then access and interact with it in your local web browser, leveraging the remote machine's computational resources.\n",
    "- On remote: jupyter notebook --no-browser --port=8080\n",
    "- On local: ssh -L 8080:localhost:8080 ntajbakhsh@workstation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JATmSI8mcyW2"
   },
   "source": [
    "üö® **WARNING**: Please note that QWEN2-VL-7B is a relatively large model, requiring significant computational resources for fine-tuning. I recommend using either 2x A6000 or 1x A100 GPUs to ensure sufficient memory and processing power. While I haven't experimented with other GPUs, you're welcome to try alternative options. However, please be aware that other GPUs may not have enough memory to accommodate the model and optimizer states during training.\n",
    "\n",
    "üö® **WARNING**: Training transformers can be significantly more memory-efficient with Flash Attention (FA) compared to traditional attention mechanisms. However, FA support is currently limited to Nvidia's Ampere series of GPUs (A100, A6000, etc.) or better. If you're using an older GPU generation, please note that you'll need to disable FA to avoid error messages. Keep in mind that disabling FA may require using additional GPUs to compensate for the reduced memory efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSHmDKNFoqjC"
   },
   "source": [
    "# 1. Install Dependencies\n",
    "\n",
    "Let‚Äôs start by installing the essential libraries we‚Äôll need for fine-tuning! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCMhPmFdIGSb"
   },
   "outputs": [],
   "source": [
    "!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
    "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1\n",
    "!pip install  matplotlib IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4pAvoQaOJ1M"
   },
   "source": [
    "We‚Äôll also need to install an earlier version of *PyTorch*, as the latest version has an issue that currently prevents this notebook from running correctly. You can learn more about the issue [here](https://github.com/pytorch/pytorch/issues/138340) and consider updating to the latest version once it‚Äôs resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8iRteA4oXVj"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upZjD4bH7JGh"
   },
   "source": [
    "# 2. HF Login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0-2Lso6wkIh"
   },
   "source": [
    "Log in to Hugging Face to upload your fine-tuned model! üóùÔ∏è\n",
    "\n",
    "You‚Äôll need to authenticate with your Hugging Face account to save and share your model directly from this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcL4-bwGIoaR"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.environ['HF_TOKEN']) # export your HF_TOKEN first. You can add this to your ~/.bashrc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMZD2Wv57JGi"
   },
   "source": [
    "## Optional Settings for an Improved Jupyter Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2BTm9Ug7JGi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9QXwbJ7ovM5"
   },
   "source": [
    "# 2. Load and Understand Dataset üìÅ\n",
    "\n",
    "In this section, you should load the [openfoodfacts/nutrition-table-detection](https://huggingface.co/datasets/openfoodfacts/nutrition-table-detection) dataset. This dataset contains product images, the extracted bar codes, and bounding boxes for the nutrition tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKIHSAHX7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: load the dataset into training and evaluation sets\n",
    "from datasets import load_dataset\n",
    "dataset_id = \"openfoodfacts/nutrition-table-detection\"\n",
    "\n",
    "# Load the dataset with train and validation splits\n",
    "ds = load_dataset(dataset_id)\n",
    "train_dataset = ds['train']\n",
    "eval_dataset = ds['validation']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Dataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J4Y1d-l7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: inspect the content of a training example\n",
    "# Let's look at the first training example\n",
    "example = train_dataset[0]\n",
    "print(\"Example keys:\", example.keys())\n",
    "print(\"\\nBarcode:\", example['barcode'])\n",
    "print(\"Image size:\", example['image'].size if hasattr(example['image'], 'size') else 'N/A')\n",
    "print(\"Number of bounding boxes:\", len(example['objects']['bbox']))\n",
    "print(\"Bounding box:\", example['objects']['bbox'][0])\n",
    "print(\"Category:\", example['objects']['category'][0])\n",
    "\n",
    "# Display the image with bounding box\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = example['image']\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Get the bounding box coordinates (normalized to 0-1)\n",
    "bbox = example['objects']['bbox'][0]\n",
    "width, height = img.size\n",
    "\n",
    "# Convert normalized coordinates to pixel coordinates\n",
    "x_min = bbox[0] * width\n",
    "y_min = bbox[1] * height\n",
    "x_max = bbox[2] * width\n",
    "y_max = bbox[3] * height\n",
    "\n",
    "# Draw the bounding box\n",
    "draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Category: {example['objects']['category'][0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQvNOB-57JGi"
   },
   "outputs": [],
   "source": [
    "# Q: why the bbox coordinates are between 0 and 1? can you overlay the bbox on the image for one example?\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The bbox coordinates are normalized to [0, 1] to make them resolution-independent.\n",
    "# This is a common practice in object detection to handle images of different sizes.\n",
    "\n",
    "# Let's visualize multiple examples with their bounding boxes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(6):\n",
    "    example = train_dataset[idx]\n",
    "    img = example['image'].copy()  # Make a copy to avoid modifying original\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width, height = img.size\n",
    "    \n",
    "    # Draw all bounding boxes for this image\n",
    "    for bbox, category in zip(example['objects']['bbox'], example['objects']['category']):\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        x_min = bbox[0] * width\n",
    "        y_min = bbox[1] * height\n",
    "        x_max = bbox[2] * width\n",
    "        y_max = bbox[3] * height\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "        draw.text((x_min, y_min-10), category, fill='red')\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Image {idx} - Size: {width}x{height}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlr2lSwM7JGi"
   },
   "outputs": [],
   "source": [
    "# get the histogram of the image sizes\n",
    "# get the histogram of the #bounding boxes per image - important for finetuning the model\n",
    "import numpy as np\n",
    "\n",
    "# Collect image sizes and bbox counts\n",
    "widths = []\n",
    "heights = []\n",
    "bbox_counts = []\n",
    "\n",
    "for example in train_dataset:\n",
    "    img = example['image']\n",
    "    widths.append(img.size[0])\n",
    "    heights.append(img.size[1])\n",
    "    bbox_counts.append(len(example['objects']['bbox']))\n",
    "\n",
    "# Create histograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Image widths histogram\n",
    "axes[0].hist(widths, bins=30, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Image Widths')\n",
    "axes[0].set_xlabel('Width (pixels)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].axvline(np.mean(widths), color='red', linestyle='--', label=f'Mean: {np.mean(widths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Image heights histogram\n",
    "axes[1].hist(heights, bins=30, edgecolor='black')\n",
    "axes[1].set_title('Distribution of Image Heights')\n",
    "axes[1].set_xlabel('Height (pixels)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].axvline(np.mean(heights), color='red', linestyle='--', label=f'Mean: {np.mean(heights):.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Bounding boxes per image histogram\n",
    "axes[2].hist(bbox_counts, bins=range(1, max(bbox_counts)+2), edgecolor='black', align='left')\n",
    "axes[2].set_title('Distribution of Bounding Boxes per Image')\n",
    "axes[2].set_xlabel('Number of Bounding Boxes')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_xticks(range(1, max(bbox_counts)+1))\n",
    "axes[2].axvline(np.mean(bbox_counts), color='red', linestyle='--', label=f'Mean: {np.mean(bbox_counts):.1f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size stats:\")\n",
    "print(f\"  Width: min={min(widths)}, max={max(widths)}, mean={np.mean(widths):.0f}, std={np.std(widths):.0f}\")\n",
    "print(f\"  Height: min={min(heights)}, max={max(heights)}, mean={np.mean(heights):.0f}, std={np.std(heights):.0f}\")\n",
    "print(f\"\\nBounding boxes per image:\")\n",
    "print(f\"  Min: {min(bbox_counts)}, Max: {max(bbox_counts)}, Mean: {np.mean(bbox_counts):.2f}\")\n",
    "print(f\"  Most common: {max(set(bbox_counts), key=bbox_counts.count)} boxes (appears {bbox_counts.count(max(set(bbox_counts), key=bbox_counts.count))} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nFWDveC7JGi"
   },
   "source": [
    "# Understand Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1h_1QI37JGi"
   },
   "source": [
    "You should read the Qwen2-VL paper to familiarize yourself with the following:\n",
    "\n",
    "- **Model Architecture**\n",
    "- **Data Processing**\n",
    "- **Chat Template**\n",
    "\n",
    "Next, review the model card and write an inference script for the model using Hugging Face.\n",
    "\n",
    "Hugging Face provides an abstract API that simplifies usage by hiding many implementation details. While this is convenient, it may leave you with a superficial understanding of the model. To deepen your knowledge, explore the Qwen2-VL code and focus on these key aspects:\n",
    "\n",
    "- **Understand the input format required by the model:**\n",
    "  - Can you create an example input where the user provides two images and one video?\n",
    "\n",
    "- **Explore `apply_chat_template`:**\n",
    "  - Run this function on the example above and analyze the output. What does it do?\n",
    "\n",
    "- **Understand `process_vision_info`:**\n",
    "  - Review the code and determine what this function returns.\n",
    "\n",
    "- **Examine `processor()`:**\n",
    "  - Investigate its functionalities, such as:\n",
    "    - Patch-ification\n",
    "    - Replicating pad tokens\n",
    "    - Text tokenization\n",
    "\n",
    "- **[Optional] Analyze `model.generate()`'s [forward pass](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L999):**\n",
    "  - Understand its operations, including:\n",
    "    - Embedding image patches through `PatchEmbed`‚Äôs [forward pass](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L272).\n",
    "    - Sending patch embeddings to a transformer for feature extraction:\n",
    "      - Grasp the concept of 2D RoPE (Rotary Position Embedding).\n",
    "      - Pay attention to [forbidden attention](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L339) when more than one image is provided.\n",
    "    - Merging the resulting feature embeddings via [PatchMerger](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L298).\n",
    "    - Processing image and text embeddings using the LLM:\n",
    "      - Pay special attention to multimodal RoPE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNIW_O0z7JGi"
   },
   "outputs": [],
   "source": [
    "# TASK: write an inference function for qwen2-vl\n",
    "import torch\n",
    "import os\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def run_qwen2vl_inference(image_path_or_pil, prompt, model_id=\"Qwen/Qwen2-VL-7B-Instruct\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Run inference with Qwen2-VL model.\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_pil: Either a file path to an image or a PIL Image object\n",
    "        prompt: Text prompt for the model\n",
    "        model_id: Model identifier from HuggingFace\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        Generated text response from the model\n",
    "    \"\"\"\n",
    "    # Load model and processor\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device\n",
    "    )\n",
    "    processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    # Handle image input - can be path or PIL Image\n",
    "    if isinstance(image_path_or_pil, str):\n",
    "        from PIL import Image\n",
    "        image = Image.open(image_path_or_pil)\n",
    "    else:\n",
    "        image = image_path_or_pil\n",
    "    \n",
    "    # Create the conversation format expected by Qwen2-VL\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template to format the conversation\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process vision information (handles image resizing, patching, etc.)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    # Prepare inputs for the model\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,  # Deterministic for object detection\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens (excluding the input)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Example usage (will be tested in next cell)\n",
    "print(\"Inference function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlaMACEb7JGj"
   },
   "source": [
    "Test your inference script using this [image](https://t4.ftcdn.net/jpg/01/57/82/05/360_F_157820583_agejYX5XeczPZuWRSCDF2YYeCGwJqUdG.jpg) with the prompt: ‚ÄúDetect the bounding box of the red car.‚Äù The model should correctly identify and locate the car in the image, confirming the script‚Äôs correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUkdCbsB7JGj"
   },
   "outputs": [],
   "source": [
    "# TASK: test the inference function\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the test image (red car)\n",
    "test_image_url = \"https://t4.ftcdn.net/jpg/01/57/82/05/360_F_157820583_agejYX5XeczPZuWRSCDF2YYeCGwJqUdG.jpg\"\n",
    "response = requests.get(test_image_url)\n",
    "test_image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the test image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(test_image)\n",
    "plt.title(\"Test Image: Red Car\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Test the inference function\n",
    "print(\"Testing Qwen2-VL inference...\")\n",
    "print(\"Prompt: 'Detect the bounding box of the red car.'\")\n",
    "print(\"\\nModel Response:\")\n",
    "\n",
    "# Uncomment below when you have GPU available\n",
    "# result = run_qwen2vl_inference(\n",
    "#     test_image, \n",
    "#     \"Detect the bounding box of the red car.\"\n",
    "# )\n",
    "# print(result)\n",
    "\n",
    "# For now, let's show what the expected output format should look like\n",
    "print(\"Expected format: <|object_ref_start|>the red car<|object_ref_end|><|box_start|>(x1,y1),(x2,y2)<|box_end|>\")\n",
    "print(\"Where coordinates are normalized to image dimensions (1000x1000 for Qwen2-VL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oosQDLcQ7JGj"
   },
   "source": [
    "# Try Qwen2VL without finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy2oO_XE7JGj"
   },
   "source": [
    "It‚Äôs a good idea to first assess the model‚Äôs current capability in detecting the nutrition table without any fine-tuning. This allows for a clear comparison between the model‚Äôs performance before and after fine-tuning. To do this, you need to write a function that extracts the bounding box by parsing the model output and then visualize the bounding box on the input image.\n",
    "\n",
    "Notice that the model‚Äôs response will likely follow a different format than wat we saw above for the dog image. Why? One possible explanation is that the nutrition table does not belong to a previously object class seen during the model‚Äôs training phase. Additionally, the bounding box coordinates returned by the model are likely inaccurate. This should highlight the necessity of fine-tuning to improve the model‚Äôs performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5JKCnIA7JGj"
   },
   "outputs": [],
   "source": [
    "# TASK: write a function to a parse model output to extract bounding box coordinates\n",
    "\n",
    "\n",
    "# TASK: write a function to visualize the bounding boxes on the input image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw_RG5kw7JGj"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGOrsuQo7JGj"
   },
   "source": [
    "The dataset requires conversion to be compatible with the Hugging Face (HF) library. Specifically, each sample must be reformatted into the OpenAI conversation format, comprising:\n",
    "\n",
    "- Roles: system, user, and assistant\n",
    "- User input: Provide an image and ask, \"Detect the bounding box of the nutrition table.\"\n",
    "- Assistant response: Format compatible with Qwen2-VL's detection question responses\n",
    "    * See pages 7 and 43 of this [paper](https://arxiv.org/pdf/2409.12191) and  [Model Card](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct#more-usage-tips) for tips\n",
    "    * Ensure inclusion of class name and bounding box coordinates using the proper special tokens.\n",
    "    * Check the expected range of bb coordinates\n",
    "    * Pay attention to the order of x,y coordinates as expected by Qwen\n",
    "\n",
    "Here is an example system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBvKAlXhI46X"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from product images.\n",
    "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG8NuzqjjbgI"
   },
   "outputs": [],
   "source": [
    "# Task: write a function to map each sample to a list of 3 dicts (one for each role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1edUqNGWTtjA"
   },
   "source": [
    "Now, let‚Äôs format the data using the chatbot structure. This will allow us to set up the interactions appropriately for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSHNqk0dkxii"
   },
   "outputs": [],
   "source": [
    "# Task: apply the function above to all samples in the training and eval datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3b76rawti6"
   },
   "source": [
    "# Model finetuning\n",
    "**Remove Model and Clean GPU**\n",
    "\n",
    "Before we proceed with training the model in the next section, let's clear the current variables and clean the GPU to free up resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxkXZuUkvy8j"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if 'inputs' in globals(): del globals()['inputs']\n",
    "    if 'model' in globals(): del globals()['model']\n",
    "    if 'processor' in globals(): del globals()['processor']\n",
    "    if 'trainer' in globals(): del globals()['trainer']\n",
    "    if 'peft_model' in globals(): del globals()['peft_model']\n",
    "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw9nm6ZP7JGk"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIrR9gP2z90z"
   },
   "source": [
    "## Load the Model for Training with NF4 weights  ‚öôÔ∏è\n",
    "\n",
    "Next, you need to load the quantized model using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index). If you want to learn more about quantization, check out [this blog post](https://huggingface.co/blog/merve/quantization) or [this one](https://www.maartengrootendorst.com/blog/quantization/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm_bJRrXsESg"
   },
   "outputs": [],
   "source": [
    "# TASK: load the NF4 model and processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65wfO29isQlX"
   },
   "source": [
    "## Set Up QLoRA and SFTConfig üöÄ\n",
    "\n",
    "Next, you need to configure [QLoRA](https://github.com/artidoro/qlora) for your training setup. QLoRA enables efficient fine-tuning of large language models while significantly reducing the memory footprint compared to traditional methods. Unlike standard LoRA, which reduces memory usage by applying a low-rank approximation, QLoRA takes it a step further by quantizing the model weights. This leads to even lower memory requirements and improved training efficiency, making it an excellent choice for optimizing our model's performance without sacrificing quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5WRzUJe-P_-"
   },
   "source": [
    "üí° NOTE:\n",
    "\n",
    "Preparing a model for QLoRA training typically involves three key steps:\n",
    "\n",
    "- Load the base model in 4-bit (using BitsAndBytesConfig).\n",
    "\n",
    "- Run prepare_model_for_kbit_training(). üö® Understand what this function does.\n",
    "\n",
    "- Apply LoRA adapters to the target modules.\n",
    "\n",
    "You can perform these steps manually, or let SFTTrainer handle steps 2 & 3 for you:\n",
    "\n",
    "- Simply load the model in 4-bit,\n",
    "\n",
    "- Pass a peft_config to SFTTrainer, which will automatically run prepare_model_for_kbit_training() (for unsharded QLoRA) and attach LoRA adapters. See lines 610 and 625 in [here](https://github.com/huggingface/trl/blob/v0.21.0/trl/trainer/sft_trainer.py)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITmkRHWCKYjf"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# Task: create LoRA config and apply LoRA to the model instrance created above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1t7Hk_f7JGn"
   },
   "source": [
    "Next, you need to create an SFT config for model finetuning. This step is critical for model convergence.\n",
    "You should set the following hyper-parameteres among others:\n",
    " - learning_rate\n",
    " - per_device_train_batch_size\n",
    " - gradient_accumulation_steps for better gradient direction estimation\n",
    " - BF16 and TF32 enablement for memory saving and faster compute\n",
    " - gradient_checkpointing for memory saving\n",
    "   \n",
    "There are other input arguments that you should also set for proper evaluation during model finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbqX1pQUKaSM"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "# TASK: create an SFT config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjQGt-iZVyef"
   },
   "source": [
    "# wandb setup\n",
    "If you have wandb account, you can set it up here.\n",
    "Let‚Äôs connect our notebook to W&B to capture essential information during training.\n",
    "Make sure to have set the logging arguments in the SFT config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckVfXDWsoF4Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "# TASK: set up wand.init\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOUrD9P-y-Kf"
   },
   "source": [
    "## Training the Model üèÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucTUbGURV2_-"
   },
   "source": [
    "You should now create a trainer object by instantiating the SFTTrainer class of HF's TRL. For this, you need to provide the training dataset, model, tokenizer, and more important a collate function.\n",
    "\n",
    "You need a collator function to properly retrieve and batch the data during the training procedure. This function will receive as the input a batch of samples:\n",
    "\n",
    "[{'role': 'system',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format. \\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
    " {'role': 'user',\n",
    "  'content': [{'type': 'image',\n",
    "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944>},\n",
    "   {'type': 'text',\n",
    "    'text': 'Detect the bounding box of the nutrition table'}]},\n",
    " {'role': 'assistant',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': '<|object_ref_start|>the nutrition table<|object_ref_end|><|box_start|>(14,57),(991,604)<|box_end|>'}]}]\n",
    "    \n",
    "[{'role': 'system',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format. \\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
    " {'role': 'user',\n",
    "  'content': [{'type': 'image',\n",
    "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408>},\n",
    "   {'type': 'text',\n",
    "    'text': 'Detect the bounding box of the nutrition table'}]},\n",
    " {'role': 'assistant',\n",
    "  'content': [{'type': 'text',\n",
    "    'text': '<|object_ref_start|>the nutrition table<|object_ref_end|><|box_start|>(147,152),(516,588)<|box_end|>'}]}]\n",
    "\n",
    "and then performs ops similar to what we did earlier in the inference script:\n",
    "  - applying chat template on each sample in the batch -> get formatted prompt\n",
    "  - applying process_vision_info on each sample in the batch -> get image pixels\n",
    "  - applying processor on formatted prompt and image pixels -> new batch\n",
    "  - modify the labels of the new batch by replacing labels correspondings to the following items to -100 (why?)\n",
    "    * text pad tokens\n",
    "    * <|vision_start|> <|vision_end|> <|image_pad|>\n",
    "\n",
    "üëâ Check out the TRL official example [scripts]( https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm.py#L87) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAzDovzylQeZ"
   },
   "outputs": [],
   "source": [
    "# TASK: Create a data collator to encode text and image pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skbpTuJlV8qN"
   },
   "source": [
    "Now, we will define the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer), which is a wrapper around the [transformers.Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class and inherits its attributes and methods. This class simplifies the fine-tuning process by properly initializing the [PeftModel](https://huggingface.co/docs/peft/v0.6.0/package_reference/peft_model) when a [PeftConfig](https://huggingface.co/docs/peft/v0.6.0/en/package_reference/config#peft.PeftConfig) object is provided. By using `SFTTrainer`, we can efficiently manage the training workflow and ensure a smooth fine-tuning experience for our Vision Language Model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_jk-U7ULYtA"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "# TASK: Create the SFT trainer and launch training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yx_sGW42dN3"
   },
   "source": [
    "# 5. Testing the Fine-Tuned Model üîç\n",
    "\n",
    "Now that we've successfully fine-tuned our Vision Language Model (VLM), it's time to evaluate its performance! In this section, we will test the model using examples from the ChartQA dataset to see how well it answers questions based on chart images. Let's dive in and explore the results! üöÄ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0KEPu6qYKqn"
   },
   "source": [
    "Let's clean up the GPU memory to ensure optimal performance üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ttx6EK8Uy8t0"
   },
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwCTPHsfujn2"
   },
   "source": [
    "We will reload the base model using the same pipeline as before, but this we will load the LoRA adpaters into the model too. LoRA adapters should be selected from the saved directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFqTNUud2lA7"
   },
   "outputs": [],
   "source": [
    "# TASK:  Load model, processor, and adapter weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqryChyLWRmR"
   },
   "source": [
    "Test the fine-tuned model on the example above, where the model previously struggled to accurately locate the nutrition table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LH6fWLid7JGp"
   },
   "outputs": [],
   "source": [
    "# TASK: test on the #20 training example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swibyq5AWctZ"
   },
   "source": [
    "Since this sample is drawn from the training set, the model has encountered it during training, which may be seen as a form of cheating. To gain a more comprehensive understanding of the model's performance, you should also evaluate it using the eval dataset. For this, write an evaluation script that measures the IoU metric between the ground truth box and the predicted boundig box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czZSBgnoef1E"
   },
   "outputs": [],
   "source": [
    "# Task: write the eval function. You can use use ops.box_iou\n",
    "from torchvision import ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATuQ6ZS6eirO",
    "outputId": "c3adc0fd-0fdc-4ff4-cc4e-14b4d9039323"
   },
   "source": [
    "Do the same evaluation for the model without finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyhR69T3dfLI"
   },
   "source": [
    "# üßë‚Äçüç≥ [Optional]  The recipe\n",
    "For the best model accuracy, one can first finetune the vision encoder while freezing the LLM. Then, we can use the ckpt above and finetune the model by applying LoRA to vision encoder and QLoRA to LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODzQhQ1R1SAR"
   },
   "source": [
    "# üîÄ Merge LoRA\n",
    "After fine-tuning with LoRA, the adapter weights can be merged back into the base model, effectively eliminating the overhead of LoRA modules during inference. This fusion produces a standalone model suitable for efficient deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbDEhQDyqbzK"
   },
   "source": [
    "# üöÄ Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjtLbVqWqFnE"
   },
   "source": [
    "Try to export your trained model (with merged LoRA weights) to vLLM and then deploy into Nvidia triton!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3SH27Iu193x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejJwH5xpcY6K"
   },
   "source": [
    "## Bonus\n",
    "\n",
    "For Qwen2-VL, implement a custom collate_fn that restricts loss computation to the answer portion only, explicitly excluding the system prompt and question from the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxFyhptrccdr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
