{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9d6333",
   "metadata": {},
   "source": [
    "This notebook, I will manually copy and paste the output of \"# Step by Step debugging the collating function before apply to chat template (to be deleted)\" section from fine_tuning_vlm_for_object_detection_trl.ipynb\n",
    "\n",
    "The structure: I will copy the code over and put the output in the markdown right after the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_formatted[0])\n",
    "print(type(train_dataset_formatted))\n",
    "print(type(train_dataset_formatted[0]))\n",
    "print(type([train_dataset_formatted[0:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0ff84",
   "metadata": {},
   "source": [
    "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x73B60D691480>, 'messages': [{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]}\n",
    "<class 'datasets.arrow_dataset.Dataset'>\n",
    "<class 'dict'>\n",
    "<class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_dataset_formatted\n",
    "\n",
    "# Extract messages and images from each sample\n",
    "messages_list = [sample['messages'] for sample in batch]\n",
    "images_list = [sample.get('image', None) for sample in batch]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(messages_list))\n",
    "print(type(images_list))\n",
    "# print(messages_list)\n",
    "# print(images_list)\n",
    "for i in range(5):\n",
    "    print(messages_list[i])\n",
    "    print(images_list[i])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ad538",
   "metadata": {},
   "source": [
    "<class 'list'>\n",
    "<class 'list'>\n",
    "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
    "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x73B69184BA60>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
    "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x73B69184B790>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(10,8),(978,994)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
    "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=692x720 at 0x73B69184B4C0>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(320,171),(627,625)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
    "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3120x4208 at 0x73B69184B880>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[{'content': [{'image': None, 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.', 'type': 'text'}], 'role': 'system'}, {'content': [{'image': 'IMAGE_PLACEHOLDER', 'text': None, 'type': 'image'}, {'image': None, 'text': 'Detect the bounding box of the nutrition table.', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(143,251),(376,468)<|box_end|>', 'type': 'text'}], 'role': 'assistant'}]\n",
    "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1944x2592 at 0x73B69184B310>\n",
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(messages_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b4309",
   "metadata": {},
   "source": [
    "1083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out samples without images\n",
    "valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "if not valid_pairs:\n",
    "    raise ValueError(\"Batch contains no valid images.\")\n",
    "\n",
    "messages_list, images_list = zip(*valid_pairs)\n",
    "messages_list = list(messages_list)\n",
    "images_list = list(images_list)\n",
    "\n",
    "# Process each sample\n",
    "texts = []\n",
    "all_images = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaaa7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conversations = []  # This will hold complete conversations\n",
    "\n",
    "for messages, image in zip(messages_list, images_list):\n",
    "    messages_with_image = []\n",
    "    # Clean up messages: remove None values and restore images\n",
    "    for msg in messages:\n",
    "        msg_copy = {'role': msg['role'], 'content': []}\n",
    "        \n",
    "        for content_item in msg['content']:\n",
    "            # Skip None entries entirely\n",
    "            if content_item is None:\n",
    "                continue\n",
    "                \n",
    "            # Process text content - filter out None text values\n",
    "            if content_item.get('type') == 'text':\n",
    "                text_value = content_item.get('text')\n",
    "                if text_value is not None and text_value != 'None':  # Check for actual None and string 'None'\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'text',\n",
    "                        'text': text_value\n",
    "                    })\n",
    "            # Process image content - replace IMAGE_PLACEHOLDER with actual PIL image\n",
    "            elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                # Check if it's IMAGE_PLACEHOLDER and replace with actual image\n",
    "                image_value = content_item.get('image')\n",
    "                if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                    # Replace with the actual PIL image from top level\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image  # Use the actual PIL image\n",
    "                    })\n",
    "                elif image_value and image_value != 'None':\n",
    "                    # Use existing image if it's not placeholder\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image_value\n",
    "                    })\n",
    "        \n",
    "        if msg_copy['content']:\n",
    "            messages_with_image.append(msg_copy)\n",
    "\n",
    "    # Add the complete conversation (3 messages) to collection\n",
    "    all_conversations.append(messages_with_image)\n",
    "\n",
    "print(\"First 5 messages_with_image examples:\")\n",
    "for i in range(min(5, len(all_conversations))):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(all_conversations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba3c97",
   "metadata": {},
   "source": [
    "First 5 messages_with_image examples:\n",
    "\n",
    "Example 0:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x73B69184BA60>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}]\n",
    "\n",
    "Example 1:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x73B69184B790>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>'}]}]\n",
    "\n",
    "Example 2:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=692x720 at 0x73B69184B4C0>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(10,8),(978,994)<|box_end|>'}]}]\n",
    "\n",
    "Example 3:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3120x4208 at 0x73B69184B880>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(320,171),(627,625)<|box_end|>'}]}]\n",
    "\n",
    "Example 4:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1944x2592 at 0x73B69184B310>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(143,251),(376,468)<|box_end|>'}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Code to make sure the data is formatted correctly after dataset.map:\n",
    "\n",
    "\n",
    "batch = train_dataset_formatted\n",
    "\n",
    "# Extract messages and images from each sample\n",
    "messages_list = [sample['messages'] for sample in batch]\n",
    "images_list = [sample.get('image', None) for sample in batch]\n",
    "\n",
    "# Filter out samples without images\n",
    "valid_pairs = [(m, img) for m, img in zip(messages_list, images_list) if img is not None]\n",
    "if not valid_pairs:\n",
    "    raise ValueError(\"Batch contains no valid images.\")\n",
    "\n",
    "messages_list, images_list = zip(*valid_pairs)\n",
    "messages_list = list(messages_list)\n",
    "images_list = list(images_list)\n",
    "\n",
    "# Process each sample\n",
    "# texts = []\n",
    "# all_images = []\n",
    "\n",
    "all_conversations = []  # This will hold complete conversations, each conversation is a list of 3 messages (system, assistant, user)\n",
    "\n",
    "for messages, image in zip(messages_list, images_list):\n",
    "    messages_with_image = []\n",
    "    # Clean up messages: remove None values and restore images\n",
    "    for msg in messages:\n",
    "        msg_copy = {'role': msg['role'], 'content': []}\n",
    "        \n",
    "        for content_item in msg['content']:\n",
    "            # Skip None entries entirely\n",
    "            if content_item is None:\n",
    "                continue\n",
    "                \n",
    "            # Process text content - filter out None text values\n",
    "            if content_item.get('type') == 'text':\n",
    "                text_value = content_item.get('text')\n",
    "                if text_value is not None and text_value != 'None':  # Check for actual None and string 'None'\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'text',\n",
    "                        'text': text_value\n",
    "                    })\n",
    "            # Process image content - replace IMAGE_PLACEHOLDER with actual PIL image\n",
    "            elif content_item.get('type') == 'image' and msg['role'] == 'user':\n",
    "                # Check if it's IMAGE_PLACEHOLDER and replace with actual image\n",
    "                image_value = content_item.get('image')\n",
    "                if image_value == 'IMAGE_PLACEHOLDER' or image_value is None:\n",
    "                    # Replace with the actual PIL image from top level\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image  # Use the actual PIL image\n",
    "                    })\n",
    "                elif image_value and image_value != 'None':\n",
    "                    # Use existing image if it's not placeholder\n",
    "                    msg_copy['content'].append({\n",
    "                        'type': 'image',\n",
    "                        'image': image_value\n",
    "                    })\n",
    "        \n",
    "        if msg_copy['content']:\n",
    "            messages_with_image.append(msg_copy)\n",
    "\n",
    "    # Add the complete conversation (3 messages) to collection\n",
    "    all_conversations.append(messages_with_image)\n",
    "\n",
    "print(\"First 5 all_conversations examples:\")\n",
    "for i in range(min(5, len(all_conversations))):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(all_conversations[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022814b",
   "metadata": {},
   "source": [
    "First 5 all_conversations examples:\n",
    "\n",
    "Example 0:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2592x1944 at 0x73AB0C0E8400>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|>'}]}]\n",
    "\n",
    "Example 1:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x408 at 0x73AB0C0E83D0>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|>'}]}]\n",
    "\n",
    "Example 2:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=692x720 at 0x73AB0C0E84F0>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(10,8),(978,994)<|box_end|>'}]}]\n",
    "\n",
    "Example 3:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3120x4208 at 0x73AB0C0E8670>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(320,171),(627,625)<|box_end|>'}]}]\n",
    "\n",
    "Example 4:\n",
    "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a Vision Language Model specialized in interpreting visual data from product images.\\nYour task is to analyze the provided product images and detect the nutrition tables in a certain format.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1944x2592 at 0x73AB0C0E86D0>}, {'type': 'text', 'text': 'Detect the bounding box of the nutrition table.'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(143,251),(376,468)<|box_end|>'}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff92509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put a batch in apply_chat_template \n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    all_conversations,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    print(text[i])\n",
    "    print(\"-\"*100)\n",
    "\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c434a2",
   "metadata": {},
   "source": [
    "<|im_start|>system\n",
    "You are a Vision Language Model specialized in interpreting visual data from product images.\n",
    "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.<|im_end|>\n",
    "<|im_start|>user\n",
    "<|vision_start|><|image_pad|><|vision_end|>Detect the bounding box of the nutrition table.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(14,57),(991,603)<|box_end|><|im_end|>\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<|im_start|>system\n",
    "You are a Vision Language Model specialized in interpreting visual data from product images.\n",
    "Your task is to analyze the provided product images and detect the nutrition tables in a certain format.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.<|im_end|>\n",
    "<|im_start|>user\n",
    "<|vision_start|><|image_pad|><|vision_end|>Detect the bounding box of the nutrition table.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<|object_ref_start|>nutrition-table<|object_ref_end|><|box_start|>(147,151),(516,588)<|box_end|><|im_end|>\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169088ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, video = process_vision_info(all_conversations)\n",
    "for i in range(5):\n",
    "    print(image[i])\n",
    "    print(\"-\"*100)\n",
    "# print(video[0:5])\n",
    "\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba5092",
   "metadata": {},
   "source": [
    "<PIL.Image.Image image mode=RGB size=2604x1932 at 0x73B60D6310F0>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<PIL.Image.Image image mode=RGB size=308x420 at 0x73B60D6B24D0>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<PIL.Image.Image image mode=RGB size=700x728 at 0x73B60D631000>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<PIL.Image.Image image mode=RGB size=3080x4144 at 0x73B69285DE10>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<PIL.Image.Image image mode=RGB size=1932x2604 at 0x73B6925AFEB0>\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5ff5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = processor(\n",
    "    text=text,\n",
    "    images=image,\n",
    "    # videos=all_videos,\n",
    "    padding=True,\n",
    "    truncation=False,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'batch_inputs: \\n {batch_inputs[\"input_ids\"][0:2, -10:]}') ## First 2 samples, last 10 tokens\n",
    "print(f'batch_inputs[\"attention_mask\"]: \\n {batch_inputs[\"attention_mask\"][0:2, -10:]}') ## 1=real token, 0=padding\n",
    "\n",
    "print(f'batch_inputs[\"input_ids\"]: \\n {batch_inputs[\"input_ids\"][0:2, 0:10]}') # First 2 samples, first 10 tokens\n",
    "print(f'batch_inputs[\"attention_mask\"]: \\n {batch_inputs[\"attention_mask\"][0:2, 0:10]}')\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "for key, value in batch_inputs.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(f'type(batch_inputs): \\n {type(batch_inputs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936298d5",
   "metadata": {},
   "source": [
    "batch_inputs: \n",
    " tensor([[    24,     16,     11,     21,     15,     18,      8, 151649, 151645,\n",
    "            198],\n",
    "        [    16,     21,     11,     20,     23,     23,      8, 151649, 151645,\n",
    "            198]])\n",
    "batch_inputs[\"attention_mask\"]: \n",
    " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "batch_inputs[\"input_ids\"]: \n",
    " tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
    "         151643],\n",
    "        [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
    "         151643]])\n",
    "batch_inputs[\"attention_mask\"]: \n",
    " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Input shapes:\n",
    "input_ids: torch.Size([1083, 16433])\n",
    "attention_mask: torch.Size([1083, 16433])\n",
    "pixel_values: torch.Size([31255872, 1176])\n",
    "image_grid_thw: torch.Size([1083, 3])\n",
    "----------------------------------------------------------------------------------------------------\n",
    "type(batch_inputs): \n",
    " <class 'transformers.feature_extraction_utils.BatchFeature'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eee9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__ method\n",
    "print(\n",
    "    f\"processor.tokenizer.pad_token_id: {processor.tokenizer.pad_token_id}\\n\"\n",
    "    f\"processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'): {processor.tokenizer.convert_tokens_to_ids('<|vision_start|>')}\\n\"\n",
    "    f\"processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'): {processor.tokenizer.convert_tokens_to_ids('<|vision_end|>')}\\n\"\n",
    "    f\"processor.tokenizer.convert_tokens_to_ids('<|image_pad|>'): {processor.tokenizer.convert_tokens_to_ids('<|image_pad|>')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc9ecd",
   "metadata": {},
   "source": [
    "processor.tokenizer.pad_token_id: 151643\n",
    "processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'): 151652\n",
    "processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'): 151653\n",
    "processor.tokenizer.convert_tokens_to_ids('<|image_pad|>'): 151655"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e961826",
   "metadata": {},
   "source": [
    "Also, the following is the context of model and tokenizer config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bf4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Analyze the tokenizer and its special tokens and model.config\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOKENIZER ANALYSIS - Understanding Special Tokens and Configuration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from transformers import Qwen2VLProcessor\n",
    "import json\n",
    "\n",
    "# Load the processor and tokenizer\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "print(\"\\n1. VOCABULARY INFORMATION:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"Padding side: {tokenizer.padding_side}\")\n",
    "print(f\"Truncation side: {tokenizer.truncation_side}\")\n",
    "\n",
    "print(f\"len(tokenizer): {len(tokenizer)}\")\n",
    "\n",
    "print(\"\\n2. SPECIAL TOKENS MAP:\")\n",
    "print(\"-\" * 40)\n",
    "special_tokens_map = tokenizer.special_tokens_map\n",
    "for token_name, token_value in special_tokens_map.items():\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token_value) if isinstance(token_value, str) else None\n",
    "    print(f\"{token_name}: '{token_value}' -> ID: {token_id}\")\n",
    "\n",
    "print(\"\\n3. ALL SPECIAL TOKENS:\")\n",
    "print(\"-\" * 40)\n",
    "all_special_tokens = tokenizer.all_special_tokens\n",
    "print(f\"Total special tokens: {len(all_special_tokens)}\")\n",
    "for i, token in enumerate(all_special_tokens[:20]):  # Show first 20\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  {i+1}. '{token}' -> ID: {token_id}\")\n",
    "if len(all_special_tokens) > 20:\n",
    "    print(f\"  ... and {len(all_special_tokens) - 20} more\")\n",
    "\n",
    "print(\"\\n4. IMPORTANT TOKEN IDS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"pad_token_id: {tokenizer.pad_token_id} ('{tokenizer.pad_token}' if exists)\")\n",
    "print(f\"bos_token_id: {tokenizer.bos_token_id} ('{tokenizer.bos_token}' if exists)\")\n",
    "print(f\"eos_token_id: {tokenizer.eos_token_id} ('{tokenizer.eos_token}' if exists)\")\n",
    "print(f\"unk_token_id: {tokenizer.unk_token_id} ('{tokenizer.unk_token}' if exists)\")\n",
    "\n",
    "# Check for vision-specific tokens\n",
    "print(\"\\n5. VISION-SPECIFIC TOKENS:\")\n",
    "print(\"-\" * 40)\n",
    "vision_tokens = [\"<|vision_start|>\", \"<|vision_end|>\", \"<|image_pad|>\", \"<|video_pad|>\", \n",
    "                 \"<|object_ref_start|>\", \"<|object_ref_end|>\", \"<|box_start|>\", \"<|box_end|>\",\n",
    "                 \"<|im_start|>\", \"<|im_end|>\"]\n",
    "for token in vision_tokens:\n",
    "    if token in tokenizer.get_vocab():\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        print(f\"'{token}' -> ID: {token_id}\")\n",
    "    else:\n",
    "        print(f\"'{token}' -> NOT IN VOCABULARY\")\n",
    "\n",
    "print(\"\\n6. CHAT TEMPLATE:\")\n",
    "print(\"-\" * 40)\n",
    "if hasattr(tokenizer, 'chat_template'):\n",
    "    print(\"Chat template exists:\")\n",
    "    # Print first 500 chars of template\n",
    "    template_str = str(tokenizer.chat_template)[:500]\n",
    "    print(template_str + \"...\" if len(str(tokenizer.chat_template)) > 500 else template_str)\n",
    "else:\n",
    "    print(\"No chat template found\")\n",
    "\n",
    "print(\"\\n7. TOKEN ID RANGE CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Valid token ID range: 0 to {tokenizer.vocab_size - 1}\")\n",
    "print(f\"Any token ID >= {tokenizer.vocab_size} will cause the training error!\")\n",
    "\n",
    "\n",
    "\n",
    "# Check Model \n",
    "\n",
    "import torch, transformers\n",
    "\n",
    "# 2) Model: use the correct family for VL\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16, # original training precision stored in config.json for this model is bfloat16, so \"auto\" = torch.bfloat16 for this model\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# 3) Collect sizes (read-only)\n",
    "tok_vocab_size = tokenizer.vocab_size\n",
    "tok_len        = len(tokenizer)\n",
    "added_vocab    = tokenizer.get_added_vocab()\n",
    "n_added        = len(added_vocab)\n",
    "\n",
    "emb_in  = model.get_input_embeddings().weight.shape[0]\n",
    "emb_out = (model.get_output_embeddings().weight.shape[0]\n",
    "           if model.get_output_embeddings() is not None else None)\n",
    "cfg_vocab = getattr(model.config, \"vocab_size\", None)\n",
    "\n",
    "print(\"---- TOKENIZER ----\")\n",
    "print(f\"tokenizer.vocab_size : {tok_vocab_size}\")\n",
    "print(f\"len(tokenizer)       : {tok_len}  (added tokens: {n_added})\")\n",
    "\n",
    "print(\"\\n---- MODEL ----\")\n",
    "print(f\"config.vocab_size    : {cfg_vocab}\")\n",
    "print(f\"input emb rows       : {emb_in}\")\n",
    "print(f\"lm_head rows         : {emb_out}\")\n",
    "print(f\"config class         : {model.config.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac63493",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "TOKENIZER ANALYSIS - Understanding Special Tokens and Configuration\n",
    "================================================================================\n",
    "/home/zhuoyuan/miniconda3/envs/vlm_Qwen2VL_object_detection/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
    "  warnings.warn(\n",
    "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
    "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
    "\n",
    "1. VOCABULARY INFORMATION:\n",
    "----------------------------------------\n",
    "Vocabulary size: 151643\n",
    "Model max length: 32768\n",
    "Padding side: left\n",
    "Truncation side: right\n",
    "len(tokenizer): 151657\n",
    "\n",
    "2. SPECIAL TOKENS MAP:\n",
    "----------------------------------------\n",
    "eos_token: '<|im_end|>' -> ID: 151645\n",
    "pad_token: '<|endoftext|>' -> ID: 151643\n",
    "additional_special_tokens: '['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']' -> ID: None\n",
    "\n",
    "3. ALL SPECIAL TOKENS:\n",
    "----------------------------------------\n",
    "Total special tokens: 14\n",
    "  1. '<|im_end|>' -> ID: 151645\n",
    "  2. '<|endoftext|>' -> ID: 151643\n",
    "  3. '<|im_start|>' -> ID: 151644\n",
    "  4. '<|object_ref_start|>' -> ID: 151646\n",
    "  5. '<|object_ref_end|>' -> ID: 151647\n",
    "  6. '<|box_start|>' -> ID: 151648\n",
    "  7. '<|box_end|>' -> ID: 151649\n",
    "  8. '<|quad_start|>' -> ID: 151650\n",
    "  9. '<|quad_end|>' -> ID: 151651\n",
    "  10. '<|vision_start|>' -> ID: 151652\n",
    "  11. '<|vision_end|>' -> ID: 151653\n",
    "  12. '<|vision_pad|>' -> ID: 151654\n",
    "  13. '<|image_pad|>' -> ID: 151655\n",
    "  14. '<|video_pad|>' -> ID: 151656\n",
    "\n",
    "4. IMPORTANT TOKEN IDS:\n",
    "----------------------------------------\n",
    "pad_token_id: 151643 ('<|endoftext|>' if exists)\n",
    "bos_token_id: None ('None' if exists)\n",
    "eos_token_id: 151645 ('<|im_end|>' if exists)\n",
    "unk_token_id: None ('None' if exists)\n",
    "\n",
    "5. VISION-SPECIFIC TOKENS:\n",
    "----------------------------------------\n",
    "'<|vision_start|>' -> ID: 151652\n",
    "'<|vision_end|>' -> ID: 151653\n",
    "'<|image_pad|>' -> ID: 151655\n",
    "'<|video_pad|>' -> ID: 151656\n",
    "'<|object_ref_start|>' -> ID: 151646\n",
    "'<|object_ref_end|>' -> ID: 151647\n",
    "'<|box_start|>' -> ID: 151648\n",
    "'<|box_end|>' -> ID: 151649\n",
    "'<|im_start|>' -> ID: 151644\n",
    "'<|im_end|>' -> ID: 151645\n",
    "\n",
    "6. CHAT TEMPLATE:\n",
    "----------------------------------------\n",
    "Chat template exists:\n",
    "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "{% endif %}<|im_start|>{{ message['role'] }}\n",
    "{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n",
    "{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = im...\n",
    "\n",
    "7. TOKEN ID RANGE CHECK:\n",
    "----------------------------------------\n",
    "Valid token ID range: 0 to 151642\n",
    "Any token ID >= 151643 will cause the training error!\n",
    "\n",
    "---- TOKENIZER ----\n",
    "tokenizer.vocab_size : 151643\n",
    "len(tokenizer)       : 151657  (added tokens: 14)\n",
    "\n",
    "---- MODEL ----\n",
    "config.vocab_size    : 152064\n",
    "input emb rows       : 152064\n",
    "lm_head rows         : 152064\n",
    "config class         : Qwen2VLConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec0288",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_Qwen2VL_object_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
